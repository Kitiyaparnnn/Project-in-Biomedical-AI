{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#Training Unsupervised B-scan classification with CNN-Based Feature Extraction",
   "id": "47ad21ba09fa99de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "import random\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ],
   "id": "218d572c56053303"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "979bd651783d1955"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Load dataset**",
   "id": "690ab68935081fec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def show_image_comparison(original, transformed):\n",
    "    \"\"\"Displays an image before and after transformation.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(original, cmap='gray')\n",
    "    axes[0].set_title(f\"Original Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(transformed.squeeze(), cmap='gray')\n",
    "    axes[1].set_title(f\"Transformed Image\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the OCT B-scan images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "\n",
    "        # Collect all image file paths\n",
    "        for root, _, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff')):\n",
    "                    self.image_paths.append(os.path.join(root, file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"L\")  # Convert to grayscale if needed\n",
    "        transformed_image = self.transform(image) if self.transform else self.transforms.ToTensor()(image)\n",
    "\n",
    "        return transformed_image\n",
    "\n",
    "    def get_image_paths(self):\n",
    "      \"\"\"Returns all image paths in the dataset.\"\"\"\n",
    "      return self.image_paths\n",
    "\n",
    "def get_data_loader(batch_size,root_dir):\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # 50% chance of horizontal flip\n",
    "        transforms.RandomRotation(degrees=(-10, 10)),  # Random rotation between -10 to 10 degrees\n",
    "        transforms.ToTensor()  # Convert images to tensors\n",
    "    ])\n",
    "\n",
    "    dataset = CustomDataset(root_dir=root_dir, transform=transform)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    return dataset, data_loader\n",
    "\n",
    "# load data\n",
    "dataset, data_loader = get_data_loader(batch_size=16, root_dir=\"/content/drive/MyDrive/Projects in Biomedical AI/Dataset/sample_JPGs/40002739/20211013/113000/L/OCT/Carl_Zeiss_Meditec_CIRRUS_HD-OCT_5000(512x1024x128)/Original/ORG_IMG_JPG/\")"
   ],
   "id": "1368b48f397b8353"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Implement Model**\n",
    "\n",
    "the unsupervised CNN autoencoder\n"
   ],
   "id": "88e395e9adfb6789"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the Autoencoder model\n",
    "class AutoencoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder (same as provided CNN)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dilated3 = nn.Conv2d(32, 64, kernel_size=3, dilation=(3, 2), padding=2)\n",
    "        self.dilated4 = nn.Conv2d(64, 64, kernel_size=3, dilation=(6, 4), padding=4)\n",
    "        self.dilated5 = nn.Conv2d(64, 64, kernel_size=3, dilation=(12, 8), padding=8)\n",
    "        self.dilated6 = nn.Conv2d(64, 128, kernel_size=3, dilation=(24, 16), padding=16)\n",
    "        self.dilated7 = nn.Conv2d(128, 128, kernel_size=3, dilation=(48, 32), padding=32)\n",
    "\n",
    "        self.conv9 = nn.Conv2d(160, 128, kernel_size=3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(128, 64, kernel_size=1)\n",
    "\n",
    "        # Decoder (to reconstruct the input)\n",
    "        self.deconv1 = nn.ConvTranspose2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "        x2 = F.relu(self.conv2(x1))\n",
    "        d3 = F.relu(self.dilated3(x2))\n",
    "        d4 = F.relu(self.dilated4(d3))\n",
    "        d5 = F.relu(self.dilated5(d4))\n",
    "        d6 = F.relu(self.dilated6(d5))\n",
    "        d7 = F.relu(self.dilated7(d6))\n",
    "\n",
    "        # Resize d7 to match x2 spatial dimensions (128x128)\n",
    "        d7_resized = F.interpolate(d7, size=(128, 128), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Feature concatenation along the channel dimension (dim=1)\n",
    "        concat = torch.cat((x2, d7_resized), dim=1)  # Output: [batch_size, 160, 128, 128]\n",
    "\n",
    "        # Feature bottleneck\n",
    "        x9 = F.relu(self.conv9(concat))\n",
    "        bottleneck = F.relu(self.conv10(x9))  # This is the feature we will use for clustering\n",
    "\n",
    "        # Decoder (Reconstruct image)\n",
    "        x = F.relu(self.deconv1(bottleneck))\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        x = F.relu(self.deconv3(x))\n",
    "        x = torch.sigmoid(self.deconv4(x))  # Output between [0,1]\n",
    "\n",
    "        return x, bottleneck  # Return reconstructed image + features for clustering\n",
    "# Example usage:\n",
    "model = AutoencoderCNN()\n",
    "print(model)"
   ],
   "id": "aa11520c7a0b4ae4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Training**",
   "id": "ff83879fb273510c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_loss(loss_history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(loss_history, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def train_model(model, data_loader, num_epochs, lr, checkpoint_path=\"best_model.pth\"):\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "    model.to(device)\n",
    "    loss_history = []  # Store loss per epoch\n",
    "    best_loss = float(\"inf\")  # Track best loss\n",
    "\n",
    "    # os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)  # Ensure checkpoint directory exists\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0  # Track total loss for the epoch\n",
    "\n",
    "        for images in data_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed, _ = model(images)  # Get reconstructed images\n",
    "            loss = criterion(reconstructed, images)  # Compare with input\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(data_loader)  # Compute average loss per epoch\n",
    "        loss_history.append(avg_loss)\n",
    "\n",
    "        # Adjust learning rate based on loss\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # # Save best model based on loss\n",
    "        # if avg_loss < best_loss:\n",
    "        #     best_loss = avg_loss\n",
    "        #     torch.save(model.state_dict(), checkpoint_path)\n",
    "        #     print(f\"Checkpoint saved at {checkpoint_path} (Best Loss: {best_loss:.4f})\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return loss_history"
   ],
   "id": "73a738573f3de5dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_epochs = 16\n",
    "lr = 0.0001"
   ],
   "id": "8626d6064bbd4ca9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# train\n",
    "# checkpoint_path = \"/best_model.pth\"\n",
    "loss = train_model(model, data_loader, num_epochs, lr, checkpoint_path)\n",
    "plot_loss(loss)"
   ],
   "id": "3003aaaf4594f34e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Extract Feature from CNN**",
   "id": "78805851ed323c6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get best model\n",
    "# best_model = model.load_state_dict(torch.load(checkpoint_path))\n",
    "best_model = model"
   ],
   "id": "eb7830a431a237f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract Features for Clustering\n",
    "features_list = []\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    for images in data_loader:\n",
    "        images = images.to(device)\n",
    "        _, features = best_model(images)\n",
    "        features_list.append(features.cpu())\n",
    "\n",
    "# Convert to NumPy for Clustering\n",
    "features_np = np.concatenate([f.numpy() for f in features_list], axis=0)\n",
    "print(f\"number of features: {len(features_list)}\")"
   ],
   "id": "1f6ba5efb16592dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Clustering**",
   "id": "7c6db111dd190332"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Flatten feature maps for clustering\n",
    "features_reshaped = features_np.reshape(features_np.shape[0], -1)\n",
    "\n",
    "# Perform Clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "labels = kmeans.fit_predict(features_reshaped)\n",
    "\n",
    "# Print Cluster Assignments\n",
    "# print(labels)"
   ],
   "id": "60640bdb85fcc54f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Reduce dimensions for visualization\n",
    "pca = PCA(n_components=2)  # Try with PCA first\n",
    "features_2d = pca.fit_transform(features_reshaped)\n",
    "\n",
    "# Alternative: t-SNE (may give better clustering visualization)\n",
    "# tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "# features_2d = tsne.fit_transform(features_reshaped)\n",
    "\n",
    "# Scatter plot of clustered data\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=features_2d[:, 0], y=features_2d[:, 1], hue=labels, palette='coolwarm', alpha=0.7)\n",
    "plt.title(\"Clustering Visualization of B-scan Features\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()"
   ],
   "id": "4f485aa5a437e5ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Select random images from each cluster\n",
    "# def plot_cluster_examples(dataset, labels, cluster_id, num_images=5):\n",
    "#     image_paths = dataset.get_image_paths()  # Get image paths\n",
    "#     indices = [i for i, label in enumerate(labels) if label == cluster_id]\n",
    "#     selected_indices = random.sample(indices, min(num_images, len(indices)))\n",
    "\n",
    "#     fig, axes = plt.subplots(1, len(selected_indices), figsize=(15, 5))\n",
    "#     for i, idx in enumerate(selected_indices):\n",
    "#         img_path = image_paths[idx]\n",
    "#         img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "#         axes[i].imshow(img)\n",
    "#         axes[i].axis(\"off\")\n",
    "#     label = \"have fovea\" if cluster_id else \"not have fovea\"\n",
    "#     plt.suptitle(f\"Cluster {label} Examples\")\n",
    "#     plt.show()\n",
    "\n",
    "# Show all images from selected cluster\n",
    "def plot_cluster_examples(dataset, labels, cluster_id=1):\n",
    "    \"\"\"\n",
    "    Plots all images belonging to a specific cluster.\n",
    "\n",
    "    Args:\n",
    "        dataset (CustomDataset): The dataset containing images.\n",
    "        labels (list or array): Cluster labels for each image.\n",
    "        cluster_id (int): The cluster ID to display.\n",
    "    \"\"\"\n",
    "    image_paths = dataset.get_image_paths()  # Get all image paths\n",
    "    indices = [i for i, label in enumerate(labels) if label == cluster_id]\n",
    "\n",
    "    if not indices:\n",
    "        print(f\"No images found for cluster {cluster_id}.\")\n",
    "        return\n",
    "\n",
    "    num_images = len(indices)\n",
    "    cols = min(num_images, 5)  # Limit to 5 images per row\n",
    "    rows = (num_images + cols - 1) // cols  # Compute required rows\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 3 * rows))\n",
    "    axes = axes.flatten() if num_images > 1 else [axes]\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        img_path = image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    # Hide any extra subplots (if images are fewer than grid size)\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(f\"All Images in Cluster {cluster_id}: {num_images} images\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot examples from both clusters\n",
    "# plot_cluster_examples(dataset, labels, cluster_id=0)\n",
    "plot_cluster_examples(dataset, labels, cluster_id=1)"
   ],
   "id": "6f31208d42f7ff1e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Locate Fovea**",
   "id": "5c647b29db7b438e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def gaussian_kernel_2d(size=3, sigma=1.0):\n",
    "    \"\"\"Creates a 2D Gaussian kernel.\"\"\"\n",
    "    x = torch.arange(size) - size // 2\n",
    "    y = torch.arange(size) - size // 2\n",
    "    x, y = torch.meshgrid(x, y, indexing='ij')\n",
    "\n",
    "    kernel = torch.exp(-(x**2 + y**2) / (2 * sigma**2))\n",
    "    kernel /= kernel.sum()\n",
    "\n",
    "    return kernel\n",
    "\n",
    "\n",
    "def apply_gaussian_smoothing_2d(output, kernel_size=3, sigma=1.0):\n",
    "    \"\"\"Applies 2D Gaussian smoothing to each depth slice of a 3D network output.\"\"\"\n",
    "    kernel = gaussian_kernel_2d(kernel_size, sigma).unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, H, W)\n",
    "\n",
    "    smoothed_slices = []\n",
    "    for d in range(output.shape[0]):  # Loop over depth\n",
    "        slice_2d = output[d].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, H, W)\n",
    "        smoothed_slice = F.conv2d(slice_2d, kernel, padding=kernel_size // 2).squeeze(0).squeeze(0)\n",
    "        smoothed_slices.append(smoothed_slice)\n",
    "\n",
    "    return torch.stack(smoothed_slices)  # Shape: (D, H, W)\n",
    "\n",
    "def fill_max_probability_color(smoothed_output):\n",
    "    \"\"\"Finds the max probability location and highlights it on a 2D grayscale image.\"\"\"\n",
    "    # Convert 3D (D, H, W) tensor to 2D by taking max along depth\n",
    "    max_projection, max_depth = torch.max(smoothed_output, dim=0)  # Shape: (H, W)\n",
    "\n",
    "    # Get max probability location\n",
    "    max_idx = torch.argmax(max_projection)\n",
    "    max_y, max_x = np.unravel_index(max_idx.cpu().numpy(), max_projection.shape)\n",
    "\n",
    "    # Normalize grayscale image to 255 range\n",
    "    gray_image = (max_projection.cpu().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "    # Convert grayscale to PIL image\n",
    "    img = Image.fromarray(gray_image, mode='L')  # 'L' for grayscale\n",
    "\n",
    "    # Highlight the max probability pixel with red\n",
    "    img_colored = img.convert(\"RGB\")  # Convert grayscale to RGB\n",
    "    pixels = img_colored.load()\n",
    "    pixels[max_x, max_y] = (255, 0, 0)  # Set max prob pixel to red\n",
    "\n",
    "    return img_colored, (max_x, max_y, max_depth[max_y, max_x].item())\n",
    "\n",
    "def process_output(output, kernel_size=3, sigma=1.0):\n",
    "    \"\"\"Applies 2D Gaussian smoothing and marks the highest probability pixel.\"\"\"\n",
    "    smoothed_output = apply_gaussian_smoothing_2d(output, kernel_size, sigma)\n",
    "    color_image, max_pos = fill_max_probability_color(smoothed_output)\n",
    "\n",
    "    return color_image, max_pos\n",
    "\n",
    "def show_image(image):\n",
    "    \"\"\"Displays an image using PIL.\"\"\"\n",
    "    image.show()"
   ],
   "id": "a478a0853f723c4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example usage\n",
    "output = torch.rand(5, 5, 5)  # Example grayscale output tensor\n",
    "color_img, max_position = process_output(output, kernel_size=1, sigma=1.0)\n",
    "\n",
    "print(\"Max Probability Position:\", max_position)\n",
    "show_image(color_img)  # Show the result using PIL"
   ],
   "id": "7d4439b70ad5f283"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6a44b83736ef7e54"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
