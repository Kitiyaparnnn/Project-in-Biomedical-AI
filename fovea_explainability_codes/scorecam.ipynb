{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "493f291b",
   "metadata": {},
   "source": [
    "## ScoreCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45185cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import json\n",
    "import re\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bba5047e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clinical_ai/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clinical_ai/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/5t/qhv6pkk115b3_mpr1bbc5c9c0000gn/T/ipykernel_2507/1385931762.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model from .pth file\n",
    "class VGG16BinaryClassifier(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(VGG16BinaryClassifier, self).__init__()\n",
    "\n",
    "        # Load VGG-16 model\n",
    "        self.vgg16 = models.vgg16(pretrained=pretrained)\n",
    "\n",
    "        # Freeze convolutional layers (optional)\n",
    "        for param in self.vgg16.features.parameters():\n",
    "            param.requires_grad = True  # Set to True if you want to fine-tune\n",
    "\n",
    "        # Modify the classifier head for binary classification\n",
    "        self.vgg16.classifier = nn.Sequential(\n",
    "            nn.Linear(25088, 4096),  # VGG-16's default input size\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 512),  # Custom hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 1)  # Output 1 logit (for binary classification)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vgg16(x)  # No sigmoid, return raw logits\n",
    "\n",
    "# Function to load the trained model\n",
    "def load_model(model_path, device):\n",
    "    # Instantiate the model\n",
    "    model = VGG16BinaryClassifier(pretrained=True)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    # print(model)\n",
    "    return model\n",
    "\n",
    "# Load model\n",
    "model_path = \"best_VGG_model_1.pth\"  \n",
    "model = load_model(model_path,device)\n",
    "\n",
    "# Select the target convolutional layer (last conv layer in VGG16 features block)\n",
    "target_layer = model.vgg16.features[28]  # Last conv layer before FC layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e555ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveValues():\n",
    "    def __init__(self, m):\n",
    "        # register a hook to save values of activations and gradients\n",
    "        self.activations = None\n",
    "        self.gradients = None\n",
    "        self.forward_hook = m.register_forward_hook(self.hook_fn_act)\n",
    "        self.backward_hook = m.register_backward_hook(self.hook_fn_grad)\n",
    "\n",
    "    def hook_fn_act(self, module, input, output):\n",
    "        self.activations = output\n",
    "\n",
    "    def hook_fn_grad(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    def remove(self):\n",
    "        self.forward_hook.remove()\n",
    "        self.backward_hook.remove()\n",
    "        \n",
    "class CAM(object):\n",
    "    \"\"\" Class Activation Mapping \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: a base model to get CAM which have global pooling and fully connected layer.\n",
    "            target_layer: conv_layer before Global Average Pooling\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "\n",
    "        # save values of activations and gradients in target_layer\n",
    "        self.values = SaveValues(self.target_layer)\n",
    "\n",
    "    def forward(self, x, idx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input image. shape =>(1, 3, H, W)\n",
    "        Return:\n",
    "            heatmap: class activation mappings of the predicted class\n",
    "        \"\"\"\n",
    "\n",
    "        # object classification\n",
    "        score = self.model(x)\n",
    "\n",
    "        prob = F.softmax(score, dim=1)\n",
    "\n",
    "        if idx is None:\n",
    "            prob, idx = torch.max(prob, dim=1)\n",
    "            idx = idx.item()\n",
    "            prob = prob.item()\n",
    "            print(\"predicted class ids {}\\t probability {}\".format(idx, prob))\n",
    "\n",
    "        # cam can be calculated from the weights of linear layer and activations\n",
    "        weight_fc = list(\n",
    "            self.model._modules.get('fc').parameters())[0].to('cpu').data\n",
    "\n",
    "        cam = self.getCAM(self.values, weight_fc, idx)\n",
    "\n",
    "        return cam, idx\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def getCAM(self, values, weight_fc, idx):\n",
    "        '''\n",
    "        values: the activations and gradients of target_layer\n",
    "            activations: feature map before GAP.  shape => (1, C, H, W)\n",
    "        weight_fc: the weight of fully connected layer.  shape => (num_classes, C)\n",
    "        idx: predicted class id\n",
    "        cam: class activation map.  shape => (1, num_classes, H, W)\n",
    "        '''\n",
    "\n",
    "        cam = F.conv2d(values.activations, weight=weight_fc[:, :, None, None])\n",
    "        _, _, h, w = cam.shape\n",
    "\n",
    "        # class activation mapping only for the predicted class\n",
    "        # cam is normalized with min-max.\n",
    "        cam = cam[:, idx, :, :]\n",
    "        cam -= torch.min(cam)\n",
    "        cam /= torch.max(cam)\n",
    "        cam = cam.view(1, 1, h, w)\n",
    "\n",
    "        return cam.data\n",
    "    \n",
    "class ScoreCAM(CAM):\n",
    "    \"\"\" Score CAM \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layer, n_batch=32):\n",
    "        super().__init__(model, target_layer)\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: a base model\n",
    "            target_layer: conv_layer you want to visualize\n",
    "        \"\"\"\n",
    "        self.n_batch = n_batch\n",
    "\n",
    "    def forward(self, x, idx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input image. shape =>(1, 3, H, W)\n",
    "            idx: the index of the target class\n",
    "        Return:\n",
    "            heatmap: class activation mappings of predicted classes\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            orig_H, orig_W = x.shape[2], x.shape[3]  # Save original size (512x1024)\n",
    "            x_resized = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "\n",
    "            device = x.device\n",
    "\n",
    "            self.model.zero_grad()\n",
    "            score = self.model(x_resized)\n",
    "            prob = F.softmax(score, dim=1)\n",
    "\n",
    "            if idx is None:\n",
    "                p, idx = torch.max(prob, dim=1)\n",
    "                idx = idx.item()\n",
    "                # print(\"predicted class ids {}\\t probability {}\".format(idx, p))\n",
    "\n",
    "            # # calculate the derivate of probabilities, not that of scores\n",
    "            # prob[0, idx].backward(retain_graph=True)\n",
    "\n",
    "            self.activations = self.values.activations.to('cpu').clone()\n",
    "            # put activation maps through relu activation\n",
    "            # because the values are not normalized with eq.(1) without relu.\n",
    "            self.activations = F.relu(self.activations)\n",
    "            self.activations = F.interpolate(self.activations, size=(224, 224), mode='bilinear')\n",
    "            _, C, _, _ = self.activations.shape\n",
    "\n",
    "            # normalization\n",
    "            act_min, _ = self.activations.view(1, C, -1).min(dim=2)\n",
    "            act_min = act_min.view(1, C, 1, 1)\n",
    "            act_max, _ = self.activations.view(1, C, -1).max(dim=2)\n",
    "            act_max = act_max.view(1, C, 1, 1)\n",
    "            denominator = torch.where(\n",
    "                (act_max - act_min) != 0., act_max - act_min, torch.tensor(1.)\n",
    "            )\n",
    "\n",
    "            self.activations = self.activations / denominator\n",
    "\n",
    "            # generate masked images and calculate class probabilities\n",
    "            probs = []\n",
    "            for i in range(0, C, self.n_batch):\n",
    "                mask = self.activations[:, i:i+self.n_batch].transpose(0, 1)\n",
    "                mask = mask.to(device)\n",
    "                \n",
    "                # 🔹 Fix: Resize mask to match original image dimensions (512x1024)\n",
    "                # mask = F.interpolate(mask, size=(orig_H, orig_W), mode='bilinear', align_corners=False)\n",
    "                masked_x = x_resized * mask\n",
    "                score = self.model(masked_x)\n",
    "                probs.append(F.softmax(score, dim=1)[:, idx].to('cpu').data)\n",
    "\n",
    "            probs = torch.stack(probs)\n",
    "            weights = probs.view(1, C, 1, 1)\n",
    "\n",
    "            # shape = > (1, 1, H, W)\n",
    "            cam = (weights * self.activations).sum(1, keepdim=True)\n",
    "            cam = F.relu(cam)\n",
    "            cam -= torch.min(cam)\n",
    "            cam /= torch.max(cam)\n",
    "\n",
    "            # Resize heatmap to original image size (512x1024)\n",
    "            cam = F.interpolate(cam, size=(orig_H, orig_W), mode='bilinear', align_corners=False)\n",
    "\n",
    "\n",
    "        return cam.data, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a574b1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1019_left: 100%|██████████| 31/31 [28:43<00:00, 55.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 centroid on Bscan of 1019_left: (288,319)\n",
      "✅ Saved binary mask to ScoreCam/1019_left/fovea_mask.png\n",
      "✅ Saved annotated image to ScoreCam/1019_left/fovea_enface.png\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "base_dir = \"predictions 2\"\n",
    "scorecam_base = \"ScoreCam\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to device and eval mode\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "scorecam = ScoreCAM(model, target_layer=target_layer)\n",
    "\n",
    "# sample_names = sorted(os.listdir(base_dir))\n",
    "sample_names = [\"1019_left\"]\n",
    "for sample_name in sample_names:\n",
    "    pred_fovea_dir = os.path.join(base_dir, sample_name, \"predictions/fovea\")\n",
    "    if not os.path.isdir(pred_fovea_dir):\n",
    "        print(f\"❌ Directory not found: {pred_fovea_dir}\")\n",
    "        continue  # Skip if not a directory\n",
    "\n",
    "    scorecam_mask_dir = os.path.join(scorecam_base, sample_name, \"masked_images\")\n",
    "    scorecam_centroid_dir = os.path.join(scorecam_base, sample_name, \"centroid\")\n",
    "    os.makedirs(scorecam_mask_dir, exist_ok=True)\n",
    "    os.makedirs(scorecam_centroid_dir, exist_ok=True)\n",
    "\n",
    "    img_filenames = sorted([\n",
    "        f for f in os.listdir(pred_fovea_dir)\n",
    "        if f.endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ])\n",
    "\n",
    "    masked_stack = []\n",
    "\n",
    "    for img_name in tqdm(img_filenames, desc=f\"Processing {sample_name}\"):\n",
    "        img_path = os.path.join(pred_fovea_dir, img_name)\n",
    "        original_img = Image.open(img_path).convert(\"RGB\")\n",
    "        input_tensor = preprocess(original_img).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            cam = scorecam(input_tensor)[0]\n",
    "            cam_np = cam.squeeze().cpu().numpy()\n",
    "            cam = cv2.resize(cam_np, original_img.size)\n",
    "\n",
    "        mask = (cam > 0.5).astype(np.uint8)\n",
    "        masked_stack.append(mask)\n",
    "\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "        original_np = np.array(original_img)\n",
    "        blended = cv2.addWeighted(original_np, 0.5, heatmap, 0.5, 0)\n",
    "        cv2.imwrite(os.path.join(scorecam_mask_dir, img_name), blended)\n",
    "\n",
    "    # Calculate centroid\n",
    "    stack_array = np.stack(masked_stack, axis=0)\n",
    "    overlap_mask = (np.sum(stack_array, axis=0) > (len(masked_stack) // 2)).astype(np.uint8)\n",
    "    moments = cv2.moments(overlap_mask)\n",
    "    cx = int(moments[\"m10\"] / moments[\"m00\"]) if moments[\"m00\"] != 0 else 0\n",
    "    cy = int(moments[\"m01\"] / moments[\"m00\"]) if moments[\"m00\"] != 0 else 0\n",
    "    print(f\"📌 centroid on Bscan of {sample_name}: ({cx},{cy})\")\n",
    "\n",
    "    # Draw centroid\n",
    "    mid_idx = len(img_filenames) // 2\n",
    "    mid_img_path = os.path.join(pred_fovea_dir, img_filenames[mid_idx])\n",
    "    mid_img = cv2.imread(mid_img_path)\n",
    "    centroid_img = mid_img.copy()\n",
    "    cv2.circle(centroid_img, (cx, cy), radius=5, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "    centroid_img_path = os.path.join(scorecam_centroid_dir, f\"{img_filenames[mid_idx].split('.')[0]}_centroid_{cx}x{cy}.png\")\n",
    "    cv2.imwrite(centroid_img_path, centroid_img)\n",
    "\n",
    "    # Save visualization\n",
    "    def show_masked_images(volume_path):\n",
    "        img_filenames = sorted([\n",
    "            f for f in os.listdir(volume_path)\n",
    "            if f.endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "        ])\n",
    "        num_images_to_show = len(img_filenames)\n",
    "        images_per_row = 5\n",
    "        num_rows = math.ceil(num_images_to_show / images_per_row)\n",
    "\n",
    "        fig, axes = plt.subplots(num_rows, images_per_row, figsize=(10, 3 * num_rows))\n",
    "        axes = axes.flatten()\n",
    "        for idx, img_name in enumerate(img_filenames):\n",
    "            img = Image.open(os.path.join(volume_path, img_name))\n",
    "            serial_number = img_name.split(\"_\")[-1].split(\".\")[0]\n",
    "            axes[idx].imshow(img)\n",
    "            axes[idx].axis(\"off\")\n",
    "            axes[idx].set_title(f\"SN: {serial_number}\")\n",
    "\n",
    "        for idx in range(num_images_to_show, len(axes)):\n",
    "            axes[idx].axis(\"off\")\n",
    "\n",
    "        plt.suptitle(f\"Masked images of {sample_name}\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(scorecam_base, sample_name, f\"masked_{sample_name}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    show_masked_images(scorecam_mask_dir)\n",
    "\n",
    "    def draw_fovea_enface(sample_name, cx, bscan_idx, enface_path, radius=10):\n",
    "        def map_centroid_to_enface(cx, bscan_idx, bscan_width=1024, num_slices=128, enface_size=512):\n",
    "            x_enface = cx\n",
    "            y_enface = int((bscan_idx / num_slices) * enface_size)\n",
    "            return x_enface, y_enface\n",
    "\n",
    "        if not os.path.exists(enface_path):\n",
    "            print(f\"❌ Enface image not found at: {enface_path}\")\n",
    "            return\n",
    "\n",
    "        enface_img = cv2.imread(enface_path)\n",
    "        h, w = enface_img.shape[:2]\n",
    "        x_enface, y_enface = map_centroid_to_enface(cx, bscan_idx)\n",
    "\n",
    "        # --- MPS-accelerated circle mask ---\n",
    "        y_grid, x_grid = torch.meshgrid(torch.arange(h, device=device), torch.arange(w, device=device), indexing='ij')\n",
    "        dist_sq = (x_grid - x_enface) ** 2 + (y_grid - y_enface) ** 2\n",
    "        circle_mask = (dist_sq <= radius**2).to(torch.uint8) * 255  # Values: 0 or 255\n",
    "\n",
    "        # Save mask only\n",
    "        save_dir = os.path.join(scorecam_base, sample_name)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        mask_path = os.path.join(save_dir, \"fovea_mask.png\")\n",
    "        cv2.imwrite(mask_path, circle_mask.cpu().numpy())\n",
    "\n",
    "        # Draw circle on image for visualization\n",
    "        annotated_img = enface_img.copy()\n",
    "        cv2.circle(annotated_img, (x_enface, y_enface), radius, (0, 255, 0), thickness=1)\n",
    "        save_img_path = os.path.join(save_dir, \"fovea_enface.png\")\n",
    "        cv2.imwrite(save_img_path, annotated_img)\n",
    "\n",
    "        print(f\"✅ Saved binary mask to {mask_path}\")\n",
    "        print(f\"✅ Saved annotated image to {save_img_path}\")\n",
    "\n",
    "    match = re.search(r\"bscan_(\\d+)_centroid_(\\d+)x\\d+\", centroid_img_path)\n",
    "    img_at_mid = int(match.group(1)) \n",
    "    cx = int(match.group(2)) \n",
    "    enface_path = input(\"Enter the path to the enface image: \")\n",
    "    draw_fovea_enface(sample_name, cx, img_at_mid, enface_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
