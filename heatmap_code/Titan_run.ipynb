{
 "cells": [
  {
   "cell_type": "code",
   "id": "f507c9a3-a62a-4989-b55c-e0e1d50e4f6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:48:04.506166Z",
     "start_time": "2025-05-05T19:48:03.062542Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "from scipy.ndimage import gaussian_filter1d, gaussian_filter"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch._inductor' from 'C:\\Users\\shrey\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_inductor\\__init__.py' has no attribute 'custom_graph_pass' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01moptim\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01moptim\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmatplotlib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplt\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorchvision\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtransforms\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransforms\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdata\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DataLoader, Dataset, random_split\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorchvision\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodels\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmodels\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\__init__.py:10\u001B[39m\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001B[39;00m\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# .extensions) before entering _meta_registrations.\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mextension\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _HAS_OPS  \u001B[38;5;66;03m# usort:skip\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorchvision\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001B[38;5;66;03m# usort:skip\u001B[39;00m\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     13\u001B[39m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mversion\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m __version__  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\models\\__init__.py:2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01malexnet\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m *\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mconvnext\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m *\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdensenet\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m *\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mefficientnet\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m *\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\models\\convnext.py:9\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mnn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m functional \u001B[38;5;28;01mas\u001B[39;00m F\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mops\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmisc\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Conv2dNormActivation, Permute\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mops\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mstochastic_depth\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m StochasticDepth\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtransforms\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_presets\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ImageClassification\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _log_api_usage_once\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\ops\\__init__.py:23\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mgiou_loss\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m generalized_box_iou_loss\n\u001B[32m     22\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmisc\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Conv2dNormActivation, Conv3dNormActivation, FrozenBatchNorm2d, MLP, Permute, SqueezeExcitation\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpoolers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m MultiScaleRoIAlign\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mps_roi_align\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ps_roi_align, PSRoIAlign\n\u001B[32m     25\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mps_roi_pool\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ps_roi_pool, PSRoIPool\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\ops\\poolers.py:10\u001B[39m\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorchvision\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mops\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mboxes\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m box_area\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _log_api_usage_once\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mroi_align\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m roi_align\n\u001B[32m     13\u001B[39m \u001B[38;5;66;03m# copying result_idx_in_level to a specific index in result[]\u001B[39;00m\n\u001B[32m     14\u001B[39m \u001B[38;5;66;03m# is not supported by ONNX tracing yet.\u001B[39;00m\n\u001B[32m     15\u001B[39m \u001B[38;5;66;03m# _onnx_merge_levels() is an implementation supported by ONNX\u001B[39;00m\n\u001B[32m     16\u001B[39m \u001B[38;5;66;03m# that merges the levels to the right indices\u001B[39;00m\n\u001B[32m     17\u001B[39m \u001B[38;5;129m@torch\u001B[39m.jit.unused\n\u001B[32m     18\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_onnx_merge_levels\u001B[39m(levels: Tensor, unmerged_results: List[Tensor]) -> Tensor:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\ops\\roi_align.py:7\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfx\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m nn, Tensor\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_dynamo\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m is_compile_supported\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mjit\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mannotations\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m BroadcastingList2\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mnn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodules\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _pair\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_dynamo\\__init__.py:13\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[33;03mTorchDynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster.\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[33;03mTorchDynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m      8\u001B[39m \u001B[33;03mseamlessly optimize PyTorch programs, including those using modern Python features.\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m config, convert_frame, eval_frame, resume_execution\n\u001B[32m     14\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbackends\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mregistry\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m list_backends, lookup_backend, register_backend\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcallback\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_dynamo\\convert_frame.py:52\u001B[39m\n\u001B[32m     50\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_C\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_dynamo\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mguards\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m GlobalStateGuard\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_dynamo\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdistributed\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m get_compile_pg\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_dynamo\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01msymbolic_convert\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m TensorifyState\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_guards\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m compile_context, CompileContext, CompileId, tracing\n\u001B[32m     54\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_logging\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m structured\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_dynamo\\symbolic_convert.py:52\u001B[39m\n\u001B[32m     50\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_logging\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_dynamo\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mexc\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m TensorifyScalarRestartAnalysis\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_guards\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tracing, TracingContext\n\u001B[32m     54\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfx\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mexperimental\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01msymbolic_shapes\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m guard_bool\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_dynamo\\exc.py:41\u001B[39m\n\u001B[32m     38\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_guards\u001B[39;00m\n\u001B[32m     40\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m config\n\u001B[32m---> \u001B[39m\u001B[32m41\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m counters\n\u001B[32m     44\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n\u001B[32m     45\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtypes\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_dynamo\\utils.py:68\u001B[39m\n\u001B[32m     65\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtyping_extensions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Literal, TypeIs\n\u001B[32m     67\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m68\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_functorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mconfig\u001B[39;00m\n\u001B[32m     69\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfx\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mexperimental\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01msymbolic_shapes\u001B[39;00m\n\u001B[32m     70\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_pytree\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpytree\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_functorch\\config.py:40\u001B[39m\n\u001B[32m     37\u001B[39m \u001B[38;5;66;03m# Applies CSE to the graph before partitioning\u001B[39;00m\n\u001B[32m     38\u001B[39m cse = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m40\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_inductor\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mconfig\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m is_fbcode\n\u001B[32m     43\u001B[39m enable_autograd_cache: \u001B[38;5;28mbool\u001B[39m = Config(\n\u001B[32m     44\u001B[39m     justknob=\u001B[33m\"\u001B[39m\u001B[33mpytorch/remote_cache:enable_local_autograd_cache\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     45\u001B[39m     env_name_force=\u001B[33m\"\u001B[39m\u001B[33mTORCHINDUCTOR_AUTOGRAD_CACHE\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     46\u001B[39m     default=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m     47\u001B[39m )\n\u001B[32m     50\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mremote_autograd_cache_default\u001B[39m() -> Optional[\u001B[38;5;28mbool\u001B[39m]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_inductor\\__init__.py:9\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mos\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtyping\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Any, IO, Optional, TYPE_CHECKING, Union\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_inductor\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mconfig\u001B[39;00m\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfx\u001B[39;00m\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_inductor\\config.py:202\u001B[39m\n\u001B[32m    195\u001B[39m b2b_gemm_pass = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    197\u001B[39m \u001B[38;5;66;03m# register custom graph optimization pass hook. so far, pre/post passes are\u001B[39;00m\n\u001B[32m    198\u001B[39m \u001B[38;5;66;03m# only applied before/after pattern_matcher in post_grad_passes.\u001B[39;00m\n\u001B[32m    199\u001B[39m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    200\u001B[39m \u001B[38;5;66;03m# Implement CustomGraphPass to allow Inductor to graph compiled artifacts\u001B[39;00m\n\u001B[32m    201\u001B[39m \u001B[38;5;66;03m# to which your custom passes have been applied:\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m202\u001B[39m post_grad_custom_pre_pass: \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_inductor\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcustom_graph_pass\u001B[49m.CustomGraphPassType = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    203\u001B[39m post_grad_custom_post_pass: torch._inductor.custom_graph_pass.CustomGraphPassType = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    205\u001B[39m \u001B[38;5;66;03m# Registers a custom joint graph pass.\u001B[39;00m\n",
      "\u001B[31mAttributeError\u001B[39m: partially initialized module 'torch._inductor' from 'C:\\Users\\shrey\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_inductor\\__init__.py' has no attribute 'custom_graph_pass' (most likely due to a circular import)"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "60c75fda-7a81-43c5-895b-7a32807ee588",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = \"best_VGG_model_1.pth\"  # Update with your model path"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9dfeb7b-fff3-4efd-941c-9baf5c316ed1",
   "metadata": {},
   "source": [
    "class VGG16BinaryClassifier(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(VGG16BinaryClassifier, self).__init__()\n",
    "        self.vgg16 = models.vgg16(pretrained=pretrained)\n",
    "        for param in self.vgg16.features.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.vgg16.classifier = nn.Sequential(\n",
    "            nn.Linear(25088, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vgg16(x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "93ec358d-5c96-4271-9d10-f57ca21db157",
   "metadata": {},
   "source": [
    "class OcclusionSensitivity:\n",
    "    def __init__(self, model, window_size=32, stride=16):\n",
    "        self.model = model\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        \n",
    "    def generate_heatmap(self, input_tensor, target_class=None):\n",
    "        \"\"\"Generate occlusion sensitivity heatmap\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Original prediction\n",
    "            original_output = torch.sigmoid(self.model(input_tensor))\n",
    "            original_prob = original_output.item()\n",
    "            original_class = 1 if original_prob > 0.5 else 0\n",
    "            target_class = target_class or original_class\n",
    "            \n",
    "            # Setup dimensions\n",
    "            b, c, h, w = input_tensor.shape\n",
    "            heatmap = torch.zeros((h, w), device=device)\n",
    "            pad = self.window_size // 2\n",
    "            \n",
    "            # Pad input for complete coverage\n",
    "            padded_input = F.pad(input_tensor, (pad, pad, pad, pad), value=0)\n",
    "            \n",
    "            # Slide occlusion window\n",
    "            for y in tqdm(range(0, h, self.stride), desc=\"Generating occlusion map\"):\n",
    "                for x in range(0, w, self.stride):\n",
    "                    # Create occluded version\n",
    "                    occluded = padded_input.clone()\n",
    "                    y_start = y + pad\n",
    "                    x_start = x + pad\n",
    "                    occluded[..., y_start:y_start+self.window_size, \n",
    "                            x_start:x_start+self.window_size] = 0\n",
    "                    \n",
    "                    # Get modified prediction\n",
    "                    output = torch.sigmoid(self.model(occluded[..., pad:-pad, pad:-pad]))\n",
    "                    current_prob = output.item()\n",
    "                    \n",
    "                    # Calculate impact score\n",
    "                    if target_class == 1:\n",
    "                        score = original_prob - current_prob\n",
    "                    else:\n",
    "                        score = current_prob - original_prob\n",
    "                    \n",
    "                    # Update heatmap\n",
    "                    y_end = min(y + self.stride, h)\n",
    "                    x_end = min(x + self.stride, w)\n",
    "                    heatmap[y:y_end, x:x_end] += score\n",
    "            \n",
    "            # Normalize and return\n",
    "            heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
    "            return heatmap.cpu().numpy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f92f2a19-3d17-44d9-a31b-f36f44615445",
   "metadata": {},
   "source": [
    "def load_model(model_path, device):\n",
    "    model = VGG16BinaryClassifier(pretrained=True)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device).eval()\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f27a9d0d-a39e-4f88-b766-47c841859a01",
   "metadata": {},
   "source": [
    "def visualize_occlusion(model, image_path, transform, window_size=32, stride=16):\n",
    "    # Load and process image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate heatmap\n",
    "    occlude = OcclusionSensitivity(model, window_size=window_size, stride=stride)\n",
    "    heatmap = occlude.generate_heatmap(input_tensor)\n",
    "    \n",
    "    # Process visualizations\n",
    "    image_np = np.array(image)\n",
    "    heatmap = cv2.resize(heatmap, image.size)\n",
    "    \n",
    "    # Create heatmap overlay\n",
    "    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_INFERNO)\n",
    "    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Blend with original image\n",
    "    blended = cv2.addWeighted(image_np, 0.7, heatmap_colored, 0.3, 0)\n",
    "    \n",
    "    return image, heatmap, heatmap_colored, blended"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9423405b-4dde-4389-94d2-d50b7c7c82f4",
   "metadata": {},
   "source": [
    "def visualize_multiple_occlusion(model, image_paths, transform, num_images=5, figsize=(20, 15), window_size=32, stride=16):\n",
    "    results = []\n",
    "    for img_path in image_paths[:num_images]:\n",
    "        results.append(visualize_occlusion(model, img_path, transform, window_size, stride=16))\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(num_images, 4, figsize=figsize)\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.2)\n",
    "    \n",
    "    for idx, (image, heatmap, heatmap_colored, blended) in enumerate(results):\n",
    "        axes[idx, 0].imshow(image)\n",
    "        axes[idx, 0].set_title(f\"Original {idx+1}\")\n",
    "        axes[idx, 0].axis('off')\n",
    "        \n",
    "        axes[idx, 1].imshow(heatmap, cmap='inferno')\n",
    "        axes[idx, 1].set_title(f\"Heatmap {idx+1}\")\n",
    "        axes[idx, 1].axis('off')\n",
    "        \n",
    "        axes[idx, 2].imshow(heatmap_colored)\n",
    "        axes[idx, 2].set_title(f\"Color Map {idx+1}\")\n",
    "        axes[idx, 2].axis('off')\n",
    "        \n",
    "        axes[idx, 3].imshow(blended)\n",
    "        axes[idx, 3].set_title(f\"Blended {idx+1}\")\n",
    "        axes[idx, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a6f11d95-6792-4c2d-9c16-ffcd85c9a5f6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# # Example Usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Load model\n",
    "#     model = load_model(MODEL_PATH, DEVICE)\n",
    "    \n",
    "#     # Define transforms\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((512, 512)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "    \n",
    "#     # Get sample images\n",
    "#     sample_images = [\n",
    "#         \"image1.jpg\",\n",
    "#         # \"image2.jpg\",\n",
    "#         # \"image3.jpg\",\n",
    "#         # \"image4.jpg\",\n",
    "#         # \"image5.jpg\"\n",
    "#         # Add your image paths here\n",
    "#     ]\n",
    "    \n",
    "#     # Generate visualizations\n",
    "#     results = visualize_multiple_occlusion(\n",
    "#         model=model,\n",
    "#         image_paths=sample_images,\n",
    "#         transform=transform,\n",
    "#         num_images=5,\n",
    "#         window_size=40,  # Adjust based on your image features\n",
    "#         stride=20\n",
    "#     )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "12b99e34-1070-48f8-8639-6c7ee8e0d798",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f3aee0ad-7323-4a9a-a986-73d8d0653ae1",
   "metadata": {},
   "source": [
    "# Process a directory of B-scans with occlusion sensitivity\n",
    "def process_bscans_with_occlusion(model, bscan_dir, output_dir, transform, window_size=32, stride=16):\n",
    "    \"\"\"\n",
    "    Process all B-scan images in a directory with occlusion sensitivity and save results\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        bscan_dir: Directory containing B-scan images\n",
    "        output_dir: Directory to save occlusion heatmaps\n",
    "        transform: Image transformation for model input\n",
    "        window_size: Size of occlusion window\n",
    "        stride: Stride for occlusion analysis\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create occlusion sensitivity analyzer\n",
    "    occluder = OcclusionSensitivity(model, window_size=window_size, stride=stride)\n",
    "    \n",
    "    # Get list of images\n",
    "    img_filenames = sorted([\n",
    "        f for f in os.listdir(bscan_dir)\n",
    "        if f.endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ])\n",
    "    \n",
    "    for img_name in tqdm(img_filenames, desc=\"Processing B-scans\"):\n",
    "        img_path = os.path.join(bscan_dir, img_name)\n",
    "        \n",
    "        # Load and process image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Generate heatmap\n",
    "        heatmap = occluder.generate_heatmap(input_tensor)\n",
    "        \n",
    "        # Process visualizations\n",
    "        image_np = np.array(image)\n",
    "        heatmap_resized = cv2.resize(heatmap, (image_np.shape[1], image_np.shape[0]))\n",
    "        \n",
    "        # Create heatmap overlay\n",
    "        heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_resized), cv2.COLORMAP_INFERNO)\n",
    "        heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Blend with original image\n",
    "        blended = cv2.addWeighted(image_np, 0.7, heatmap_colored, 0.3, 0)\n",
    "        \n",
    "        # Save results\n",
    "        output_path = os.path.join(output_dir, img_name)\n",
    "        cv2.imwrite(output_path, cv2.cvtColor(blended, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        # Save raw heatmap for further processing\n",
    "        heatmap_path = os.path.join(output_dir, f\"heatmap_{img_name}\")\n",
    "        cv2.imwrite(heatmap_path, np.uint8(255 * heatmap_resized))\n",
    "    \n",
    "    print(f\"✅ Processed {len(img_filenames)} B-scans with occlusion sensitivity\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "751d5bda-47d6-43f8-951d-97914bab7519",
   "metadata": {},
   "source": [
    "# Generate fovea probability map from occlusion heatmaps\n",
    "def generate_occlusion_fovea_map(enface_path, occlusions_dir, num_slices=128):\n",
    "    \"\"\"\n",
    "    Generate a fovea probability map from occlusion sensitivity heatmaps\n",
    "    \n",
    "    Args:\n",
    "        enface_path: Path to the enface image\n",
    "        occlusions_dir: Directory containing occlusion heatmap images\n",
    "        num_slices: Number of B-scan slices (default=128)\n",
    "        \n",
    "    Returns:\n",
    "        enface_img: Original enface image\n",
    "        prob_map: Generated probability map\n",
    "    \"\"\"\n",
    "    # Load enface image\n",
    "    enface_img = cv2.imread(enface_path)\n",
    "    enface_img = cv2.resize(enface_img, (512, 512))\n",
    "    \n",
    "    # Initialize heatmap canvas\n",
    "    prob_map = np.zeros((512, 512), dtype=np.float32)\n",
    "    y_positions = []\n",
    "    \n",
    "    # Read occlusion heatmap files\n",
    "    img_filenames = sorted([\n",
    "        f for f in os.listdir(occlusions_dir)\n",
    "        if f.startswith(\"1002\") and f.endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ])\n",
    "    \n",
    "    for img_name in img_filenames:\n",
    "        img_path = os.path.join(occlusions_dir, img_name)\n",
    "        heatmap_img = cv2.imread(img_path)\n",
    "        # heatmap_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if heatmap_img is None:\n",
    "            continue\n",
    "            \n",
    "        # Extract index from filename\n",
    "        match = re.search(r'bscan_(\\d+)', img_name)\n",
    "        if not match:\n",
    "            continue\n",
    "            \n",
    "        idx = int(match.group(1))\n",
    "        y_pos = int((idx / num_slices) * 512)\n",
    "        y_positions.append(y_pos)\n",
    "        \n",
    "        # Resize and potentially roll (adjust shift if needed)\n",
    "        heatmap_img = cv2.resize(heatmap_img, (512, 512))\n",
    "        heatmap_img = np.roll(heatmap_img, shift=-4, axis=1)  # Same as ScoreCam implementation\n",
    "        \n",
    "        # Collapse vertically to horizontal profile\n",
    "        profile = np.sum(heatmap_img, axis=0).astype(np.float32)\n",
    "        profile = gaussian_filter1d(profile, sigma=2)  # Smooth profile\n",
    "        if profile.max() > 0:\n",
    "            profile /= profile.max()  # Normalize\n",
    "            \n",
    "        # Add profile to map at y_pos\n",
    "        prob_map[y_pos, :] = profile\n",
    "    \n",
    "    # Fill above & below missing lines with 0\n",
    "    if y_positions:\n",
    "        y_min, y_max = min(y_positions), max(y_positions)\n",
    "        for y in range(0, y_min):\n",
    "            prob_map[y, :] = 0.0\n",
    "        for y in range(y_max + 1, 512):\n",
    "            prob_map[y, :] = 0.0\n",
    "    \n",
    "    # Smooth vertically\n",
    "    prob_map = gaussian_filter(prob_map, sigma=(1.5, 1.5))\n",
    "    prob_map /= prob_map.max() + 1e-6  # Normalize\n",
    "    \n",
    "    return enface_img, prob_map"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0525c6cc-4aed-4acc-8105-7ebd77e7fc67",
   "metadata": {},
   "source": [
    "# Visualize the generated occlusion fovea map\n",
    "def visualize_occlusion_fovea_map(enface_img, prob_map, output_path=None):\n",
    "    \"\"\"\n",
    "    Visualize the generated occlusion fovea map\n",
    "    \n",
    "    Args:\n",
    "        enface_img: Original enface image\n",
    "        prob_map: Generated probability map\n",
    "        output_path: Path to save the visualization (optional)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(enface_img, cmap='gray')\n",
    "    heat = plt.imshow(prob_map, cmap='jet', alpha=0.5)\n",
    "    cbar = plt.colorbar(heat, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(\"Fovea Probability\", fontsize=12)\n",
    "    cbar.set_ticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✅ Saved visualization to {output_path}\")\n",
    "    \n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "26a9aa82-29ea-4c0f-86a1-8945c2a3efc9",
   "metadata": {},
   "source": [
    "# Main function to execute the pipeline\n",
    "def main():\n",
    "    # Configuration\n",
    "    model_path = \"best_VGG_model_1.pth\"\n",
    "    bscan_dir = \"Data\"  # Directory containing B-scan images\n",
    "    bscan_dir = os.path.join(bscan_dir, \"diseased_eyes\", \"fovea_predictions\")\n",
    "\n",
    "    print(bscan_dir)\n",
    "     # Directory to save occlusion heatmaps\n",
    "    enface_path = \"enface.jpg\"  # Path to the enface image\n",
    "    # Define transforms\n",
    "    #\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    # Load model\n",
    "    model = load_model(model_path, device)\n",
    "\n",
    "    for category  in [\"Early\", \"GA\", \"Int\"]:\n",
    "        current_dir  = os.path.join(bscan_dir, category )\n",
    "        print(f\"\\nProcessing category: {category}\")\n",
    "        folders = [f for f in os.listdir(current_dir)\n",
    "                 if os.path.isdir(os.path.join(current_dir, f))]\n",
    "        for folder in folders:\n",
    "            occlusions_output_dir = \"img_masked_img\"\n",
    "            folder_path = os.path.join(current_dir, folder)\n",
    "            folder_path = os.path.join(folder_path, \"fovea\")\n",
    "\n",
    "            occlusions_output_dir = os.path.join(occlusions_output_dir, category )\n",
    "            occlusions_output_dir = os.path.join(occlusions_output_dir, folder)\n",
    "\n",
    "            os.makedirs(occlusions_output_dir, exist_ok=True)\n",
    "            # Process B-scans with occlusion sensitivity\n",
    "            process_bscans_with_occlusion(\n",
    "                model=model,\n",
    "                bscan_dir=folder_path,\n",
    "                output_dir=occlusions_output_dir,\n",
    "                transform=transform,\n",
    "                window_size=32,\n",
    "                stride=16\n",
    "            )\n",
    "\n",
    "    \n",
    "    # Generate and visualize fovea map\n",
    "    # enface_img, prob_map = generate_occlusion_fovea_map(\n",
    "    #     enface_path=enface_path,\n",
    "    #     occlusions_dir=occlusions_output_dir\n",
    "    # )\n",
    "    #\n",
    "    # # Visualize and save the result\n",
    "    # visualize_occlusion_fovea_map(\n",
    "    #     enface_img=enface_img,\n",
    "    #     prob_map=prob_map,\n",
    "    #     output_path=\"Occlusions_fovea_probability_map.png\"\n",
    "    # )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "599461ac-5541-4671-944a-90ce5a85cf22",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62843794-1c3a-4e5b-8a72-e7c380c0a2a7",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from scipy.interpolate import griddata\n",
    "from tqdm import tqdm\n",
    "\n",
    "def map_bscan_heatmaps_to_enface(bscan_heatmaps, enface_image, pattern_type='foveal', \n",
    "                                 num_scans=None, smooth_factor=5, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Map B-scan heatmaps onto an enface image with thresholding to isolate bright regions\n",
    "    \n",
    "    Args:\n",
    "        bscan_heatmaps: List of numpy arrays containing heatmaps from OCT B-scans\n",
    "        enface_image: Numpy array of the enface image\n",
    "        pattern_type: The scan pattern - 'foveal', 'raster', or 'radial'\n",
    "        num_scans: Number of B-scans (if None, uses len(bscan_heatmaps))\n",
    "        smooth_factor: Size of Gaussian kernel for final smoothing\n",
    "        threshold: Value (0-1) to threshold the heatmap, only keeping values above this\n",
    "        \n",
    "    Returns:\n",
    "        The enface image with overlaid heatmap (only bright regions)\n",
    "    \"\"\"\n",
    "    # Make sure enface is RGB\n",
    "    if len(enface_image.shape) == 2:\n",
    "        enface_rgb = cv2.cvtColor(enface_image, cv2.COLOR_GRAY2RGB)\n",
    "    else:\n",
    "        enface_rgb = enface_image.copy()\n",
    "    \n",
    "    h, w = enface_rgb.shape[:2]\n",
    "    \n",
    "    # If num_scans not provided, use length of heatmaps\n",
    "    if num_scans is None:\n",
    "        num_scans = len(bscan_heatmaps)\n",
    "    \n",
    "    # Initialize points and values for interpolation\n",
    "    points = []\n",
    "    values = []\n",
    "    \n",
    "    # Generate scan coordinates based on pattern type\n",
    "    if pattern_type == 'foveal':\n",
    "        # Foveal scan pattern (all B-scans pass through center/fovea)\n",
    "        center_x, center_y = w // 2, h // 2\n",
    "        \n",
    "        # Distribute angles evenly\n",
    "        angles = np.linspace(0, np.pi, num_scans)  # 180 degrees coverage\n",
    "        \n",
    "        for i, angle in enumerate(angles):\n",
    "            if i >= len(bscan_heatmaps):\n",
    "                break\n",
    "                \n",
    "            heatmap = bscan_heatmaps[i]\n",
    "            \n",
    "            # Calculate line endpoints (line passing through center)\n",
    "            radius = min(w, h) // 2 - 5  # Slight margin from edge\n",
    "            \n",
    "            # Both ends of the line (passing through center)\n",
    "            x1 = int(center_x + radius * np.cos(angle))\n",
    "            y1 = int(center_y + radius * np.sin(angle))\n",
    "            x2 = int(center_x - radius * np.cos(angle))\n",
    "            y2 = int(center_y - radius * np.sin(angle))\n",
    "            \n",
    "            # Create the line\n",
    "            line_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "            cv2.line(line_mask, (x1, y1), (x2, y2), 255, 1)\n",
    "            line_y, line_x = np.where(line_mask > 0)\n",
    "            \n",
    "            # Skip if no points\n",
    "            if len(line_x) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Resize heatmap to match line length\n",
    "            num_points = len(line_x)\n",
    "            resized_heatmap = cv2.resize(heatmap, (1, num_points), interpolation=cv2.INTER_LINEAR)\n",
    "            \n",
    "            # Add points and values\n",
    "            for j, (x, y) in enumerate(zip(line_x, line_y)):\n",
    "                if resized_heatmap[j, 0] > 0.05:  # Threshold for computation efficiency\n",
    "                    points.append([x, y])\n",
    "                    values.append(resized_heatmap[j, 0])\n",
    "    \n",
    "    # Add other pattern types as needed (raster, radial, etc.)\n",
    "    \n",
    "    # Create grid for interpolation\n",
    "    grid_x, grid_y = np.meshgrid(np.arange(w), np.arange(h))\n",
    "    \n",
    "    # Check if we have enough points for interpolation\n",
    "    if len(points) < 4:\n",
    "        print(\"Warning: Not enough points for interpolation. Check your heatmaps.\")\n",
    "        return enface_rgb\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    points = np.array(points)\n",
    "    values = np.array(values)\n",
    "    \n",
    "    # Interpolate values onto the grid\n",
    "    grid_z = griddata(points, values, (grid_x, grid_y), method='linear', fill_value=0)\n",
    "    \n",
    "    # Smooth the result\n",
    "    if smooth_factor > 0:\n",
    "        grid_z = cv2.GaussianBlur(grid_z, (smooth_factor, smooth_factor), 0)\n",
    "    \n",
    "    # Normalize the heatmap\n",
    "    grid_z = (grid_z - grid_z.min()) / (grid_z.max() - grid_z.min() + 1e-8)\n",
    "    \n",
    "    # Apply threshold to keep only bright regions\n",
    "    grid_z[grid_z < threshold] = 0\n",
    "    \n",
    "    # Optional: Renormalize after thresholding for better contrast\n",
    "    if np.max(grid_z) > 0:  # Check to avoid division by zero\n",
    "        grid_z = (grid_z - grid_z.min()) / (grid_z.max() - grid_z.min() + 1e-8)\n",
    "    \n",
    "    # Apply colormap to create colored heatmap\n",
    "    heatmap_colored = cv2.applyColorMap(np.uint8(255 * grid_z), cv2.COLORMAP_HOT)\n",
    "    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Create a mask for overlay (alpha channel)\n",
    "    alpha_mask = (grid_z > 0).astype(np.float32)\n",
    "    alpha_mask = cv2.GaussianBlur(alpha_mask, (5, 5), 0)  # Smooth the edges\n",
    "    \n",
    "    # Create result image\n",
    "    result = enface_rgb.copy()\n",
    "    \n",
    "    # Only blend where the mask is non-zero\n",
    "    for c in range(3):  # RGB channels\n",
    "        result[:,:,c] = enface_rgb[:,:,c] * (1 - alpha_mask) + heatmap_colored[:,:,c] * alpha_mask\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_enface_heatmap_with_threshold(heatmap_dir, enface_path, output_path=None, \n",
    "                                         pattern_type='foveal', smooth_factor=5, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Process enface image with thresholded heatmap overlay\n",
    "    \n",
    "    Args:\n",
    "        heatmap_dir: Directory containing heatmap images\n",
    "        enface_path: Path to the enface image\n",
    "        output_path: Path to save the result (optional)\n",
    "        pattern_type: Scan pattern type ('foveal', 'raster', 'radial')\n",
    "        smooth_factor: Size of Gaussian blur kernel for smoothing\n",
    "        threshold: Value (0-1) to threshold the heatmap, only showing bright regions\n",
    "        \n",
    "    Returns:\n",
    "        The resulting enface image with overlaid thresholded heatmap\n",
    "    \"\"\"\n",
    "    # Load heatmaps\n",
    "    heatmap_files = sorted([f for f in os.listdir(heatmap_dir) \n",
    "                          if f.startswith('heatmap_') or f.endswith('.png') or f.endswith('.jpg')])\n",
    "    \n",
    "    bscan_heatmaps = []\n",
    "    for heatmap_file in tqdm(heatmap_files, desc=\"Loading heatmaps\"):\n",
    "        path = os.path.join(heatmap_dir, heatmap_file)\n",
    "        heatmap = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if heatmap is None:\n",
    "            print(f\"Warning: Could not load {path}\")\n",
    "            continue\n",
    "            \n",
    "        # Normalize\n",
    "        heatmap = heatmap.astype(np.float32) / 255.0\n",
    "        bscan_heatmaps.append(heatmap)\n",
    "    \n",
    "    print(f\"Loaded {len(bscan_heatmaps)} heatmap images\")\n",
    "    \n",
    "    if len(bscan_heatmaps) == 0:\n",
    "        print(\"No heatmaps found. Check your directory.\")\n",
    "        return None\n",
    "    \n",
    "    # Load enface image\n",
    "    enface_image = cv2.imread(enface_path)\n",
    "    if enface_image is None:\n",
    "        print(f\"Could not load enface image: {enface_path}\")\n",
    "        return None\n",
    "        \n",
    "    enface_image = cv2.cvtColor(enface_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Map heatmaps to enface with threshold\n",
    "    result = map_bscan_heatmaps_to_enface(\n",
    "        bscan_heatmaps, \n",
    "        enface_image, \n",
    "        pattern_type=pattern_type, \n",
    "        smooth_factor=smooth_factor,\n",
    "        threshold=threshold\n",
    "    )\n",
    "    \n",
    "    # Save if output path provided\n",
    "    if output_path:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        # Make sure the file has a valid extension\n",
    "        valid_extensions = ['.png', '.jpg', '.jpeg', '.tiff', '.bmp']\n",
    "        file_ext = os.path.splitext(output_path)[1].lower()\n",
    "        \n",
    "        if file_ext not in valid_extensions:\n",
    "            # Default to PNG if extension is invalid\n",
    "            output_path = output_path + '.png'\n",
    "            print(f\"Added .png extension to output path: {output_path}\")\n",
    "            \n",
    "        cv2.imwrite(output_path, cv2.cvtColor(result, cv2.COLOR_RGB2BGR))\n",
    "        print(f\"Saved result to: {output_path}\")\n",
    "    \n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    # Configuration\n",
    "    model_path = \"best_VGG_model_1.pth\"\n",
    "    bscan_dir = os.path.join(\"Data\", \"diseased_eyes\")\n",
    "    print(f\"Base directory: {bscan_dir}\")\n",
    "\n",
    "    occlusions_output_dir = \"diseased_img_masked_img\"\n",
    "    enface_path = \"enface.jpg\"\n",
    "\n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Load model\n",
    "    model = load_model(model_path, device)\n",
    "\n",
    "    # Process each category\n",
    "    for category in [\"Early\", \"GA\", \"Int\"]:\n",
    "        current_dir = os.path.join(bscan_dir, category)\n",
    "        print(f\"\\nProcessing category: {category}\")\n",
    "\n",
    "        # Check if directory exists\n",
    "        if not os.path.exists(current_dir):\n",
    "            print(f\"Directory {current_dir} does not exist\")\n",
    "            continue\n",
    "\n",
    "        # List folders in current directory\n",
    "        folders = [f for f in os.listdir(current_dir)\n",
    "                   if os.path.isdir(os.path.join(current_dir, f))]\n",
    "\n",
    "        print(f\"Found {len(folders)} subdirectories:\")\n",
    "        for folder in folders:\n",
    "            print(f\" - {folder}\")\n",
    "\n",
    "        # Create output directory\n",
    "        category_output_dir = os.path.join(occlusions_output_dir, category)\n",
    "        os.makedirs(category_output_dir, exist_ok=True)\n",
    "\n",
    "        # Process B-scans\n",
    "        process_bscans_with_occlusion(\n",
    "            model=model,\n",
    "            bscan_dir=current_dir,\n",
    "            output_dir=category_output_dir,\n",
    "            transform=transform,\n",
    "            window_size=32,\n",
    "            stride=16\n",
    "        )\n",
    "\n",
    "    # Visualization code would follow...\n"
   ],
   "id": "e1d00c5196f670f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8f32f4b4-2700-42e2-82e3-9a40c0b6c278",
   "metadata": {},
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "26c4f5c6-c634-4c90-b43f-aeaf180d69d6",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
