{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Training Supervised Model for B-scan slices binary classification",
   "id": "72f1e3e296b5eb2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset,random_split\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm"
   ],
   "id": "fed7c5c98d7aadf0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "if not torch.backends.mps.is_available():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "549290272d4b53df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# FOR Supervised Model\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, color, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.color = color\n",
    "        self.data = []\n",
    "\n",
    "        for label, folder in enumerate([\"fovea_no\", \"fovea_yes\"]):\n",
    "            folder_path = os.path.join(root_dir, folder)\n",
    "            for filename in os.listdir(folder_path):\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                if os.path.isfile(img_path):\n",
    "                    self.data.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        image = Image.open(img_path).convert(self.color)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class DataLoaderModule:\n",
    "    def __init__(self, root_dir, color, batch_size=32, test_split=0.2):\n",
    "        self.root_dir = root_dir\n",
    "        self.color = color\n",
    "        self.batch_size = batch_size\n",
    "        self.test_split = test_split\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),  # 50% chance of horizontal flip\n",
    "            transforms.RandomRotation(degrees=(-10, 10)),  # Random rotation between -10 to 10 degrees\n",
    "            transforms.ToTensor()  # Convert images to tensors\n",
    "        ])\n",
    "\n",
    "        self.dataset = CustomDataset(root_dir, color, transform=self.transform)\n",
    "        self.train_loader, self.test_loader = self.split_dataset()\n",
    "\n",
    "    def split_dataset(self):\n",
    "        print(f\"total dataset: {len(self.dataset)}\")\n",
    "        test_size = int(len(self.dataset) * self.test_split)\n",
    "        train_size = len(self.dataset) - test_size\n",
    "        train_set, test_set = random_split(self.dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "        train_loader = DataLoader(train_set, batch_size=self.batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_set, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        print(f\"Train set size: {train_size}\")\n",
    "        print(f\"Test set size: {test_size}\")\n",
    "\n",
    "        return train_loader, test_loader"
   ],
   "id": "268fcee5d2375a19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class VGG16BinaryClassifier(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(VGG16BinaryClassifier, self).__init__()\n",
    "\n",
    "        # Load VGG-16 model\n",
    "        self.vgg16 = models.vgg16(pretrained=pretrained)\n",
    "\n",
    "        # Freeze convolutional layers (optional)\n",
    "        for param in self.vgg16.features.parameters():\n",
    "            param.requires_grad = True  # Set to True if you want to fine-tune\n",
    "\n",
    "        # Modify the classifier head for binary classification\n",
    "        self.vgg16.classifier = nn.Sequential(\n",
    "            nn.Linear(25088, 4096),  # VGG-16's default input size\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 512),  # Custom hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 1)  # Output 1 logit (for binary classification)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vgg16(x)  # No sigmoid, return raw logits\n",
    "\n",
    "# Instantiate the model\n",
    "vgg_model = VGG16BinaryClassifier(pretrained=True)\n",
    "print(vgg_model)"
   ],
   "id": "b03812fdf2077a25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_training_results(train_loss, val_loss, train_acc, val_acc, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Plots training/validation loss and confusion matrices for training and validation data.\n",
    "\n",
    "    Parameters:\n",
    "    - train_loss (list): List of training loss per epoch.\n",
    "    - val_loss (list): List of validation loss per epoch.\n",
    "    - y_train_true (array-like): True labels for training data.\n",
    "    - y_train_pred (array-like): Predicted labels for training data.\n",
    "    - y_val_true (array-like): True labels for validation data.\n",
    "    - y_val_pred (array-like): Predicted labels for validation data.\n",
    "    - class_names (list): List of class names for confusion matrix.\n",
    "\n",
    "    Returns:\n",
    "    - A single figure with three subplots.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    # Create figure with 3 subplots (1 row, 3 columns)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # 1️⃣ Plot Train & Validation Loss\n",
    "    axes[0].plot(epochs, train_loss, label=\"Train Loss\", marker='o')\n",
    "    axes[0].plot(epochs, val_loss, label=\"Validation Loss\", marker='s')\n",
    "    axes[0].set_title(\"Training & Validation Loss\")\n",
    "    axes[0].set_xlabel(\"Epochs\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # 2️⃣ Plot Train Accuracy\n",
    "    axes[1].plot(epochs, train_acc, label=\"Train Acc\", marker='o')\n",
    "    axes[1].plot(epochs, val_acc, label=\"Validation ACC\", marker='s')\n",
    "    axes[1].set_title(\"Training & Validation Accuracy\")\n",
    "    axes[1].set_xlabel(\"Epochs\")\n",
    "    axes[1].set_ylabel(\"Accuracy\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    # 3️⃣ Validation Confusion Matrix\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Oranges\", xticklabels=[\"healthy_no\", \"healthy_yes\"], yticklabels=[\"healthy_no\", \"healthy_yes\"], ax=axes[2])\n",
    "    axes[2].set_title(\"Confusion Matrix\")\n",
    "    axes[2].set_xlabel(\"Predicted\")\n",
    "    axes[2].set_ylabel(\"Actual\")\n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "94a78d3bc0c13543"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs, lr, checkpoint_path=\"best_CNN_model.pth\"):\n",
    "    # Define loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    model.to(device)\n",
    "\n",
    "    # Store losses and accuracy\n",
    "    train_loss_history, val_loss_history = [], []\n",
    "    train_acc_history, val_acc_history = [], []\n",
    "\n",
    "    best_acc = 0  \n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_epoch_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_epoch_loss += loss.item()\n",
    "\n",
    "            # Store predictions\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "\n",
    "            # Update accuracy counts\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_train_loss = train_epoch_loss / len(train_loader)\n",
    "        train_accuracy = correct / total\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "        train_acc_history.append(train_accuracy)\n",
    "\n",
    "        # Validation phase (no gradient computation)\n",
    "        model.eval()\n",
    "        val_epoch_loss, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader):\n",
    "                images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_epoch_loss += loss.item()\n",
    "\n",
    "                # Store predictions\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "                # Collect predictions and labels for confusion matrix\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "\n",
    "        avg_val_loss = val_epoch_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "        val_acc_history.append(val_accuracy)\n",
    "\n",
    "        # Concatenate all predictions and labels\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "\n",
    "        # Print training progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "        # Save the best model (based on validation loss)\n",
    "        if val_accuracy > best_acc:\n",
    "            best_acc = val_accuracy\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Checkpoint saved at {checkpoint_path} (Best Val Loss: {best_acc:.4f})\")\n",
    "\n",
    "    print(\"## Training complete! ##\")\n",
    "    return train_loss_history, val_loss_history, train_acc_history, val_acc_history, all_preds.numpy(), all_labels.numpy()"
   ],
   "id": "c244f5dd34a3da18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Data Engineering\n",
    "batch_size = 16\n",
    "root_path = \"../dataset\"\n",
    "data_module = DataLoaderModule(root_dir=root_path, color = \"RGB\", batch_size=batch_size)\n",
    "train_loader, test_loader = data_module.train_loader, data_module.test_loader"
   ],
   "id": "7411831af46e2f03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# VGG16\n",
    "num_epochs = 10\n",
    "lr = 0.0001\n",
    "\n",
    "# Training\n",
    "train_loss_history, val_loss_history, train_acc, val_acc, preds, labels = train_model(vgg_model, train_loader, test_loader, num_epochs, lr, \"best_VGG_model_1.pth\")\n",
    "plot_training_results(train_loss_history, val_loss_history, train_acc, val_acc,preds, labels)"
   ],
   "id": "6320e7a4ef49ca5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test model",
   "id": "392f2d30b51fa75d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import shutil"
   ],
   "id": "2abb443ffcf7b1b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "if not torch.backends.mps.is_available():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "ac37c786b1267bb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)  # Set seed for PyTorch\n",
    "    np.random.seed(seed)  # Set seed for NumPy\n",
    "    random.seed(seed)  # Set seed for Python's random module\n",
    "\n",
    "    # Ensure reproducibility for GPU (MPS for Mac)\n",
    "    torch.mps.manual_seed(seed)  # For MPS (Metal on Mac)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a fixed seed\n",
    "set_seed(42)"
   ],
   "id": "60d3b8ac72eb5c8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Custom dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_path, transform=None):\n",
    "        self.image_paths = []\n",
    "        self.file_names = []\n",
    "        self.transform = transform\n",
    "\n",
    "        for root, _, files in os.walk(image_path):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff')):\n",
    "                    self.image_paths.append(os.path.join(root, file))\n",
    "                    self.file_names.append(file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        file_name = self.file_names[idx]\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # Convert to grayscale\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, file_name  # Return both image tensor & filename\n",
    "  \n",
    "def get_transforms():\n",
    "    return transforms.Compose([\n",
    "          transforms.Resize((224, 224)),\n",
    "          transforms.RandomHorizontalFlip(p=0.5),  # 50% chance of horizontal flip\n",
    "          transforms.RandomRotation(degrees=(-10, 10)),  # Random rotation between -10 to 10 degrees\n",
    "          transforms.ToTensor()  # Convert images to tensors\n",
    "    ])\n",
    "\n",
    "# Preprocess images & return DataLoader\n",
    "def preprocess_image(image_path, batch_size=16):\n",
    "    transform = get_transforms()\n",
    "    dataset = ImageDataset(image_path, transform=transform)\n",
    "    print(f\"Total images: {len(dataset)}\")\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return test_loader  # Returning DataLoader"
   ],
   "id": "28475063af80b393"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class VGG16BinaryClassifier(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(VGG16BinaryClassifier, self).__init__()\n",
    "\n",
    "        # Load VGG-16 model\n",
    "        self.vgg16 = models.vgg16(pretrained=pretrained)\n",
    "\n",
    "        # Freeze convolutional layers (optional)\n",
    "        for param in self.vgg16.features.parameters():\n",
    "            param.requires_grad = True  # Set to True if you want to fine-tune\n",
    "\n",
    "        # Modify the classifier head for binary classification\n",
    "        self.vgg16.classifier = nn.Sequential(\n",
    "            nn.Linear(25088, 4096),  # VGG-16's default input size\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 512),  # Custom hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 1)  # Output 1 logit (for binary classification)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vgg16(x)  # No sigmoid, return raw logits\n",
    "\n",
    "# Function to load the trained model\n",
    "def load_model(model_path, device):\n",
    "    # Instantiate the model\n",
    "    model = VGG16BinaryClassifier(pretrained=True)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(model)\n",
    "    return model\n"
   ],
   "id": "96e805b9461536d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def model_prediction(model, test_loader, save_txt_path, source_dir, dest_dir):\n",
    "    # Open file for writing predictions\n",
    "    dest_dir_fovea = dest_dir+\"predictions/fovea\"\n",
    "    dest_dir_no_fovea = dest_dir+\"predictions/no_fovea\"\n",
    "    os.makedirs(dest_dir_fovea, exist_ok=True)\n",
    "    os.makedirs(dest_dir_no_fovea, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad(), open(save_txt_path, \"w\") as f: # changed to \"w\" from \"a\" ; this is to overwrite the old file and help maintain the latest copy of the classification output\n",
    "        for images, filenames in tqdm(test_loader):\n",
    "            images = images.to(device)  \n",
    "\n",
    "            # Model inference\n",
    "            outputs = model(images)\n",
    "            predicted_labels = torch.sigmoid(outputs).round().int()  # Convert logits to binary labels\n",
    "\n",
    "            # Save results\n",
    "            fovea = 0\n",
    "\n",
    "            for filename, label in zip(filenames, predicted_labels.cpu().numpy()):\n",
    "                f.write(f\"{filename} {label}\\n\")\n",
    "                src_path = os.path.join(source_dir, filename)\n",
    "                if label == 1:\n",
    "                    dest_path = os.path.join(dest_dir_fovea, filename)\n",
    "                    fovea += 1\n",
    "                else:\n",
    "                    dest_path = os.path.join(dest_dir_no_fovea, filename)\n",
    "                shutil.copy(src_path, dest_path)\n",
    "\n",
    "            print(f\"Batch Summary: fovea {fovea} images\")\n",
    "\n",
    "\n",
    "def show_result(volume_path, save_txt_path, max_images=20):\n",
    "    # Read the saved text file\n",
    "    with open(save_txt_path, \"r\") as f:\n",
    "        class_1_images = [line.rsplit(maxsplit=1) for line in f.readlines()]\n",
    "\n",
    "    # Function to extract squence number from img_name\n",
    "    def extract_serial_number(img_name):\n",
    "        return img_name.split(\"_\")[-1].split(\".\")[0]  # Extract last part before \".jpg\"\n",
    "\n",
    "    # Convert labels to integers correctly and filter images where label == 1\n",
    "    class_1 = [(img_name, extract_serial_number(img_name)) for img_name, label in class_1_images if int(label.strip(\"[]\")) == 1]\n",
    "\n",
    "    num_images_to_show = len(class_1)\n",
    "    images_per_row = 5\n",
    "    num_rows = math.ceil(num_images_to_show / images_per_row)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, images_per_row, figsize=(10, 3 * num_rows))\n",
    "    axes = axes.flatten()  # Flatten to make indexing easier\n",
    "\n",
    "    for idx, (img_name, serial_number) in enumerate(class_1[:num_images_to_show]):\n",
    "        img = Image.open(os.path.join(volume_path, img_name))  # Load image\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].axis(\"off\")\n",
    "        axes[idx].set_title(f\"SN: {serial_number}\")  # Display sequence number as title\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for idx in range(num_images_to_show, len(axes)):\n",
    "        axes[idx].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "20584b8c54b0a9b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_path = \"best_VGG_model_1.pth\"\n",
    "model = load_model(model_path,device)"
   ],
   "id": "fc37b07fd7076b53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5107a95dd21c2901"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GradCam++",
   "id": "5f8088421a0c67f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define GradCAM++ class\n",
    "class GradCAMpp:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "\n",
    "        # Placeholders for activations and gradients\n",
    "        self.activations = None\n",
    "        self.gradients = None\n",
    "\n",
    "        # Register hooks to capture activations and gradients\n",
    "        self.target_layer.register_forward_hook(self.save_activation)\n",
    "        self.target_layer.register_backward_hook(self.save_gradient)\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def generate_cam(self, input_tensor, target_class=None):\n",
    "        # Forward pass through the model\n",
    "        output = self.forward_pass(input_tensor)\n",
    "\n",
    "        # If no target class is provided, use the class with the highest score\n",
    "        if target_class is None:\n",
    "            target_class = torch.argmax(output).item()\n",
    "\n",
    "        # Backward pass for the specific target class\n",
    "        one_hot = torch.zeros_like(output)\n",
    "        one_hot[0][target_class] = 1\n",
    "        output.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "        # Compute weights using GradCAM++ formula\n",
    "        gradients = self.gradients.cpu().data.numpy()[0]\n",
    "        activations = self.activations.cpu().data.numpy()[0]\n",
    "\n",
    "        weights = np.zeros(gradients.shape[1:], dtype=np.float32)\n",
    "\n",
    "        for i in range(gradients.shape[0]):\n",
    "            grad_channel = gradients[i]\n",
    "            alpha_num = grad_channel.mean()\n",
    "            alpha_denom = 2 * grad_channel.std() + 1e-10  # Avoid division by zero\n",
    "\n",
    "            alpha = alpha_num / alpha_denom  # GradCAM++ weighting formula\n",
    "            weights += alpha * np.maximum(grad_channel, 0)\n",
    "\n",
    "        cam = np.sum(weights * activations, axis=0)\n",
    "\n",
    "        # Normalize CAM to [0, 1]\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam -= cam.min()\n",
    "        cam /= cam.max() + 1e-8\n",
    "\n",
    "        return cam\n",
    "\n",
    "# Visualization helper function\n",
    "def visualize_cam(cam, original_image):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    heatmap = cv2.resize(heatmap, (original_image.size[0], original_image.size[1]))\n",
    "    overlayed_image = np.array(original_image) * 0.5 + heatmap * 0.5\n",
    "    return heatmap, overlayed_image.astype(np.uint8)\n"
   ],
   "id": "147cc9e2fad6b075"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load a pre-trained VGG16 model and set it to evaluation mode\n",
    "model = models.vgg16(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Specify the target layer for GradCAM++\n",
    "target_layer = model.features[-1]  # Last convolutional layer of VGG16\n",
    "#\n",
    "# # Initialize GradCAM++\n",
    "# gradcampp = GradCAMpp(model=model, target_layer=target_layer)\n",
    "# # Define preprocessing transformations for the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load and preprocess an image\n",
    "image_path = \"/home/shreyas/PycharmProjects/Project-in-Biomedical-AI/heatmap_code/predictions/fovea_yes/predictions/fovea/2002000096_20220623_111500_OD_Carl_Zeiss_Meditec_5000_512x1024x128_ORG_IMG_JPG_064.jpg\"\n",
    "original_image = Image.open(image_path).convert('RGB')\n",
    "input_tensor = transform(original_image).unsqueeze(0)  # Add batch dimension\n",
    "input_tensor.requires_grad_(True)  # Enable gradient computation if needed\n",
    "\n",
    "input_tensor = input_tensor.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "model.to(input_tensor.device)\n"
   ],
   "id": "877bfc479c631ba7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Generate CAM for a specific class (e.g., class index 10)\n",
    "target_class_index = None  # Automatically infer class if None\n",
    "# Example: Experimenting with target layers in VGG16\n",
    "target_layers = [\n",
    "    model.features[-1],   # Last convolutional layer\n",
    "    model.features[24],   # Intermediate convolutional layer\n",
    "    model.features[17]    # Earlier convolutional layer\n",
    "]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "original_image = Image.open(image_path).convert('RGB')\n",
    "for i, layer in enumerate(target_layers):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    print(f\"Testing target layer {i}: {layer}\")\n",
    "    gradcampp = GradCAMpp(model, target_layer=layer)\n",
    "    cam = gradcampp.generate_cam(input_tensor, target_class=target_class_index)\n",
    "    heatmap, blended_image = visualize_cam(cam, original_image)\n",
    "    plt.imshow(blended_image)\n",
    "    plt.title(f\"Layer {i}\")\n",
    "plt.show()\n",
    "# cam = gradcampp.generate_cam(input_tensor=input_tensor, target_class=target_class_index)\n",
    "#\n",
    "# # Visualize the CAM overlayed on the original image\n",
    "# heatmap, overlayed_image = visualize_cam(cam, original_image)\n",
    "#\n",
    "# # Display results using Matplotlib\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.subplot(1, 3, 1)\n",
    "# plt.title(\"Original Image\")\n",
    "# plt.imshow(original_image)\n",
    "# plt.axis('off')\n",
    "#\n",
    "# plt.subplot(1, 3, 2)\n",
    "# plt.title(\"GradCAM++ Heatmap\")\n",
    "# plt.imshow(heatmap)\n",
    "# plt.axis('off')\n",
    "#\n",
    "# plt.subplot(1, 3, 3)\n",
    "# plt.title(\"Overlayed Image\")\n",
    "# plt.imshow(overlayed_image)\n",
    "# plt.axis('off')\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ],
   "id": "30eecb6b5b89ced"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c7cde865500396ee"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
