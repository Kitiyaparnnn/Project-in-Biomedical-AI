{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2190f508",
   "metadata": {},
   "source": [
    "#### Extending code from zzEnface2Enface.ipybnb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8783af",
   "metadata": {},
   "source": [
    "## Populate New Directory, enface_legend\n",
    "\n",
    "## NOT applying mask yet in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c988676",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r\"C:\\Users\\micah\\OneDrive\\Gmail OneDrive\\Desktop\\CMU\\_Spring 2025\\Projects in BME\\Fovea Detection\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# === SET YOUR ACTUAL BASE PATH HERE ===\n",
    "base_dir = r\"C:\\Users\\micah\\OneDrive\\Gmail OneDrive\\Desktop\\CMU\\_Spring 2025\\Projects in BME\\Fovea Detection\"\n",
    "src_masked_root = os.path.join(base_dir, \"enface_images\", \"img_masked_img\")\n",
    "src_raw_root = os.path.join(base_dir, \"predictions\")\n",
    "dst_root = os.path.join(base_dir, \"enface_legend\")\n",
    "\n",
    "# === Ensure destination root exists ===\n",
    "os.makedirs(dst_root, exist_ok=True)\n",
    "\n",
    "# === Patient folders ===\n",
    "patient_folders = sorted([d for d in os.listdir(src_masked_root) if os.path.isdir(os.path.join(src_masked_root, d))])\n",
    "\n",
    "# === Copy logic ===\n",
    "for folder in patient_folders:\n",
    "    dst_folder = os.path.join(dst_root, folder)\n",
    "    dst_masked = os.path.join(dst_folder, \"enface mask\")\n",
    "    dst_raw = os.path.join(dst_folder, \"raw enface\")\n",
    "\n",
    "    os.makedirs(dst_masked, exist_ok=True)\n",
    "    os.makedirs(dst_raw, exist_ok=True)\n",
    "\n",
    "    # Get enface PNG from masked folder\n",
    "    src_masked_folder = os.path.join(src_masked_root, folder)\n",
    "    enface_png = next((f for f in os.listdir(src_masked_folder) if \"enface\" in f.lower() and f.lower().endswith(\".png\")), None)\n",
    "    if enface_png:\n",
    "        shutil.copy(os.path.join(src_masked_folder, enface_png), os.path.join(dst_masked, enface_png))\n",
    "    else:\n",
    "        print(f\"❌ No enface PNG found in {src_masked_folder}\")\n",
    "\n",
    "    # Get JPG from predictions folder\n",
    "    src_raw_folder = os.path.join(src_raw_root, folder)\n",
    "    if os.path.exists(src_raw_folder):\n",
    "        raw_jpg = next((f for f in os.listdir(src_raw_folder) if f.lower().endswith(\".jpg\")), None)\n",
    "        if raw_jpg:\n",
    "            shutil.copy(os.path.join(src_raw_folder, raw_jpg), os.path.join(dst_raw, raw_jpg))\n",
    "        else:\n",
    "            print(f\"❌ No JPG in {src_raw_folder}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Predictions folder missing for {folder}\")\n",
    "\n",
    "print(\"✅ enface_legend structure created and populated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7be978c",
   "metadata": {},
   "source": [
    "### Filter out poor enface images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9789c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# === Updated ignore list ===\n",
    "ignored_folders = [\n",
    "    \"1002_right\", \"1003_left\", \"1004_left\", \"1005_right\", \"1006_right\",\n",
    "    \"1007_left\", \"1009_right\", \"1011_right\", \"1012_left\", \"1013_left\",\n",
    "    \"1013_right\", \"1014_left\", \"1014_right\", \"1015_right\", \"1016_left\",\n",
    "    \"1016_right\", \"1017_right\", \"1019_right\", \"1021_right\", \"1022_left\",\n",
    "    \"1023_left\", \"1023_right\", \"1024_right\", \"1025_left\", \"1026_left\",\n",
    "    \"1027_left\", \"1027_right\", \"1028_left\", \"1029_left\", \"1029_right\",\n",
    "    \"1030_left\", \"1030_right\", \"1031_right\"\n",
    "]\n",
    "\n",
    "# === Paths ===\n",
    "base_dir = r\"C:\\Users\\micah\\OneDrive\\Gmail OneDrive\\Desktop\\CMU\\_Spring 2025\\Projects in BME\\Fovea Detection\"\n",
    "src_legend_root = os.path.join(base_dir, \"enface_legend\")\n",
    "dst_curated_root = os.path.join(base_dir, \"curated_enface\")\n",
    "\n",
    "# === Recreate curated_enface directory ===\n",
    "if os.path.exists(dst_curated_root):\n",
    "    shutil.rmtree(dst_curated_root)\n",
    "os.makedirs(dst_curated_root, exist_ok=True)\n",
    "\n",
    "# === Copy allowed folders ===\n",
    "for folder in os.listdir(src_legend_root):\n",
    "    src_path = os.path.join(src_legend_root, folder)\n",
    "    dst_path = os.path.join(dst_curated_root, folder)\n",
    "    if os.path.isdir(src_path) and folder not in ignored_folders:\n",
    "        shutil.copytree(src_path, dst_path)\n",
    "\n",
    "# === Summary ===\n",
    "print(\"✅ curated_enface recreated successfully.\")\n",
    "print(f\"Ignored {len(ignored_folders)} folders:\")\n",
    "print(ignored_folders)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3195b23",
   "metadata": {},
   "source": [
    "# ViT based MAEncoder\n",
    "### see newest code. we are now using relative positional embeddings. not using multi-cropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5fe92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import os\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# import random\n",
    "\n",
    "# THRESH = 0.75\n",
    "\n",
    "# # =============== Dataset Loader with Full Augmentations ===============\n",
    "# class EnfaceMaskedDataset(Dataset):\n",
    "#     def __init__(self, root_dir, image_size=512):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.image_size = image_size\n",
    "#         self.patients = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "#         self.augment = transforms.Compose([\n",
    "#             transforms.RandomAffine(degrees=15, translate=(0.2, 0.2), scale=(0.8, 1.2)),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "#             transforms.RandomVerticalFlip()\n",
    "#         ])\n",
    "#         self.to_tensor = transforms.ToTensor()\n",
    "#         self.resize = transforms.Resize((image_size, image_size))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.patients)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         patient = self.patients[idx]\n",
    "#         enface_path = os.path.join(self.root_dir, patient, 'raw enface')\n",
    "#         mask_path = os.path.join(self.root_dir, patient, 'enface mask')\n",
    "\n",
    "#         enface_file = next((f for f in os.listdir(enface_path) if f.lower().endswith('.jpg')), None)\n",
    "#         mask_file = next((f for f in os.listdir(mask_path) if f.lower().endswith(('.png', '.jpg'))), None)\n",
    "\n",
    "#         enface_img = Image.open(os.path.join(enface_path, enface_file)).convert('RGB')\n",
    "#         mask_img = Image.open(os.path.join(mask_path, mask_file)).convert('L')\n",
    "\n",
    "#         seed = np.random.randint(2147483647)\n",
    "#         random.seed(seed)\n",
    "#         torch.manual_seed(seed)\n",
    "#         enface_img = self.augment(enface_img)\n",
    "#         random.seed(seed)\n",
    "#         torch.manual_seed(seed)\n",
    "#         mask_img = self.augment(mask_img)\n",
    "\n",
    "#         enface_tensor = self.to_tensor(self.resize(enface_img)) * 2 - 1\n",
    "#         mask_tensor = self.to_tensor(self.resize(mask_img)).squeeze(0)\n",
    "#         mask_tensor = (mask_tensor > THRESH).float()\n",
    "\n",
    "#         return enface_tensor, mask_tensor\n",
    "\n",
    "# # =============== Relative Positional Bias ===============\n",
    "# def get_rel_pos_index(h, w):\n",
    "#     coords = torch.stack(torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\"))\n",
    "#     coords_flat = coords.flatten(1)\n",
    "#     rel_coords = coords_flat[:, :, None] - coords_flat[:, None, :]\n",
    "#     rel_coords = rel_coords.permute(1, 2, 0).contiguous()\n",
    "#     rel_coords[:, :, 0] += h - 1\n",
    "#     rel_coords[:, :, 1] += w - 1\n",
    "#     rel_coords[:, :, 0] *= 2 * w - 1\n",
    "#     return rel_coords.sum(-1)\n",
    "\n",
    "# # =============== Vision Transformer Encoder (Relative) ===============\n",
    "# class PatchEmbedding(nn.Module):\n",
    "#     def __init__(self, in_channels=3, patch_size=16, emb_size=768):\n",
    "#         super().__init__()\n",
    "#         self.patch_size = patch_size\n",
    "#         self.proj = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "#         self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B = x.shape[0]\n",
    "#         x = self.proj(x).flatten(2).transpose(1, 2)  # [B, N, D]\n",
    "#         cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "#         x = torch.cat((cls_tokens, x), dim=1)\n",
    "#         return x\n",
    "\n",
    "# class RelPosAttention(nn.Module):\n",
    "#     def __init__(self, dim, num_heads, window_size):\n",
    "#         super().__init__()\n",
    "#         self.num_heads = num_heads\n",
    "#         self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
    "#         self.scale = (dim // num_heads) ** -0.5\n",
    "#         self.proj = nn.Linear(dim, dim)\n",
    "#         self.rel_index = get_rel_pos_index(window_size, window_size)\n",
    "#         self.rel_bias = nn.Parameter(torch.zeros((2 * window_size - 1) ** 2, num_heads))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, N, C = x.shape\n",
    "#         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "#         q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "#         attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "#         rel = self.rel_bias[self.rel_index.to(x.device)].unsqueeze(0).permute(0, 3, 1, 2)\n",
    "#         attn = attn + rel[:, :, :N, :N]\n",
    "#         attn = attn.softmax(dim=-1)\n",
    "#         x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "#         return self.proj(x)\n",
    "\n",
    "# class TransformerBlock(nn.Module):\n",
    "#     def __init__(self, dim, num_heads, mlp_ratio=4.0, dropout=0.1, window_size=32):\n",
    "#         super().__init__()\n",
    "#         self.norm1 = nn.LayerNorm(dim)\n",
    "#         self.attn = RelPosAttention(dim, num_heads, window_size)\n",
    "#         self.norm2 = nn.LayerNorm(dim)\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Linear(int(dim * mlp_ratio), dim),\n",
    "#             nn.Dropout(dropout)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.attn(self.norm1(x))\n",
    "#         x = x + self.mlp(self.norm2(x))\n",
    "#         return x\n",
    "\n",
    "# class ViTEncoder(nn.Module):\n",
    "#     def __init__(self, patch_size=16, emb_size=768, img_size=512, num_layers=12, num_heads=12):\n",
    "#         super().__init__()\n",
    "#         self.patch_embed = PatchEmbedding(patch_size=patch_size, emb_size=emb_size)\n",
    "#         self.layers = nn.Sequential(*[\n",
    "#             TransformerBlock(dim=emb_size, num_heads=num_heads, window_size=img_size // patch_size)\n",
    "#             for _ in range(num_layers)\n",
    "#         ])\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.patch_embed(x)[:, 1:, :]\n",
    "#         return self.layers(x)\n",
    "\n",
    "# # =============== Run Encoder on Curated Data ===============\n",
    "# if __name__ == '__main__':\n",
    "#     # curated_path = r\"C:\\Users\\micah\\OneDrive\\Gmail OneDrive\\Desktop\\CMU\\_Spring 2025\\Projects in BME\\Fovea Detection\\curated_enface\"\n",
    "#     # curated_path = \"/content/drive/MyDrive/curted_enface\"\n",
    "#     dataset = EnfaceMaskedDataset(curated_path, image_size=512)\n",
    "#     loader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "#     model = ViTEncoder(img_size=512)\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model.to(device)\n",
    "#     model.train()\n",
    "\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)\n",
    "#     patchify = nn.Unfold(kernel_size=16, stride=16)\n",
    "\n",
    "#     for epoch in range(200):\n",
    "#         epoch_loss = 0\n",
    "#         for imgs, _ in loader:\n",
    "#             imgs = imgs.to(device)\n",
    "#             full_img = imgs.clone()\n",
    "#             masked_full = full_img.clone()\n",
    "\n",
    "#             # Full image masking (random patch occlusion)\n",
    "#             B, C, H, W = masked_full.shape\n",
    "#             for b in range(B):\n",
    "#                 for _ in range(20):\n",
    "#                     top = random.randint(0, H - 64)\n",
    "#                     left = random.randint(0, W - 64)\n",
    "#                     masked_full[b, :, top:top+64, left:left+64] = 0\n",
    "\n",
    "#             # Multi-crop masking\n",
    "#             loss_total = 0\n",
    "#             stride = 128\n",
    "#             crop_size = 256\n",
    "#             for i in range(0, H - crop_size + 1, stride):\n",
    "#                 for j in range(0, W - crop_size + 1, stride):\n",
    "#                     crop = imgs[:, :, i:i+crop_size, j:j+crop_size].clone()\n",
    "#                     masked_crop = crop.clone()\n",
    "#                     for b in range(B):\n",
    "#                         top = random.randint(0, crop_size - 64)\n",
    "#                         left = random.randint(0, crop_size - 64)\n",
    "#                         masked_crop[b, :, top:top+64, left:left+64] = 0\n",
    "\n",
    "#                     patch_crops = patchify(crop).transpose(1, 2)\n",
    "#                     patch_masked = patchify(masked_crop).transpose(1, 2)\n",
    "#                     missing_patch = (patch_masked == 0).any(dim=2)\n",
    "#                     valid_mask = ~missing_patch\n",
    "\n",
    "#                     features = model(masked_crop)\n",
    "#                     valid_pred = features[valid_mask].reshape(-1, features.size(-1))\n",
    "#                     valid_targets = patch_crops[valid_mask].reshape(-1, patch_crops.size(-1))\n",
    "#                     loss_total += nn.MSELoss()(valid_pred, valid_targets)\n",
    "\n",
    "#             # Full-image masking loss\n",
    "#             full_patch = patchify(full_img).transpose(1, 2)\n",
    "#             full_patch_mask = patchify(masked_full).transpose(1, 2)\n",
    "#             missing_patch_full = (full_patch_mask == 0).any(dim=2)\n",
    "#             valid_mask_full = ~missing_patch_full\n",
    "#             full_features = model(masked_full)\n",
    "#             valid_pred_full = full_features[valid_mask_full].reshape(-1, full_features.size(-1))\n",
    "#             valid_targets_full = full_patch[valid_mask_full].reshape(-1, full_patch.size(-1))\n",
    "#             loss_total += nn.MSELoss()(valid_pred_full, valid_targets_full)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss_total.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             epoch_loss += loss_total.item()\n",
    "\n",
    "#         print(f\"[Epoch {epoch}] Combined Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "#     # os.makedirs(\"models\", exist_ok=True)\n",
    "#     # torch.save(model.state_dict(), \"models/vit_encoder_multicrop_augmented.pth\")\n",
    "#     # save_path = \"/content/drive/MyDrive/curted_enface/supreme_vit_mae.pth\"\n",
    "#     os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "#     torch.save(model.state_dict(), save_path)\n",
    "#     print(\"✅ Encoder saved at:\", save_path)\n",
    "\n",
    "# ran this on colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c94bc91",
   "metadata": {},
   "source": [
    "# Ensure that the masks are loading properly\n",
    "### did not add morphological closing or contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848d50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_autoencoder_vit.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "THRESH_COLOR1 = (255, 200, 0)  # Bright orange (outer mask)\n",
    "THRESH_COLOR2 = (255, 255, 0)  # Lighter yellow-orange (inner mask)\n",
    "TOLERANCE = 60  # Allow variation in RGB for thresholding\n",
    "\n",
    "# =============== Dataset Loader ===============\n",
    "class EnfaceMaskedDataset(Dataset):\n",
    "    def __init__(self, root_dir, image_size=512):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_size = image_size\n",
    "        self.patients = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        self.augment = transforms.Compose([\n",
    "            transforms.RandomAffine(degrees=15, translate=(0.2, 0.2), scale=(0.8, 1.2)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip()\n",
    "        ])\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.resize = transforms.Resize((image_size, image_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patients)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient = self.patients[idx]\n",
    "        enface_path = os.path.join(self.root_dir, patient, 'raw enface')\n",
    "        mask_path = os.path.join(self.root_dir, patient, 'enface mask')\n",
    "\n",
    "        enface_file = next(f for f in os.listdir(enface_path) if f.lower().endswith('.jpg'))\n",
    "        mask_file = next(f for f in os.listdir(mask_path) if f.lower().endswith(('.png', '.jpg')))\n",
    "\n",
    "        enface_img = Image.open(os.path.join(enface_path, enface_file)).convert('RGB')\n",
    "        mask_img = Image.open(os.path.join(mask_path, mask_file)).convert('RGB')\n",
    "\n",
    "        seed = np.random.randint(2147483647)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        enface_img = self.augment(enface_img)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        mask_img = self.augment(mask_img)\n",
    "\n",
    "        enface_tensor = self.to_tensor(self.resize(enface_img)) * 2 - 1\n",
    "\n",
    "        mask_np = np.array(self.resize(mask_img))\n",
    "\n",
    "        lower1 = np.array([c - TOLERANCE for c in THRESH_COLOR1])\n",
    "        upper1 = np.array([c + TOLERANCE for c in THRESH_COLOR1])\n",
    "        lower2 = np.array([c - TOLERANCE for c in THRESH_COLOR2])\n",
    "        upper2 = np.array([c + TOLERANCE for c in THRESH_COLOR2])\n",
    "\n",
    "        mask_bin1 = np.all((mask_np >= lower1) & (mask_np <= upper1), axis=-1)\n",
    "        mask_bin2 = np.all((mask_np >= lower2) & (mask_np <= upper2), axis=-1)\n",
    "        mask_combined = (mask_bin1 | mask_bin2).astype(np.float32)\n",
    "        mask_tensor = torch.tensor(mask_combined)\n",
    "\n",
    "        return enface_tensor, mask_tensor\n",
    "\n",
    "# =============== Visualize Random Transformed Samples ===============\n",
    "def visualize_augmented_samples(dataset_path, num_samples=9):\n",
    "    dataset = EnfaceMaskedDataset(dataset_path)\n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i, idx in enumerate(indices):\n",
    "        img, mask = dataset[idx]\n",
    "        masked_img = img.clone()\n",
    "        for c in range(3):\n",
    "            masked_img[c] *= (1 - mask)\n",
    "\n",
    "        img_np = ((masked_img + 1) / 2).permute(1, 2, 0).numpy()\n",
    "        mask_np = mask.numpy()\n",
    "\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(img_np)\n",
    "        plt.imshow(mask_np, cmap=\"Reds\", alpha=0.4)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Sample {idx}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    visualize_augmented_samples(\"curated_enface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e9e16",
   "metadata": {},
   "source": [
    "# UMapp++ With Skip Connections Based Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972c9118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define threshold colors for segmentation masks\n",
    "THRESH_COLOR1 = (255, 200, 0)\n",
    "THRESH_COLOR2 = (255, 255, 0)\n",
    "TOLERANCE = 60\n",
    "\n",
    "# =============== Utility: 2D Sin-Cos Positional Embedding ===============\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size):\n",
    "    \"\"\"Generate 2D positional embeddings with correct dimensions\"\"\"\n",
    "    assert embed_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "    # Generate grid coordinates for both axes\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "    \n",
    "    # Get embeddings for each axis\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid_h)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid_w)\n",
    "    \n",
    "    # Create 2D embeddings through broadcasting\n",
    "    emb = np.concatenate([\n",
    "        np.repeat(emb_h[:, None, :], grid_size, axis=1),\n",
    "        np.repeat(emb_w[None, :, :], grid_size, axis=0)\n",
    "    ], axis=-1)\n",
    "    \n",
    "    return emb.reshape(-1, embed_dim)\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"Generate 1D positional embeddings from grid positions\"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # Frequency calculation\n",
    "    \n",
    "    # Ensure pos is treated as array even with single value\n",
    "    pos = np.asarray(pos).reshape(-1)\n",
    "    out = np.outer(pos, omega)  # Vectorized calculation\n",
    "    \n",
    "    return np.concatenate([np.sin(out), np.cos(out)], axis=1)\n",
    "\n",
    "# =============== Dataset Loader for Segmentation ===============\n",
    "class EnfaceSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, image_size=512):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_size = image_size\n",
    "        self.patients = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        self.augment = transforms.Compose([\n",
    "            transforms.RandomAffine(degrees=15, translate=(0.2, 0.2), scale=(0.8, 1.2)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip()\n",
    "        ])\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.resize = transforms.Resize((image_size, image_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patients)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient = self.patients[idx]\n",
    "        enface_path = os.path.join(self.root_dir, patient, 'raw enface')\n",
    "        mask_path = os.path.join(self.root_dir, patient, 'enface mask')\n",
    "\n",
    "        # Handle potential file not found errors gracefully\n",
    "        try:\n",
    "            enface_file = next(f for f in os.listdir(enface_path) if f.lower().endswith('.jpg'))\n",
    "            mask_file = next(f for f in os.listdir(mask_path) if f.lower().endswith(('.png', '.jpg')))\n",
    "        except (StopIteration, FileNotFoundError):\n",
    "            print(f\"Error finding image files for patient: {patient}\")\n",
    "            # Return a valid but empty sample as fallback\n",
    "            return torch.zeros(3, self.image_size, self.image_size), torch.zeros(self.image_size, self.image_size)\n",
    "\n",
    "        try:\n",
    "            enface_img = Image.open(os.path.join(enface_path, enface_file)).convert('RGB')\n",
    "            mask_img = Image.open(os.path.join(mask_path, mask_file)).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image files for patient {patient}: {e}\")\n",
    "            return torch.zeros(3, self.image_size, self.image_size), torch.zeros(self.image_size, self.image_size)\n",
    "\n",
    "        # Apply consistent augmentation to both image and mask\n",
    "        seed = np.random.randint(2147483647)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        enface_img = self.augment(enface_img)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        mask_img = self.augment(mask_img)\n",
    "\n",
    "        # Convert to tensor and normalize to [-1, 1] for the image\n",
    "        enface_tensor = self.to_tensor(self.resize(enface_img)) * 2 - 1\n",
    "\n",
    "        # Create binary mask by thresholding for both color ranges\n",
    "        mask_np = np.array(self.resize(mask_img))\n",
    "        lower1 = np.array([c - TOLERANCE for c in THRESH_COLOR1])\n",
    "        upper1 = np.array([c + TOLERANCE for c in THRESH_COLOR1])\n",
    "        lower2 = np.array([c - TOLERANCE for c in THRESH_COLOR2])\n",
    "        upper2 = np.array([c + TOLERANCE for c in THRESH_COLOR2])\n",
    "\n",
    "        mask_bin1 = np.all((mask_np >= lower1) & (mask_np <= upper1), axis=-1)\n",
    "        mask_bin2 = np.all((mask_np >= lower2) & (mask_np <= upper2), axis=-1)\n",
    "        mask_combined = (mask_bin1 | mask_bin2).astype(np.float32)\n",
    "        mask_tensor = torch.tensor(mask_combined)\n",
    "\n",
    "        return enface_tensor, mask_tensor\n",
    "\n",
    "\n",
    "# =============== Relative Positional Embeddings ===============\n",
    "def get_rel_pos_index(h, w):\n",
    "    \"\"\"Calculate relative position indices\"\"\"\n",
    "    coords = torch.stack(torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\"))\n",
    "    coords_flat = coords.flatten(1)\n",
    "    rel_coords = coords_flat[:, :, None] - coords_flat[:, None, :]\n",
    "    rel_coords = rel_coords.permute(1, 2, 0).contiguous()\n",
    "    rel_coords[:, :, 0] += h - 1\n",
    "    rel_coords[:, :, 1] += w - 1\n",
    "    rel_coords[:, :, 0] *= 2 * w - 1\n",
    "    return rel_coords.sum(-1)\n",
    "\n",
    "class RelativeAttention(nn.Module):\n",
    "    \"\"\"Attention mechanism with relative positional embeddings\"\"\"\n",
    "    def __init__(self, dim, num_heads, grid_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.rel_index = get_rel_pos_index(grid_size, grid_size)\n",
    "        self.rel_bias = nn.Parameter(torch.zeros((2 * grid_size - 1) ** 2, num_heads))\n",
    "        nn.init.trunc_normal_(self.rel_bias, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Calculate attention scores\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        # Handle relative position bias\n",
    "        rel_bias = self.rel_bias[self.rel_index.to(x.device)]\n",
    "        if N > self.rel_index.shape[0]:\n",
    "            # Handle potential mismatch in sequence length\n",
    "            pad_size = N - self.rel_index.shape[0]\n",
    "            rel_bias = F.pad(rel_bias, (0, 0, 0, pad_size, 0, pad_size), value=0)\n",
    "        rel = rel_bias.unsqueeze(0).permute(0, 3, 1, 2)\n",
    "        \n",
    "        attn = attn + rel[:, :, :N, :N]\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block with relative positional embeddings\"\"\"\n",
    "    def __init__(self, dim, num_heads, grid_size, mlp_ratio=4., norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = RelativeAttention(dim, num_heads, grid_size)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(dim * mlp_ratio), dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# =============== MAE Model (Encoder Only) ===============\n",
    "class MAEEncoder(nn.Module):\n",
    "    \"\"\"MAE Encoder with relative positional embeddings\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=512,\n",
    "        patch_size=32,\n",
    "        in_chans=3,\n",
    "        embed_dim=768,\n",
    "        depth=8,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Encoder parameters\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Size info\n",
    "        self.img_size = img_size\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        self.grid_size = img_size // patch_size\n",
    "        self.num_patches = self.grid_size ** 2\n",
    "        \n",
    "        # Encoder embeddings\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, 1 + self.num_patches, embed_dim),\n",
    "            requires_grad=False\n",
    "        )  # fixed sin-cos embedding\n",
    "\n",
    "        # Encoder blocks with intermediate outputs for skip connections\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                embed_dim, \n",
    "                num_heads, \n",
    "                self.grid_size, \n",
    "                mlp_ratio=mlp_ratio, \n",
    "                norm_layer=norm_layer\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Initialize cls_token\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "        \n",
    "        # Initialize positional embedding\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.embed_dim, self.grid_size)\n",
    "        pos_embed = torch.from_numpy(pos_embed).float().unsqueeze(0)\n",
    "        pe_token = torch.zeros(1, 1, self.embed_dim)\n",
    "        self.pos_embed.data.copy_(torch.cat([pe_token, pos_embed], dim=1))\n",
    "        \n",
    "        # Initialize linear layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x, return_intermediates=False):\n",
    "        \"\"\"\n",
    "        Forward pass with option to return intermediate features for skip connections\n",
    "        \"\"\"\n",
    "        # Convert image to patches\n",
    "        x = self.patch_embed(x)  # [B, C, H/P, W/P]\n",
    "        x = x.flatten(2).transpose(1, 2)  # [B, N, C]\n",
    "        \n",
    "        # Add positional embedding (without cls token)\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "        \n",
    "        # Append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # Apply transformer blocks and collect intermediate features\n",
    "        intermediates = []\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "            if return_intermediates:\n",
    "                intermediates.append(x[:, 1:].clone())  # Skip cls token\n",
    "        \n",
    "        # Apply final normalization\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Return features after cls token\n",
    "        if return_intermediates:\n",
    "            return x[:, 1:], intermediates  # Return without cls token\n",
    "        else:\n",
    "            return x[:, 1:]  # Return without cls token\n",
    "\n",
    "# =============== UNet++ Decoder with Skip Connections ===============\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Double convolution block for UNet++\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if mid_channels is None:\n",
    "            mid_channels = out_channels\n",
    "            \n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, skip_channels=0, bilinear=False):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            up_channels = in_channels\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            up_channels = in_channels // 2\n",
    "\n",
    "        self.conv = ConvBlock(up_channels + skip_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        x1 = self.up(x1)\n",
    "        if x2 is not None:\n",
    "            # Handle potential size mismatch\n",
    "            diffY = x2.size()[2] - x1.size()[2]\n",
    "            diffX = x2.size()[3] - x1.size()[3]\n",
    "            x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "            x = torch.cat([x2, x1], dim=1)\n",
    "        else:\n",
    "            x = x1\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNetPPDecoder(nn.Module):\n",
    "    \"\"\"UNet++ decoder for segmentation with dense skip connections\"\"\"\n",
    "    def __init__(self, encoder_dim=768, grid_size=16):\n",
    "        super().__init__()\n",
    "        # Reshaping from encoder output to 2D feature maps\n",
    "        self.grid_size = grid_size\n",
    "        self.encoder_dim = encoder_dim\n",
    "        \n",
    "        # Nested U-Net structure with dense skip connections\n",
    "        # X_0,0 (encoder output) -> X_0,1 -> X_0,2 -> X_0,3 -> X_0,4\n",
    "        # X_1,0                  -> X_1,1 -> X_1,2 -> X_1,3\n",
    "        # X_2,0                  -> X_2,1 -> X_2,2\n",
    "        # X_3,0                  -> X_3,1\n",
    "        # X_4,0\n",
    "        \n",
    "        # First level (deepest) \n",
    "        self.conv_x00 = ConvBlock(encoder_dim, 512)\n",
    "        \n",
    "        # Second level\n",
    "        self.up_x01 = UpBlock(512, 256)\n",
    "        self.conv_x10 = ConvBlock(encoder_dim, 256)\n",
    "        self.conv_x11 = ConvBlock(512, 256)  # 256 + 256\n",
    "        \n",
    "        # Third level\n",
    "        self.up_x02 = UpBlock(256, 128)\n",
    "        self.up_x12 = UpBlock(256, 128)\n",
    "        self.conv_x20 = ConvBlock(encoder_dim, 128)\n",
    "        self.conv_x21 = ConvBlock(256, 128)  # 128 + 128\n",
    "        self.conv_x22 = ConvBlock(384, 128)  # 128 + 128 + 128\n",
    "        \n",
    "        # Fourth level\n",
    "        self.up_x03 = UpBlock(128, 64)\n",
    "        self.up_x13 = UpBlock(128, 64)\n",
    "        self.up_x23 = UpBlock(128, 64)\n",
    "        self.conv_x30 = ConvBlock(encoder_dim, 64)\n",
    "        self.conv_x31 = ConvBlock(128, 64)   # 64 + 64\n",
    "        self.conv_x32 = ConvBlock(192, 64)   # 64 + 64 + 64\n",
    "        self.conv_x33 = ConvBlock(256, 64)   # 64 + 64 + 64 + 64\n",
    "        \n",
    "        # Final level (nearest to output)\n",
    "        self.up_x04 = UpBlock(64, 32)\n",
    "        self.up_x14 = UpBlock(64, 32)\n",
    "        self.up_x24 = UpBlock(64, 32)\n",
    "        self.up_x34 = UpBlock(64, 32)\n",
    "        self.conv_x40 = ConvBlock(encoder_dim, 32)\n",
    "        self.conv_x41 = ConvBlock(64, 32)    # 32 + 32\n",
    "        self.conv_x42 = ConvBlock(96, 32)    # 32 + 32 + 32\n",
    "        self.conv_x43 = ConvBlock(128, 32)   # 32 + 32 + 32 + 32\n",
    "        self.conv_x44 = ConvBlock(160, 32)   # 32 + 32 + 32 + 32 + 32\n",
    "        \n",
    "        # Output layers\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 1, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, encoded_features, intermediates):\n",
    "        B = encoded_features.shape[0]\n",
    "        grid_size = self.grid_size\n",
    "        \n",
    "        # Process encoder features \n",
    "        x00 = encoded_features.permute(0, 2, 1).reshape(B, self.encoder_dim, grid_size, grid_size)\n",
    "        x00 = self.conv_x00(x00)  # [B, 512, 16, 16]\n",
    "\n",
    "        # Process intermediates with upsampling\n",
    "        if len(intermediates) >= 4:\n",
    "            # Upsample encoder intermediates to match decoder levels\n",
    "            x10 = F.interpolate(intermediates[3].permute(0, 2, 1).reshape(B, self.encoder_dim, grid_size, grid_size), \n",
    "                            scale_factor=2, mode='bilinear')\n",
    "            x10 = self.conv_x10(x10)  # [B, 256, 32, 32]\n",
    "\n",
    "            x20 = F.interpolate(intermediates[2].permute(0, 2, 1).reshape(B, self.encoder_dim, grid_size, grid_size),\n",
    "                            scale_factor=4, mode='bilinear')\n",
    "            x20 = self.conv_x20(x20)  # [B, 128, 64, 64]\n",
    "\n",
    "            x30 = F.interpolate(intermediates[1].permute(0, 2, 1).reshape(B, self.encoder_dim, grid_size, grid_size),\n",
    "                            scale_factor=8, mode='bilinear')\n",
    "            x30 = self.conv_x30(x30)  # [B, 64, 128, 128]\n",
    "\n",
    "            x40 = F.interpolate(intermediates[0].permute(0, 2, 1).reshape(B, self.encoder_dim, grid_size, grid_size),\n",
    "                            scale_factor=16, mode='bilinear')\n",
    "            x40 = self.conv_x40(x40)  # [B, 32, 256, 256]\n",
    "\n",
    "        \n",
    "        # First level up path and skip connections\n",
    "        x01 = self.up_x01(x00)  # [B, 256, 32, 32]\n",
    "        x11 = self.conv_x11(torch.cat([x10, x01], dim=1))  # [B, 256, 32, 32]\n",
    "        \n",
    "        # Second level\n",
    "        x02 = self.up_x02(x01)  # [B, 128, 64, 64]\n",
    "        x12 = self.up_x12(x11)  # [B, 128, 64, 64]\n",
    "        x22 = self.conv_x22(torch.cat([x20, x02, x12], dim=1))  # [B, 128, 64, 64]\n",
    "        \n",
    "        # Third level\n",
    "        x03 = self.up_x03(x02)  # [B, 64, 128, 128]\n",
    "        x13 = self.up_x13(x12)  # [B, 64, 128, 128]\n",
    "        x23 = self.up_x23(x22)  # [B, 64, 128, 128]\n",
    "        x33 = self.conv_x33(torch.cat([x30, x03, x13, x23], dim=1))  # [B, 64, 128, 128]\n",
    "        \n",
    "        # Fourth level\n",
    "        x04 = self.up_x04(x03)  # [B, 32, 256, 256]\n",
    "        x14 = self.up_x14(x13)  # [B, 32, 256, 256]\n",
    "        x24 = self.up_x24(x23)  # [B, 32, 256, 256]\n",
    "        x34 = self.up_x34(x33)  # [B, 32, 256, 256]\n",
    "        x44 = self.conv_x44(torch.cat([x40, x04, x14, x24, x34], dim=1))  # [B, 32, 256, 256]\n",
    "        \n",
    "        # Final upsampling to reach original image size\n",
    "        # Need to go from 256x256 to 512x512\n",
    "        final_features = F.interpolate(x44, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # Output layer - produces segmentation mask\n",
    "        out = self.final(final_features)  # [B, 1, 512, 512]\n",
    "        \n",
    "        return out\n",
    "\n",
    "# =============== Combined Model ===============\n",
    "class MAESegmentation(nn.Module):\n",
    "    \"\"\"Combined MAE encoder with UNet++ decoder for segmentation\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=512,\n",
    "        patch_size=32,\n",
    "        in_chans=3,\n",
    "        embed_dim=768,\n",
    "        encoder_depth=8,\n",
    "        encoder_num_heads=12,\n",
    "        mlp_ratio=4.,\n",
    "        pretrained_mae_path=None,\n",
    "        freeze_encoder=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create encoder with exact same architecture as MAE encoder\n",
    "        self.encoder = MAEEncoder(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            depth=encoder_depth,\n",
    "            num_heads=encoder_num_heads,\n",
    "            mlp_ratio=mlp_ratio\n",
    "        )\n",
    "        \n",
    "        # Load pretrained weights if provided\n",
    "        if pretrained_mae_path:\n",
    "            self._load_pretrained_encoder(pretrained_mae_path)\n",
    "        \n",
    "        # Freeze encoder parameters if specified\n",
    "        if freeze_encoder:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Calculate grid size based on image size and patch size\n",
    "        grid_size = img_size // patch_size\n",
    "        \n",
    "        # Create UNet++ decoder\n",
    "        self.decoder = UNetPPDecoder(\n",
    "            encoder_dim=embed_dim, \n",
    "            grid_size=grid_size\n",
    "        )\n",
    "    \n",
    "    def _load_pretrained_encoder(self, pretrained_path):\n",
    "        \"\"\"Load pretrained weights from MAE model checkpoint\"\"\"\n",
    "        try:\n",
    "            # Handle different checkpoint formats\n",
    "            checkpoint = torch.load(pretrained_path, map_location='cpu')\n",
    "            \n",
    "            # If the checkpoint is a dict with 'model_state_dict' key\n",
    "            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "                state_dict = checkpoint['model_state_dict']\n",
    "            else:\n",
    "                state_dict = checkpoint\n",
    "                \n",
    "            # Filter and load encoder-specific parameters\n",
    "            encoder_dict = {}\n",
    "            for k, v in state_dict.items():\n",
    "                # Only load parameters for components present in our encoder\n",
    "                # Remove decoder related keys and other non-encoder components\n",
    "                if k.startswith('patch_embed') or k.startswith('blocks') or k.startswith('norm') or \\\n",
    "                   k.startswith('cls_token') or k.startswith('pos_embed'):\n",
    "                    encoder_dict[k] = v\n",
    "            \n",
    "            # Attempt to load weights\n",
    "            msg = self.encoder.load_state_dict(encoder_dict, strict=False)\n",
    "            print(f\"Loaded pretrained MAE encoder from {pretrained_path}\")\n",
    "            print(f\"Missing keys: {msg.missing_keys}\")\n",
    "            print(f\"Unexpected keys: {msg.unexpected_keys}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pretrained weights: {e}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get encoder features with intermediates for skip connections\n",
    "        encoded_features, intermediates = self.encoder(x, return_intermediates=True)\n",
    "        \n",
    "        # Pass to decoder\n",
    "        segmentation = self.decoder(encoded_features, intermediates)\n",
    "        \n",
    "        return segmentation\n",
    "\n",
    "# =============== Loss Functions ===============\n",
    "# =============== Loss Functions ===============\n",
    "def dice_loss(pred, target, smooth=1e-6):\n",
    "    \"\"\"Dice loss for segmentation\"\"\"\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    intersection = (pred * target).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "    return 1 - dice\n",
    "\n",
    "def area_loss(pred, max_area=40*40):\n",
    "    \"\"\"Penalize predictions that exceed the maximum allowed area\"\"\"\n",
    "    # Convert logits to binary predictions\n",
    "    pred_binary = (torch.sigmoid(pred) > 0.5).float()\n",
    "    # Calculate area of each prediction in the batch\n",
    "    areas = pred_binary.sum(dim=[2, 3])  # Sum over height and width dimensions\n",
    "    # Calculate penalty for exceeding max_area\n",
    "    # Use ReLU to only penalize when area > max_area\n",
    "    penalty = F.relu(areas - max_area) / max_area\n",
    "    return penalty.mean()\n",
    "\n",
    "def empty_penalty(pred, weight=0.1):\n",
    "    \"\"\"Penalize empty predictions (no segmentation)\"\"\"\n",
    "    pred_binary = (torch.sigmoid(pred) > 0.5).float()\n",
    "    # Check if any prediction is completely empty\n",
    "    empty_mask = (pred_binary.sum(dim=[2, 3]) == 0).float()\n",
    "    return weight * empty_mask.mean()\n",
    "\n",
    "def combined_segmentation_loss(pred, target, area_weight=0.2, empty_weight=0.1, max_area=75*75):\n",
    "    \"\"\"Combined loss function for segmentation with area constraint\"\"\"\n",
    "    # Binary cross entropy loss\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target.unsqueeze(1))\n",
    "    \n",
    "    # Dice loss\n",
    "    dice = dice_loss(pred, target.unsqueeze(1))\n",
    "    \n",
    "    # Area constraint loss\n",
    "    area = area_loss(pred, max_area=max_area)\n",
    "    \n",
    "    # Empty prediction penalty\n",
    "    empty = empty_penalty(pred, weight=empty_weight)\n",
    "    \n",
    "    # Combine losses (weighted sum)\n",
    "    total_loss = 0.4 * bce + 0.4 * dice + area_weight * area + empty_weight * empty\n",
    "    \n",
    "    # Return individual loss components for monitoring\n",
    "    return total_loss, {\n",
    "        'bce': bce.item(),\n",
    "        'dice': dice.item(),\n",
    "        'area': area.item(),\n",
    "        'empty': empty.item()\n",
    "    }\n",
    "\n",
    "\n",
    "# =============== Training Function ===============\n",
    "def train_segmentation(\n",
    "    data_path,\n",
    "    pretrained_mae_path,\n",
    "    batch_size=2,\n",
    "    num_epochs=150,\n",
    "    learning_rate=1.5e-4,  # Match MAE's learning rate\n",
    "    output_dir=\"segmentation_results2\",\n",
    "    save_interval=5,\n",
    "    device=None,\n",
    "    max_area=75*75,  # Maximum area in pixels for fovea segmentation\n",
    "    area_weight=0.2,  # Weight for area constraint loss\n",
    "    empty_weight=0.1  # Weight for empty prediction penalty\n",
    "):\n",
    "    \"\"\"Train segmentation model using pretrained MAE encoder with area constraints\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, \"checkpoints\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, \"visuals\"), exist_ok=True)\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = EnfaceSegmentationDataset(data_path)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Set to 0 to avoid CUDA issues\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    print(f\"Dataset loaded with {len(dataset)} samples\")\n",
    "    \n",
    "    # Create model\n",
    "    model = MAESegmentation(\n",
    "        img_size=512,\n",
    "        patch_size=32,  # Match MAE's patch size\n",
    "        embed_dim=768,  # Match MAE's embedding dimension\n",
    "        encoder_depth=8,  # Match MAE's encoder depth\n",
    "        encoder_num_heads=12,  # Match MAE's encoder heads\n",
    "        pretrained_mae_path=pretrained_mae_path,\n",
    "        freeze_encoder=True  # Freeze encoder weights initially\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizer - only update decoder parameters if encoder is frozen\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=learning_rate,\n",
    "        betas=(0.9, 0.95),  # Match MAE's optimizer settings\n",
    "        weight_decay=0.05   # Match MAE's weight decay\n",
    "    )\n",
    "\n",
    "    # Learning rate scheduler - use CosineAnnealingLR to match MAE's scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=num_epochs,\n",
    "        eta_min=1e-5\n",
    "    )\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    # Create CSV file to track loss components\n",
    "    loss_tracker_path = os.path.join(output_dir, \"loss_tracking.csv\")\n",
    "    with open(loss_tracker_path, 'w') as f:\n",
    "        f.write(\"epoch,batch,total_loss,bce_loss,dice_loss,area_loss,empty_loss\\n\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_bce_loss = 0.0\n",
    "        epoch_dice_loss = 0.0\n",
    "        epoch_area_loss = 0.0\n",
    "        epoch_empty_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch_idx, (images, masks) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss, loss_components = combined_segmentation_loss(\n",
    "                outputs, \n",
    "                masks, \n",
    "                area_weight=area_weight,\n",
    "                empty_weight=empty_weight,\n",
    "                max_area=max_area\n",
    "            )\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients for stability (like in MAE)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            # Track metrics\n",
    "            batch_loss = loss.item()\n",
    "            epoch_loss += batch_loss\n",
    "            epoch_bce_loss += loss_components['bce']\n",
    "            epoch_dice_loss += loss_components['dice']\n",
    "            epoch_area_loss += loss_components['area']\n",
    "            epoch_empty_loss += loss_components['empty']\n",
    "            num_batches += 1\n",
    "\n",
    "            # Print progress and log loss components\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch}/{num_epochs-1}, Batch {batch_idx}/{len(dataloader)}, \" +\n",
    "                     f\"Loss: {batch_loss:.4f}, BCE: {loss_components['bce']:.4f}, \" +\n",
    "                     f\"Dice: {loss_components['dice']:.4f}, Area: {loss_components['area']:.4f}, \" + \n",
    "                     f\"Empty: {loss_components['empty']:.4f}\")\n",
    "                \n",
    "                # Log to CSV\n",
    "                with open(loss_tracker_path, 'a') as f:\n",
    "                    f.write(f\"{epoch},{batch_idx},{batch_loss:.6f},{loss_components['bce']:.6f},\" +\n",
    "                           f\"{loss_components['dice']:.6f},{loss_components['area']:.6f},{loss_components['empty']:.6f}\\n\")\n",
    "\n",
    "        # Update learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # End of epoch tracking\n",
    "        avg_epoch_loss = epoch_loss / num_batches if num_batches > 0 else 0\n",
    "        avg_bce_loss = epoch_bce_loss / num_batches if num_batches > 0 else 0\n",
    "        avg_dice_loss = epoch_dice_loss / num_batches if num_batches > 0 else 0\n",
    "        avg_area_loss = epoch_area_loss / num_batches if num_batches > 0 else 0\n",
    "        avg_empty_loss = epoch_empty_loss / num_batches if num_batches > 0 else 0\n",
    "        \n",
    "        print(f\"Epoch {epoch} Summary:\")\n",
    "        print(f\"  Total Loss: {avg_epoch_loss:.4f}\")\n",
    "        print(f\"  BCE Loss: {avg_bce_loss:.4f}\")\n",
    "        print(f\"  Dice Loss: {avg_dice_loss:.4f}\")\n",
    "        print(f\"  Area Loss: {avg_area_loss:.4f}\")\n",
    "        print(f\"  Empty Loss: {avg_empty_loss:.4f}\")\n",
    "        print(f\"  Learning Rate: {lr_scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        # Save model checkpoint\n",
    "        if epoch % save_interval == 0 or epoch == num_epochs - 1 or avg_epoch_loss < best_loss:\n",
    "            checkpoint_path = os.path.join(output_dir, \"checkpoints\", f\"model_epoch_{epoch}.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': {\n",
    "                    'total': avg_epoch_loss,\n",
    "                    'bce': avg_bce_loss,\n",
    "                    'dice': avg_dice_loss,\n",
    "                    'area': avg_area_loss,\n",
    "                    'empty': avg_empty_loss\n",
    "                },\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Model saved to {checkpoint_path}\")\n",
    "            \n",
    "            if avg_epoch_loss < best_loss:\n",
    "                best_loss = avg_epoch_loss\n",
    "                best_model_path = os.path.join(output_dir, \"checkpoints\", \"best_model.pt\")\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': {\n",
    "                        'total': avg_epoch_loss,\n",
    "                        'bce': avg_bce_loss,\n",
    "                        'dice': avg_dice_loss,\n",
    "                        'area': avg_area_loss,\n",
    "                        'empty': avg_empty_loss\n",
    "                    },\n",
    "                }, best_model_path)\n",
    "                print(f\"✅ New best model found: {best_model_path}\")\n",
    "\n",
    "        # Save visualizations and area stats\n",
    "        if epoch % save_interval == 0 or epoch == num_epochs - 1:\n",
    "            vis_dir = os.path.join(output_dir, \"visuals\", f\"epoch_{epoch}\")\n",
    "            os.makedirs(vis_dir, exist_ok=True)\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Get a batch of samples\n",
    "                images, masks = next(iter(dataloader))\n",
    "                images = images.to(device)\n",
    "                pred_masks_logits = model(images)\n",
    "                pred_masks = (torch.sigmoid(pred_masks_logits) > 0.5).float()\n",
    "                \n",
    "                # Calculate areas for visualization\n",
    "                areas = []\n",
    "                for i in range(min(4, len(images))):\n",
    "                    # Original image\n",
    "                    save_image((images[i] + 1) / 2, os.path.join(vis_dir, f\"input_{i}.png\"))\n",
    "                    \n",
    "                    # Ground truth mask\n",
    "                    gt_mask = masks[i].unsqueeze(0)\n",
    "                    gt_area = gt_mask.sum().item()\n",
    "                    save_image(gt_mask, os.path.join(vis_dir, f\"gt_mask_{i}.png\"))\n",
    "                    \n",
    "                    # Predicted mask\n",
    "                    pred = pred_masks[i]\n",
    "                    pred_area = pred.sum().item()\n",
    "                    save_image(pred, os.path.join(vis_dir, f\"pred_mask_{i}.png\"))\n",
    "                    \n",
    "                    # Overlay of prediction on input\n",
    "                    overlay_img = images[i].cpu().clone()\n",
    "                    # Set the red channel to highlight the prediction\n",
    "                    overlay_img[0] = overlay_img[0] * (1 - pred[0]) + pred[0]\n",
    "                    save_image((overlay_img + 1) / 2, os.path.join(vis_dir, f\"overlay_{i}.png\"))\n",
    "                    \n",
    "                    areas.append((i, gt_area, pred_area))\n",
    "                \n",
    "                # Save area stats\n",
    "                with open(os.path.join(vis_dir, \"area_stats.txt\"), 'w') as f:\n",
    "                    f.write(\"Sample, GT Area (pixels), Pred Area (pixels), Max Area (75x75 = 5625)\\n\")\n",
    "                    for i, gt_area, pred_area in areas:\n",
    "                        f.write(f\"{i}, {gt_area}, {pred_area}, 5625\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return model\n",
    "\n",
    "train_segmentation('curated_enface', 'models/mae_model_epoch_199.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8bd984",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# === Utility Functions ===\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size):\n",
    "    \"\"\"Generate 2D positional embeddings with correct dimensions\"\"\"\n",
    "    assert embed_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "    # Generate grid coordinates for both axes\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "    \n",
    "    # Get embeddings for each axis\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid_h)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid_w)\n",
    "    \n",
    "    # Create 2D embeddings through broadcasting\n",
    "    emb = np.concatenate([\n",
    "        np.repeat(emb_h[:, None, :], grid_size, axis=1),\n",
    "        np.repeat(emb_w[None, :, :], grid_size, axis=0)\n",
    "    ], axis=-1)\n",
    "    \n",
    "    return emb.reshape(-1, embed_dim)\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"Generate 1D positional embeddings from grid positions\"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # Frequency calculation\n",
    "    \n",
    "    # Ensure pos is treated as array even with single value\n",
    "    pos = np.asarray(pos).reshape(-1)\n",
    "    out = np.outer(pos, omega)  # Vectorized calculation\n",
    "    \n",
    "    return np.concatenate([np.sin(out), np.cos(out)], axis=1)\n",
    "\n",
    "def get_rel_pos_index(h, w):\n",
    "    \"\"\"Calculate relative position indices\"\"\"\n",
    "    coords = torch.stack(torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\"))\n",
    "    coords_flat = coords.flatten(1)\n",
    "    rel_coords = coords_flat[:, :, None] - coords_flat[:, None, :]\n",
    "    rel_coords = rel_coords.permute(1, 2, 0).contiguous()\n",
    "    rel_coords[:, :, 0] += h - 1\n",
    "    rel_coords[:, :, 1] += w - 1\n",
    "    rel_coords[:, :, 0] *= 2 * w - 1\n",
    "    return rel_coords.sum(-1)\n",
    "\n",
    "# === Overlay Utility ===\n",
    "def overlay_mask(image, mask, alpha=0.4):\n",
    "    mask_bin = (mask > 0.8).astype(np.float32)\n",
    "    overlay = image.astype(np.float32).copy()\n",
    "    overlay[mask_bin > 0] = overlay[mask_bin > 0] * (1 - alpha) + alpha * 255\n",
    "    return overlay.clip(0, 255).astype(np.uint8)\n",
    "\n",
    "# === RelativeAttention for MAE Architecture ===\n",
    "class RelativeAttention(nn.Module):\n",
    "    \"\"\"Attention mechanism with relative positional embeddings\"\"\"\n",
    "    def __init__(self, dim, num_heads, grid_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.rel_index = get_rel_pos_index(grid_size, grid_size)\n",
    "        self.rel_bias = nn.Parameter(torch.zeros((2 * grid_size - 1) ** 2, num_heads))\n",
    "        nn.init.trunc_normal_(self.rel_bias, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Calculate attention scores\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        # Handle relative position bias\n",
    "        rel_bias = self.rel_bias[self.rel_index.to(x.device)]\n",
    "        if N > self.rel_index.shape[0]:\n",
    "            # Handle potential mismatch in sequence length\n",
    "            pad_size = N - self.rel_index.shape[0]\n",
    "            rel_bias = F.pad(rel_bias, (0, 0, 0, pad_size, 0, pad_size), value=0)\n",
    "        rel = rel_bias.unsqueeze(0).permute(0, 3, 1, 2)\n",
    "        \n",
    "        attn = attn + rel[:, :, :N, :N]\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block with relative positional embeddings\"\"\"\n",
    "    def __init__(self, dim, num_heads, grid_size, mlp_ratio=4., norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = RelativeAttention(dim, num_heads, grid_size)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(dim * mlp_ratio), dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# === MAE Encoder ===\n",
    "class MAEEncoder(nn.Module):\n",
    "    \"\"\"MAE Encoder with relative positional embeddings\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=512,\n",
    "        patch_size=32,\n",
    "        in_chans=3,\n",
    "        embed_dim=768,\n",
    "        depth=8,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Encoder parameters\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Size info\n",
    "        self.img_size = img_size\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        self.grid_size = img_size // patch_size\n",
    "        self.num_patches = self.grid_size ** 2\n",
    "        \n",
    "        # Encoder embeddings\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, 1 + self.num_patches, embed_dim),\n",
    "            requires_grad=False\n",
    "        )  # fixed sin-cos embedding\n",
    "\n",
    "        # Encoder blocks with intermediate outputs for skip connections\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                embed_dim, \n",
    "                num_heads, \n",
    "                self.grid_size, \n",
    "                mlp_ratio=mlp_ratio, \n",
    "                norm_layer=norm_layer\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Initialize cls_token\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "        \n",
    "        # Initialize positional embedding\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.embed_dim, self.grid_size)\n",
    "        pos_embed = torch.from_numpy(pos_embed).float().unsqueeze(0)\n",
    "        pe_token = torch.zeros(1, 1, self.embed_dim)\n",
    "        self.pos_embed.data.copy_(torch.cat([pe_token, pos_embed], dim=1))\n",
    "        \n",
    "        # Initialize linear layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x, return_intermediates=False):\n",
    "        \"\"\"\n",
    "        Forward pass with option to return intermediate features for skip connections\n",
    "        \"\"\"\n",
    "        # Convert image to patches\n",
    "        x = self.patch_embed(x)  # [B, C, H/P, W/P]\n",
    "        x = x.flatten(2).transpose(1, 2)  # [B, N, C]\n",
    "        \n",
    "        # Add positional embedding (without cls token)\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "        \n",
    "        # Append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # Apply transformer blocks and collect intermediate features\n",
    "        intermediates = []\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "            if return_intermediates:\n",
    "                intermediates.append(x[:, 1:].clone())  # Skip cls token\n",
    "        \n",
    "        # Apply final normalization\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Return features after cls token\n",
    "        if return_intermediates:\n",
    "            return x[:, 1:], intermediates  # Return without cls token\n",
    "        else:\n",
    "            return x[:, 1:]  # Return without cls token\n",
    "\n",
    "# === UNet++ Decoder ===\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Double convolution block for UNet++\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if mid_channels is None:\n",
    "            mid_channels = out_channels\n",
    "            \n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, skip_channels=0, bilinear=False):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            up_channels = in_channels\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            up_channels = in_channels // 2\n",
    "\n",
    "        self.conv = ConvBlock(up_channels + skip_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        x1 = self.up(x1)\n",
    "        if x2 is not None:\n",
    "            # Handle potential size mismatch\n",
    "            diffY = x2.size()[2] - x1.size()[2]\n",
    "            diffX = x2.size()[3] - x1.size()[3]\n",
    "            x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "            x = torch.cat([x2, x1], dim=1)\n",
    "        else:\n",
    "            x = x1\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNetPPDecoder(nn.Module):\n",
    "    \"\"\"UNet++ decoder for segmentation with dense skip connections\"\"\"\n",
    "    def __init__(self, encoder_dim=768, grid_size=16):\n",
    "        super().__init__()\n",
    "        # Reshaping from encoder output to 2D feature maps\n",
    "        self.grid_size = grid_size\n",
    "        self.encoder_dim = encoder_dim\n",
    "        \n",
    "        # Nested U-Net structure with dense skip connections\n",
    "        # First level (deepest) \n",
    "        self.conv_x00 = ConvBlock(encoder_dim, 512)\n",
    "        \n",
    "        # Second level\n",
    "        self.up_x01 = UpBlock(512, 256)\n",
    "        self.conv_x10 = ConvBlock(encoder_dim, 256)\n",
    "        self.conv_x11 = ConvBlock(512, 256)  # 256 + 256\n",
    "        \n",
    "        # Third level\n",
    "        self.up_x02 = UpBlock(256, 128)\n",
    "        self.up_x12 = UpBlock(256, 128)\n",
    "        self.conv_x20 = ConvBlock(encoder_dim, 128)\n",
    "        self.conv_x21 = ConvBlock(256, 128)  # 128 + 128\n",
    "        self.conv_x22 = ConvBlock(384, 128)  # 128 + 128 + 128\n",
    "        \n",
    "        # Fourth level\n",
    "        self.up_x03 = UpBlock(128, 64)\n",
    "        self.up_x13 = UpBlock(128, 64)\n",
    "        self.up_x23 = UpBlock(128, 64)\n",
    "        self.conv_x30 = ConvBlock(encoder_dim, 64)\n",
    "        self.conv_x31 = ConvBlock(128, 64)   # 64 + 64\n",
    "        self.conv_x32 = ConvBlock(192, 64)   # 64 + 64 + 64\n",
    "        self.conv_x33 = ConvBlock(256, 64)   # 64 + 64 + 64 + 64\n",
    "        \n",
    "        # Final level (nearest to output)\n",
    "        self.up_x04 = UpBlock(64, 32)\n",
    "        self.up_x14 = UpBlock(64, 32)\n",
    "        self.up_x24 = UpBlock(64, 32)\n",
    "        self.up_x34 = UpBlock(64, 32)\n",
    "        self.conv_x40 = ConvBlock(encoder_dim, 32)\n",
    "        self.conv_x41 = ConvBlock(64, 32)    # 32 + 32\n",
    "        self.conv_x42 = ConvBlock(96, 32)    # 32 + 32 + 32\n",
    "        self.conv_x43 = ConvBlock(128, 32)   # 32 + 32 + 32 + 32\n",
    "        self.conv_x44 = ConvBlock(160, 32)   # 32 + 32 + 32 + 32 + 32\n",
    "        \n",
    "        # Output layers\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 1, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, encoded_features, intermediates):\n",
    "        B = encoded_features.shape[0]\n",
    "        grid_size = self.grid_size\n",
    "        \n",
    "        # Process encoder features \n",
    "        x00 = encoded_features.permute(0, 2, 1).reshape(B, self.encoder_dim, grid_size, grid_size)\n",
    "        x00 = self.conv_x00(x00)  # [B, 512, 16, 16]\n",
    "\n",
    "        # Process intermediates with upsampling\n",
    "        if len(intermediates) >= 4:\n",
    "            # Upsample encoder intermediates to match decoder levels\n",
    "            x10 = F.interpolate(intermediates[3].permute(0, 2, 1).reshape(B, self.encoder_dim, grid_size, grid_size), \n",
    "                            scale_factor=2, mode='bilinear')\n",
    "            x10 = self.conv_x10(x10)  # [B, 256, 32, 32]\n",
    "\n",
    "            x20 = F.interpolate(intermediates[2].permute(0, 2, 1).reshape(B, self.encoder_dim, grid_size, grid_size),\n",
    "                            scale_factor=4, mode='bilinear')\n",
    "            x20 = self.conv_x20(x20)  # [B, 128, 64, 64]\n",
    "\n",
    "            x30 = F.interpolate(intermediates[1].permute(0, 2, 1).reshape(B, self.encoder_dim, grid_size, grid_size),\n",
    "                            scale_factor=8, mode='bilinear')\n",
    "            x30 = self.conv_x30(x30)  # [B, 64, 128, 128]\n",
    "\n",
    "            x40 = F.interpolate(intermediates[0].permute(0, 2, 1).reshape(B, self.encoder_dim, grid_size, grid_size),\n",
    "                            scale_factor=16, mode='bilinear')\n",
    "            x40 = self.conv_x40(x40)  # [B, 32, 256, 256]\n",
    "\n",
    "        \n",
    "        # First level up path and skip connections\n",
    "        x01 = self.up_x01(x00)  # [B, 256, 32, 32]\n",
    "        x11 = self.conv_x11(torch.cat([x10, x01], dim=1))  # [B, 256, 32, 32]\n",
    "        \n",
    "        # Second level\n",
    "        x02 = self.up_x02(x01)  # [B, 128, 64, 64]\n",
    "        x12 = self.up_x12(x11)  # [B, 128, 64, 64]\n",
    "        x22 = self.conv_x22(torch.cat([x20, x02, x12], dim=1))  # [B, 128, 64, 64]\n",
    "        \n",
    "        # Third level\n",
    "        x03 = self.up_x03(x02)  # [B, 64, 128, 128]\n",
    "        x13 = self.up_x13(x12)  # [B, 64, 128, 128]\n",
    "        x23 = self.up_x23(x22)  # [B, 64, 128, 128]\n",
    "        x33 = self.conv_x33(torch.cat([x30, x03, x13, x23], dim=1))  # [B, 64, 128, 128]\n",
    "        \n",
    "        # Fourth level\n",
    "        x04 = self.up_x04(x03)  # [B, 32, 256, 256]\n",
    "        x14 = self.up_x14(x13)  # [B, 32, 256, 256]\n",
    "        x24 = self.up_x24(x23)  # [B, 32, 256, 256]\n",
    "        x34 = self.up_x34(x33)  # [B, 32, 256, 256]\n",
    "        x44 = self.conv_x44(torch.cat([x40, x04, x14, x24, x34], dim=1))  # [B, 32, 256, 256]\n",
    "        \n",
    "        # Final upsampling to reach original image size\n",
    "        # Need to go from 256x256 to 512x512\n",
    "        final_features = F.interpolate(x44, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # Output layer - produces segmentation mask\n",
    "        out = self.final(final_features)  # [B, 1, 512, 512]\n",
    "        \n",
    "        return out\n",
    "\n",
    "# === Complete Model ===\n",
    "class MAESegmentation(nn.Module):\n",
    "    \"\"\"Combined MAE encoder with UNet++ decoder for segmentation\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=512,\n",
    "        patch_size=32,\n",
    "        in_chans=3,\n",
    "        embed_dim=768,\n",
    "        encoder_depth=8,\n",
    "        encoder_num_heads=12,\n",
    "        mlp_ratio=4.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create encoder\n",
    "        self.encoder = MAEEncoder(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            depth=encoder_depth,\n",
    "            num_heads=encoder_num_heads,\n",
    "            mlp_ratio=mlp_ratio\n",
    "        )\n",
    "                \n",
    "        # Calculate grid size based on image size and patch size\n",
    "        grid_size = img_size // patch_size\n",
    "        \n",
    "        # Create UNet++ decoder\n",
    "        self.decoder = UNetPPDecoder(\n",
    "            encoder_dim=embed_dim, \n",
    "            grid_size=grid_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get encoder features with intermediates for skip connections\n",
    "        encoded_features, intermediates = self.encoder(x, return_intermediates=True)\n",
    "        \n",
    "        # Pass to decoder\n",
    "        segmentation = self.decoder(encoded_features, intermediates)\n",
    "        \n",
    "        return segmentation\n",
    "\n",
    "# ============== MAIN CODE FOR VISUALIZATION ==============\n",
    "print(\"Setting up environment...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the trained model\n",
    "print(\"Loading model...\")\n",
    "model = MAESegmentation(\n",
    "    img_size=512,\n",
    "    patch_size=32,\n",
    "    embed_dim=768,\n",
    "    encoder_depth=8,\n",
    "    encoder_num_heads=12\n",
    ").to(device)\n",
    "\n",
    "# First load the encoder weights\n",
    "try:\n",
    "    print(\"Loading encoder...\")\n",
    "    encoder_path = \"models/mae_model_epoch_199.pt\"\n",
    "    encoder_state = torch.load(encoder_path, map_location=device)\n",
    "    model.encoder.load_state_dict(encoder_state, strict=False)\n",
    "    print(\"Encoder loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading encoder: {e}\")\n",
    "    print(\"Using randomly initialized encoder\")\n",
    "\n",
    "# Then load the whole model with pretrained decoder\n",
    "try:\n",
    "    print(\"Loading decoder...\")\n",
    "    decoder_path = \"segmentation_results2/checkpoints/best_model.pt\"\n",
    "    checkpoint = torch.load(decoder_path, map_location=device)\n",
    "    \n",
    "    if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n",
    "        print(\"Found model_state_dict in checkpoint\")\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "    else:\n",
    "        print(\"Loading state_dict directly\")\n",
    "        model.load_state_dict(checkpoint, strict=False)\n",
    "        \n",
    "    print(\"Model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Using randomly initialized weights\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Set up image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load images\n",
    "prediction_root = \"C:/Users/micah/OneDrive/Gmail OneDrive/Desktop/CMU/_Spring 2025/Projects in BME/Fovea Detection/predictions\"\n",
    "print(f\"Loading images from {prediction_root}\")\n",
    "\n",
    "# Find folders\n",
    "folders = []\n",
    "try:\n",
    "    folders = sorted([os.path.join(prediction_root, f) for f in os.listdir(prediction_root) \n",
    "                     if os.path.isdir(os.path.join(prediction_root, f))])\n",
    "    # Randomly shuffle for different visualizations each run\n",
    "    random.shuffle(folders)\n",
    "    print(f\"Found {len(folders)} image folders\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading folders: {e}\")\n",
    "    folders = []\n",
    "\n",
    "# Process images and get predictions\n",
    "imgs, overlays, filenames = [], [], []\n",
    "for folder in folders[:20]:  # Limit to 20 folders for efficiency\n",
    "    try:\n",
    "        img_file = next((f for f in os.listdir(folder) if f.lower().endswith(('.jpg', '.png', '.jpeg'))), None)\n",
    "        if not img_file:\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(folder, img_file)\n",
    "        print(f\"Processing {img_file}\")\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = transform(img).unsqueeze(0).to(device) * 2 - 1  # Scale to [-1, 1]\n",
    "\n",
    "        # Optional: Apply fovea mask if available\n",
    "        mask_path = os.path.join(folder, \"mask\", \"fovea_mask.png\")\n",
    "        if os.path.exists(mask_path):\n",
    "            mask_img = Image.open(mask_path).convert(\"L\")\n",
    "            mask_tensor = transform(mask_img).squeeze(0).to(device)\n",
    "            for c in range(3):\n",
    "                img_tensor[0, c] *= (1 - mask_tensor)\n",
    "            print(f\"[✓] Applied mask for {img_file}\")\n",
    "\n",
    "        # Generate prediction\n",
    "        with torch.no_grad():\n",
    "            pred = model(img_tensor)\n",
    "            pred_sigmoid = torch.sigmoid(pred)[0, 0].cpu().numpy()\n",
    "            print(f\"Prediction range: {pred_sigmoid.min():.3f} to {pred_sigmoid.max():.3f}\")\n",
    "\n",
    "        # Convert image to grayscale for visualization\n",
    "        img_np = np.array(img.resize((512, 512)))\n",
    "        img_gray = np.mean(img_np, axis=2) if img_np.ndim == 3 else img_np\n",
    "        \n",
    "        # Create overlay with different thresholds\n",
    "        threshold = 0.5\n",
    "\n",
    "\n",
    "\n",
    "        ##########################################################################################\n",
    "\n",
    "        mask_binary = (pred_sigmoid > threshold).astype(np.float32)\n",
    "        overlay = overlay_mask(img_gray, mask_binary, alpha=0.4)\n",
    "\n",
    "        # Store results\n",
    "        imgs.append(img_gray)\n",
    "        overlays.append(overlay)\n",
    "        filenames.append(img_file)\n",
    "        \n",
    "        if len(imgs) >= 16:  # Limit to 16 images for display\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {folder}: {e}\")\n",
    "        continue\n",
    "\n",
    "# If we couldn't load any real images, create mock data\n",
    "if len(imgs) == 0:\n",
    "    print(\"No images processed. Creating mock data for visualization...\")\n",
    "    from skimage.draw import circle\n",
    "    \n",
    "    # Create mock retinal images and fovea masks\n",
    "    for i in range(16):\n",
    "        # Create circular retinal image with fovea\n",
    "        mock_img = np.zeros((512, 512), dtype=np.uint8)\n",
    "        # Retina background\n",
    "        rr, cc = circle(256, 256, 230, shape=(512, 512))\n",
    "        mock_img[rr, cc] = 180\n",
    "        \n",
    "        # Fovea region\n",
    "        fovea_x = random.randint(226, 286)\n",
    "        fovea_y = random.randint(226, 286)\n",
    "        fovea_r = random.randint(15, 30)\n",
    "        rr, cc = circle(fovea_y, fovea_x, fovea_r, shape=(512, 512))\n",
    "        \n",
    "        # Create binary mask and overlay\n",
    "        mask = np.zeros((512, 512), dtype=np.float32)\n",
    "        mask[rr, cc] = 1.0\n",
    "        overlay = overlay_mask(mock_img, mask)\n",
    "        \n",
    "        # Add to our collections\n",
    "        imgs.append(mock_img)\n",
    "        overlays.append(overlay)\n",
    "        filenames.append(f\"mock_image_{i}.jpg\")\n",
    "\n",
    "print(f\"Successfully processed {len(imgs)} images\")\n",
    "\n",
    "# === Create visualizations ===\n",
    "\n",
    "# 1. Single random image with side-by-side comparison\n",
    "random_idx = random.randint(0, len(imgs)-1)\n",
    "f, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax[0].imshow(imgs[random_idx], cmap='gray')\n",
    "ax[0].set_title(f\"Input Image: {filenames[random_idx]}\", fontsize=14)\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(overlays[random_idx], cmap='gray')\n",
    "ax[1].set_title(\"Predicted Fovea Mask Overlay\", fontsize=14)\n",
    "ax[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('random_fovea_prediction.png')\n",
    "plt.show()\n",
    "\n",
    "# 2. Multiple side-by-side comparisons\n",
    "num_samples = min(3, len(imgs))\n",
    "sample_indices = random.sample(range(len(imgs)), num_samples)\n",
    "\n",
    "f, axs = plt.subplots(num_samples, 2, figsize=(12, 5*num_samples))\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    if num_samples == 1:\n",
    "        ax_row = axs\n",
    "    else:\n",
    "        ax_row = axs[i]\n",
    "        \n",
    "    ax_row[0].imshow(imgs[idx], cmap='gray')\n",
    "    ax_row[0].set_title(f\"Input: {filenames[idx]}\", fontsize=14)\n",
    "    ax_row[0].axis('off')\n",
    "    \n",
    "    ax_row[1].imshow(overlays[idx], cmap='gray')\n",
    "    ax_row[1].set_title(f\"Fovea Detection\", fontsize=14)\n",
    "    ax_row[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('multiple_fovea_predictions.png')\n",
    "plt.show()\n",
    "\n",
    "# 3. Grid view of all detections\n",
    "grid_size = min(16, len(imgs))\n",
    "grid_indices = random.sample(range(len(imgs)), grid_size)\n",
    "\n",
    "rows = int(np.ceil(np.sqrt(grid_size)))\n",
    "cols = int(np.ceil(grid_size / rows))\n",
    "\n",
    "f, axs = plt.subplots(rows, cols, figsize=(16, 16))\n",
    "axs = np.atleast_2d(axs)  # Ensure 2D even with single row/column\n",
    "\n",
    "for i, idx in enumerate(grid_indices):\n",
    "    row, col = i // cols, i % cols\n",
    "    axs[row, col].imshow(overlays[idx], cmap='gray')\n",
    "    axs[row, col].set_title(f\"{filenames[idx]}\", fontsize=10)\n",
    "    axs[row, col].axis('off')\n",
    "\n",
    "# Turn off any unused subplots\n",
    "for i in range(grid_size, rows * cols):\n",
    "    row, col = i // cols, i % cols\n",
    "    axs[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('grid_predictions.png')\n",
    "plt.show()\n",
    "\n",
    "# 4. Threshold comparison for a single image\n",
    "random_idx = random.randint(0, len(imgs)-1)\n",
    "img = imgs[random_idx]\n",
    "\n",
    "\n",
    "##################s\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Load Model (Assume already defined and trained) ===\n",
    "model.eval()\n",
    "\n",
    "# === Paths ===\n",
    "enface_root = \"enface_legend\"\n",
    "subfolders = [os.path.join(enface_root, d) for d in os.listdir(enface_root) if os.path.isdir(os.path.join(enface_root, d))]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def overlay_mask(image, mask, alpha=0.4):\n",
    "    mask_bin = (mask > 0.5).astype(np.float32)\n",
    "    overlay = image.astype(np.float32).copy()\n",
    "    overlay[mask_bin > 0] = overlay[mask_bin > 0] * (1 - alpha) + alpha * 255\n",
    "    return overlay.clip(0, 255).astype(np.uint8)\n",
    "\n",
    "def get_valid_samples(num_needed=24):\n",
    "    samples = []\n",
    "    random.shuffle(subfolders)\n",
    "    for folder in subfolders:\n",
    "        raw_path = os.path.join(folder, \"raw enface\")\n",
    "        mask_path = os.path.join(folder, \"enface mask\")\n",
    "\n",
    "        try:\n",
    "            img_file = next(f for f in os.listdir(raw_path) if f.lower().endswith(('.jpg', '.jpeg', '.png')))\n",
    "            mask_file = next(f for f in os.listdir(mask_path) if f.lower().endswith(('.jpg', '.jpeg', '.png')))\n",
    "        except StopIteration:\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(raw_path, img_file)\n",
    "        mask_path = os.path.join(mask_path, mask_file)\n",
    "\n",
    "        if os.path.exists(img_path) and os.path.exists(mask_path):\n",
    "            samples.append((img_path, mask_path))\n",
    "        if len(samples) >= num_needed:\n",
    "            break\n",
    "    return samples\n",
    "\n",
    "# === Load Samples ===\n",
    "samples = get_valid_samples(24)\n",
    "\n",
    "# === Display in 4 batches of 6x3 ===\n",
    "for batch in range(4):\n",
    "    fig, axs = plt.subplots(6, 3, figsize=(15, 20))\n",
    "    for row in range(6):\n",
    "        idx = batch * 6 + row\n",
    "        if idx >= len(samples):\n",
    "            for col in range(3):\n",
    "                axs[row, col].axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        img_path, mask_path = samples[idx]\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            mask = Image.open(mask_path).convert(\"L\")\n",
    "        except:\n",
    "            for col in range(3):\n",
    "                axs[row, col].axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        img_np = np.array(img.resize((512, 512)))\n",
    "        img_tensor = transform(img).unsqueeze(0).to(device) * 2 - 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(img_tensor)\n",
    "            pred_sigmoid = torch.sigmoid(pred)[0, 0].cpu().numpy()\n",
    "\n",
    "        # Overlay\n",
    "        pred_overlay = overlay_mask(np.mean(img_np, axis=2), pred_sigmoid)\n",
    "\n",
    "        axs[row, 0].imshow(np.mean(img_np, axis=2), cmap=\"gray\")\n",
    "        axs[row, 0].set_title(\"GT: Input\")\n",
    "        axs[row, 1].imshow(np.array(mask.resize((512, 512))), cmap=\"gray\")\n",
    "        axs[row, 1].set_title(\"GT: Mask\")\n",
    "        axs[row, 2].imshow(pred_overlay, cmap=\"gray\")\n",
    "        axs[row, 2].set_title(\"Pred: Overlay\")\n",
    "\n",
    "        for col in range(3):\n",
    "            axs[row, col].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"grid_{batch}.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d21924",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "train model for longer with smaller lr. maybe that will solve the problem of huge sizes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinical_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
