{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# !ls /content/drive/MyDrive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC4WssNDxDj5",
        "outputId": "5ddaca28-19c7-45f1-fe1e-31a8c1c0b420"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Navigate to your project directory\n",
        "import os\n",
        "\n",
        "# Replace with your actual path inside Google Drive\n",
        "# curated_path = '/content/drive/MyDrive/curated_enface'\n",
        "curated_path = \"/content/drive/MyDrive/curated_enface\"  # correct spelling!\n",
        "!ls /content/drive/MyDrive/curated_enface\n",
        "\n",
        "# save_path = '/content/drive/MyDrive/curated_enface/supreme_vit_mae.pth'\n",
        "save_path = \"/content/drive/MyDrive/curated_enface/xsupreme_vit_mae.pth\"\n",
        "\n",
        "os.chdir(curated_path )\n",
        "\n",
        "# 3. Confirm you're in the right place\n",
        "print(\"Current directory:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B3OXebgxG_a",
        "outputId": "b2f570c1-d61b-4d5e-9e8b-1f29c5cc8410"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1002_left   1007_right\t1011_left   1020_left\t1025_right\n",
            "1003_right  1008_left\t1012_right  1020_right\t1026_right\n",
            "1004_right  1008_right\t1015_left   1021_left\t1028_right\n",
            "1005_left   1009_left\t1017_left   1022_right\t1032_left\n",
            "1006_left   1010_right\t1019_left   1024_left\t1032_right\n",
            "Current directory: /content/drive/MyDrive/curated_enface\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3Sz7owSwp6j",
        "outputId": "5923765f-5ad4-47ea-a030-5c88fbacaefd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Using curated_path: /content/drive/MyDrive/curated_enface\n",
            "Starting MAE training on /content/drive/MyDrive/curated_enface\n",
            "Parameters: batch_size=2, patch_size=32, embed_dim=768\n",
            "Grid size: 16x16, Num patches: 256\n",
            "Patch dim: 3072\n",
            "Model is on device: cuda:0\n",
            "Epoch 0/199\n",
            "  Batch 0: Loss = 0.3120\n",
            "  Batch 10: Loss = 0.2732\n",
            "Epoch 0: Loss = 0.3300, LR = 0.000001\n",
            "Model saved to mae_model_epoch_0.pt\n",
            "Epoch 1/199\n",
            "  Batch 0: Loss = 0.2612\n",
            "  Batch 10: Loss = 0.2372\n",
            "Epoch 1: Loss = 0.2923, LR = 0.000001\n",
            "Epoch 2/199\n",
            "  Batch 0: Loss = 0.2427\n",
            "  Batch 10: Loss = 0.3047\n",
            "Epoch 2: Loss = 0.2685, LR = 0.000001\n",
            "Epoch 3/199\n",
            "  Batch 0: Loss = 0.2175\n",
            "  Batch 10: Loss = 0.2054\n",
            "Epoch 3: Loss = 0.2492, LR = 0.000001\n",
            "Epoch 4/199\n",
            "  Batch 0: Loss = 0.2796\n",
            "  Batch 10: Loss = 0.1879\n",
            "Epoch 4: Loss = 0.2333, LR = 0.000001\n",
            "Epoch 5/199\n",
            "  Batch 0: Loss = 0.4436\n",
            "  Batch 10: Loss = 0.1936\n",
            "Epoch 5: Loss = 0.2183, LR = 0.000001\n",
            "Epoch 6/199\n",
            "  Batch 0: Loss = 0.4363\n",
            "  Batch 10: Loss = 0.1611\n",
            "Epoch 6: Loss = 0.2067, LR = 0.000001\n",
            "Epoch 7/199\n",
            "  Batch 0: Loss = 0.1606\n",
            "  Batch 10: Loss = 0.2454\n",
            "Epoch 7: Loss = 0.1981, LR = 0.000001\n",
            "Epoch 8/199\n",
            "  Batch 0: Loss = 0.1705\n",
            "  Batch 10: Loss = 0.4089\n",
            "Epoch 8: Loss = 0.1896, LR = 0.000001\n",
            "Epoch 9/199\n",
            "  Batch 0: Loss = 0.1669\n",
            "  Batch 10: Loss = 0.2338\n",
            "Epoch 9: Loss = 0.1831, LR = 0.000001\n",
            "Epoch 10/199\n",
            "  Batch 0: Loss = 0.1518\n",
            "  Batch 10: Loss = 0.1328\n",
            "Epoch 10: Loss = 0.1753, LR = 0.000001\n",
            "Epoch 11/199\n",
            "  Batch 0: Loss = 0.1434\n",
            "  Batch 10: Loss = 0.3706\n",
            "Epoch 11: Loss = 0.1767, LR = 0.000001\n",
            "Epoch 12/199\n",
            "  Batch 0: Loss = 0.1296\n",
            "  Batch 10: Loss = 0.1163\n",
            "Epoch 12: Loss = 0.1714, LR = 0.000001\n",
            "Epoch 13/199\n",
            "  Batch 0: Loss = 0.1370\n",
            "  Batch 10: Loss = 0.1180\n",
            "Epoch 13: Loss = 0.1590, LR = 0.000001\n",
            "Epoch 14/199\n",
            "  Batch 0: Loss = 0.1997\n",
            "  Batch 10: Loss = 0.1112\n",
            "Epoch 14: Loss = 0.1546, LR = 0.000001\n",
            "Epoch 15/199\n",
            "  Batch 0: Loss = 0.2031\n",
            "  Batch 10: Loss = 0.3428\n",
            "Epoch 15: Loss = 0.1507, LR = 0.000001\n",
            "Epoch 16/199\n",
            "  Batch 0: Loss = 0.1893\n",
            "  Batch 10: Loss = 0.1404\n",
            "Epoch 16: Loss = 0.1465, LR = 0.000001\n",
            "Epoch 17/199\n",
            "  Batch 0: Loss = 0.1089\n",
            "  Batch 10: Loss = 0.1096\n",
            "Epoch 17: Loss = 0.1438, LR = 0.000001\n",
            "Epoch 18/199\n",
            "  Batch 0: Loss = 0.1165\n",
            "  Batch 10: Loss = 0.0974\n",
            "Epoch 18: Loss = 0.1456, LR = 0.000001\n",
            "Epoch 19/199\n",
            "  Batch 0: Loss = 0.1898\n",
            "  Batch 10: Loss = 0.1075\n",
            "Epoch 19: Loss = 0.1373, LR = 0.000001\n",
            "Epoch 20/199\n",
            "  Batch 0: Loss = 0.0979\n",
            "  Batch 10: Loss = 0.1035\n",
            "Epoch 20: Loss = 0.1324, LR = 0.000001\n",
            "Model saved to mae_model_epoch_20.pt\n",
            "Epoch 21/199\n",
            "  Batch 0: Loss = 0.0974\n",
            "  Batch 10: Loss = 0.1172\n",
            "Epoch 21: Loss = 0.1354, LR = 0.000001\n",
            "Epoch 22/199\n",
            "  Batch 0: Loss = 0.0991\n",
            "  Batch 10: Loss = 0.1197\n",
            "Epoch 22: Loss = 0.1274, LR = 0.000001\n",
            "Epoch 23/199\n",
            "  Batch 0: Loss = 0.1007\n",
            "  Batch 10: Loss = 0.1252\n",
            "Epoch 23: Loss = 0.1232, LR = 0.000001\n",
            "Epoch 24/199\n",
            "  Batch 0: Loss = 0.1526\n",
            "  Batch 10: Loss = 0.0964\n",
            "Epoch 24: Loss = 0.1213, LR = 0.000001\n",
            "Epoch 25/199\n",
            "  Batch 0: Loss = 0.1025\n",
            "  Batch 10: Loss = 0.1033\n",
            "Epoch 25: Loss = 0.1189, LR = 0.000001\n",
            "Epoch 26/199\n",
            "  Batch 0: Loss = 0.1443\n",
            "  Batch 10: Loss = 0.0861\n",
            "Epoch 26: Loss = 0.1149, LR = 0.000001\n",
            "Epoch 27/199\n",
            "  Batch 0: Loss = 0.0939\n",
            "  Batch 10: Loss = 0.0823\n",
            "Epoch 27: Loss = 0.1127, LR = 0.000001\n",
            "Epoch 28/199\n",
            "  Batch 0: Loss = 0.1649\n",
            "  Batch 10: Loss = 0.1388\n",
            "Epoch 28: Loss = 0.1101, LR = 0.000001\n",
            "Epoch 29/199\n",
            "  Batch 0: Loss = 0.1432\n",
            "  Batch 10: Loss = 0.0837\n",
            "Epoch 29: Loss = 0.1088, LR = 0.000001\n",
            "Epoch 30/199\n",
            "  Batch 0: Loss = 0.1445\n",
            "  Batch 10: Loss = 0.2785\n",
            "Epoch 30: Loss = 0.1060, LR = 0.000001\n",
            "Epoch 31/199\n",
            "  Batch 0: Loss = 0.0725\n",
            "  Batch 10: Loss = 0.0662\n",
            "Epoch 31: Loss = 0.1088, LR = 0.000001\n",
            "Epoch 32/199\n",
            "  Batch 0: Loss = 0.0909\n",
            "  Batch 10: Loss = 0.1532\n",
            "Epoch 32: Loss = 0.1012, LR = 0.000001\n",
            "Epoch 33/199\n",
            "  Batch 0: Loss = 0.0772\n",
            "  Batch 10: Loss = 0.0736\n",
            "Epoch 33: Loss = 0.1005, LR = 0.000001\n",
            "Epoch 34/199\n",
            "  Batch 0: Loss = 0.0917\n",
            "  Batch 10: Loss = 0.0670\n",
            "Epoch 34: Loss = 0.0974, LR = 0.000001\n",
            "Epoch 35/199\n",
            "  Batch 0: Loss = 0.0630\n",
            "  Batch 10: Loss = 0.0594\n",
            "Epoch 35: Loss = 0.0945, LR = 0.000001\n",
            "Epoch 36/199\n",
            "  Batch 0: Loss = 0.0606\n",
            "  Batch 10: Loss = 0.1180\n",
            "Epoch 36: Loss = 0.0932, LR = 0.000001\n",
            "Epoch 37/199\n",
            "  Batch 0: Loss = 0.0784\n",
            "  Batch 10: Loss = 0.0744\n",
            "Epoch 37: Loss = 0.0922, LR = 0.000001\n",
            "Epoch 38/199\n",
            "  Batch 0: Loss = 0.0700\n",
            "  Batch 10: Loss = 0.0631\n",
            "Epoch 38: Loss = 0.0897, LR = 0.000001\n",
            "Epoch 39/199\n",
            "  Batch 0: Loss = 0.1310\n",
            "  Batch 10: Loss = 0.0712\n",
            "Epoch 39: Loss = 0.0884, LR = 0.000001\n",
            "Epoch 40/199\n",
            "  Batch 0: Loss = 0.0613\n",
            "  Batch 10: Loss = 0.0523\n",
            "Epoch 40: Loss = 0.0868, LR = 0.000001\n",
            "Model saved to mae_model_epoch_40.pt\n",
            "Epoch 41/199\n",
            "  Batch 0: Loss = 0.0454\n",
            "  Batch 10: Loss = 0.1234\n",
            "Epoch 41: Loss = 0.0849, LR = 0.000001\n",
            "Epoch 42/199\n",
            "  Batch 0: Loss = 0.0503\n",
            "  Batch 10: Loss = 0.0711\n",
            "Epoch 42: Loss = 0.0836, LR = 0.000001\n",
            "Epoch 43/199\n",
            "  Batch 0: Loss = 0.0707\n",
            "  Batch 10: Loss = 0.0509\n",
            "Epoch 43: Loss = 0.0824, LR = 0.000001\n",
            "Epoch 44/199\n",
            "  Batch 0: Loss = 0.2419\n",
            "  Batch 10: Loss = 0.1049\n",
            "Epoch 44: Loss = 0.0799, LR = 0.000001\n",
            "Epoch 45/199\n",
            "  Batch 0: Loss = 0.0613\n",
            "  Batch 10: Loss = 0.1112\n",
            "Epoch 45: Loss = 0.0792, LR = 0.000001\n",
            "Epoch 46/199\n",
            "  Batch 0: Loss = 0.0569\n",
            "  Batch 10: Loss = 0.0459\n",
            "Epoch 46: Loss = 0.0774, LR = 0.000001\n",
            "Epoch 47/199\n",
            "  Batch 0: Loss = 0.0569\n",
            "  Batch 10: Loss = 0.0410\n",
            "Epoch 47: Loss = 0.0760, LR = 0.000001\n",
            "Epoch 48/199\n",
            "  Batch 0: Loss = 0.0456\n",
            "  Batch 10: Loss = 0.0465\n",
            "Epoch 48: Loss = 0.0751, LR = 0.000001\n",
            "Epoch 49/199\n",
            "  Batch 0: Loss = 0.0458\n",
            "  Batch 10: Loss = 0.0377\n",
            "Epoch 49: Loss = 0.0742, LR = 0.000001\n",
            "Epoch 50/199\n",
            "  Batch 0: Loss = 0.0421\n",
            "  Batch 10: Loss = 0.0934\n",
            "Epoch 50: Loss = 0.0727, LR = 0.000001\n",
            "Epoch 51/199\n",
            "  Batch 0: Loss = 0.0966\n",
            "  Batch 10: Loss = 0.0507\n",
            "Epoch 51: Loss = 0.0716, LR = 0.000001\n",
            "Epoch 52/199\n",
            "  Batch 0: Loss = 0.0931\n",
            "  Batch 10: Loss = 0.0746\n",
            "Epoch 52: Loss = 0.0717, LR = 0.000001\n",
            "Epoch 53/199\n",
            "  Batch 0: Loss = 0.0506\n",
            "  Batch 10: Loss = 0.0347\n",
            "Epoch 53: Loss = 0.0716, LR = 0.000001\n",
            "Epoch 54/199\n",
            "  Batch 0: Loss = 0.0546\n",
            "  Batch 10: Loss = 0.1151\n",
            "Epoch 54: Loss = 0.0729, LR = 0.000001\n",
            "Epoch 55/199\n",
            "  Batch 0: Loss = 0.0538\n",
            "  Batch 10: Loss = 0.0573\n",
            "Epoch 55: Loss = 0.0680, LR = 0.000001\n",
            "Epoch 56/199\n",
            "  Batch 0: Loss = 0.2232\n",
            "  Batch 10: Loss = 0.0322\n",
            "Epoch 56: Loss = 0.0713, LR = 0.000001\n",
            "Epoch 57/199\n",
            "  Batch 0: Loss = 0.0686\n",
            "  Batch 10: Loss = 0.0366\n",
            "Epoch 57: Loss = 0.0665, LR = 0.000001\n",
            "Epoch 58/199\n",
            "  Batch 0: Loss = 0.0472\n",
            "  Batch 10: Loss = 0.0335\n",
            "Epoch 58: Loss = 0.0650, LR = 0.000001\n",
            "Epoch 59/199\n",
            "  Batch 0: Loss = 0.0922\n",
            "  Batch 10: Loss = 0.0500\n",
            "Epoch 59: Loss = 0.0645, LR = 0.000001\n",
            "Epoch 60/199\n",
            "  Batch 0: Loss = 0.2279\n",
            "  Batch 10: Loss = 0.0328\n",
            "Epoch 60: Loss = 0.0681, LR = 0.000001\n",
            "Model saved to mae_model_epoch_60.pt\n",
            "Epoch 61/199\n",
            "  Batch 0: Loss = 0.2207\n",
            "  Batch 10: Loss = 0.0315\n",
            "Epoch 61: Loss = 0.0629, LR = 0.000001\n",
            "Epoch 62/199\n",
            "  Batch 0: Loss = 0.0415\n",
            "  Batch 10: Loss = 0.0366\n",
            "Epoch 62: Loss = 0.0761, LR = 0.000001\n",
            "Epoch 63/199\n",
            "  Batch 0: Loss = 0.0647\n",
            "  Batch 10: Loss = 0.0394\n",
            "Epoch 63: Loss = 0.0618, LR = 0.000001\n",
            "Epoch 64/199\n",
            "  Batch 0: Loss = 0.0350\n",
            "  Batch 10: Loss = 0.0430\n",
            "Epoch 64: Loss = 0.0626, LR = 0.000001\n",
            "Epoch 65/199\n",
            "  Batch 0: Loss = 0.0346\n",
            "  Batch 10: Loss = 0.2548\n",
            "Epoch 65: Loss = 0.0606, LR = 0.000001\n",
            "Epoch 66/199\n",
            "  Batch 0: Loss = 0.0492\n",
            "  Batch 10: Loss = 0.0293\n",
            "Epoch 66: Loss = 0.0605, LR = 0.000001\n",
            "Epoch 67/199\n",
            "  Batch 0: Loss = 0.0246\n",
            "  Batch 10: Loss = 0.0764\n",
            "Epoch 67: Loss = 0.0597, LR = 0.000001\n",
            "Epoch 68/199\n",
            "  Batch 0: Loss = 0.0339\n",
            "  Batch 10: Loss = 0.0404\n",
            "Epoch 68: Loss = 0.0584, LR = 0.000001\n",
            "Epoch 69/199\n",
            "  Batch 0: Loss = 0.2200\n",
            "  Batch 10: Loss = 0.0361\n",
            "Epoch 69: Loss = 0.0593, LR = 0.000001\n",
            "Epoch 70/199\n",
            "  Batch 0: Loss = 0.0999\n",
            "  Batch 10: Loss = 0.0386\n",
            "Epoch 70: Loss = 0.0581, LR = 0.000001\n",
            "Epoch 71/199\n",
            "  Batch 0: Loss = 0.0577\n",
            "  Batch 10: Loss = 0.0315\n",
            "Epoch 71: Loss = 0.0574, LR = 0.000001\n",
            "Epoch 72/199\n",
            "  Batch 0: Loss = 0.0497\n",
            "  Batch 10: Loss = 0.0805\n",
            "Epoch 72: Loss = 0.0571, LR = 0.000001\n",
            "Epoch 73/199\n",
            "  Batch 0: Loss = 0.0823\n",
            "  Batch 10: Loss = 0.2010\n",
            "Epoch 73: Loss = 0.0561, LR = 0.000001\n",
            "Epoch 74/199\n",
            "  Batch 0: Loss = 0.2008\n",
            "  Batch 10: Loss = 0.0697\n",
            "Epoch 74: Loss = 0.0560, LR = 0.000001\n",
            "Epoch 75/199\n",
            "  Batch 0: Loss = 0.0298\n",
            "  Batch 10: Loss = 0.0481\n",
            "Epoch 75: Loss = 0.0554, LR = 0.000001\n",
            "Epoch 76/199\n",
            "  Batch 0: Loss = 0.0235\n",
            "  Batch 10: Loss = 0.0267\n",
            "Epoch 76: Loss = 0.0545, LR = 0.000001\n",
            "Epoch 77/199\n",
            "  Batch 0: Loss = 0.1971\n",
            "  Batch 10: Loss = 0.0421\n",
            "Epoch 77: Loss = 0.0551, LR = 0.000001\n",
            "Epoch 78/199\n",
            "  Batch 0: Loss = 0.0406\n",
            "  Batch 10: Loss = 0.0539\n",
            "Epoch 78: Loss = 0.0547, LR = 0.000001\n",
            "Epoch 79/199\n",
            "  Batch 0: Loss = 0.1931\n",
            "  Batch 10: Loss = 0.0317\n",
            "Epoch 79: Loss = 0.0545, LR = 0.000001\n",
            "Epoch 80/199\n",
            "  Batch 0: Loss = 0.0589\n",
            "  Batch 10: Loss = 0.0174\n",
            "Epoch 80: Loss = 0.0537, LR = 0.000001\n",
            "Model saved to mae_model_epoch_80.pt\n",
            "Epoch 81/199\n",
            "  Batch 0: Loss = 0.0305\n",
            "  Batch 10: Loss = 0.1861\n",
            "Epoch 81: Loss = 0.0546, LR = 0.000001\n",
            "Epoch 82/199\n",
            "  Batch 0: Loss = 0.0254\n",
            "  Batch 10: Loss = 0.0288\n",
            "Epoch 82: Loss = 0.0527, LR = 0.000001\n",
            "Epoch 83/199\n",
            "  Batch 0: Loss = 0.0244\n",
            "  Batch 10: Loss = 0.0477\n",
            "Epoch 83: Loss = 0.0526, LR = 0.000001\n",
            "Epoch 84/199\n",
            "  Batch 0: Loss = 0.0215\n",
            "  Batch 10: Loss = 0.0346\n",
            "Epoch 84: Loss = 0.0524, LR = 0.000001\n",
            "Epoch 85/199\n",
            "  Batch 0: Loss = 0.0900\n",
            "  Batch 10: Loss = 0.0239\n",
            "Epoch 85: Loss = 0.0529, LR = 0.000001\n",
            "Epoch 86/199\n",
            "  Batch 0: Loss = 0.0464\n",
            "  Batch 10: Loss = 0.0424\n",
            "Epoch 86: Loss = 0.0519, LR = 0.000001\n",
            "Epoch 87/199\n",
            "  Batch 0: Loss = 0.0350\n",
            "  Batch 10: Loss = 0.0316\n",
            "Epoch 87: Loss = 0.0634, LR = 0.000001\n",
            "Epoch 88/199\n",
            "  Batch 0: Loss = 0.0891\n",
            "  Batch 10: Loss = 0.0349\n",
            "Epoch 88: Loss = 0.0514, LR = 0.000001\n",
            "Epoch 89/199\n",
            "  Batch 0: Loss = 0.0246\n",
            "  Batch 10: Loss = 0.0629\n",
            "Epoch 89: Loss = 0.0521, LR = 0.000001\n",
            "Epoch 90/199\n",
            "  Batch 0: Loss = 0.0365\n",
            "  Batch 10: Loss = 0.0405\n",
            "Epoch 90: Loss = 0.0626, LR = 0.000001\n",
            "Epoch 91/199\n",
            "  Batch 0: Loss = 0.0328\n",
            "  Batch 10: Loss = 0.0210\n",
            "Epoch 91: Loss = 0.0513, LR = 0.000001\n",
            "Epoch 92/199\n",
            "  Batch 0: Loss = 0.0261\n",
            "  Batch 10: Loss = 0.0267\n",
            "Epoch 92: Loss = 0.0499, LR = 0.000001\n",
            "Epoch 93/199\n",
            "  Batch 0: Loss = 0.0209\n",
            "  Batch 10: Loss = 0.0680\n",
            "Epoch 93: Loss = 0.0507, LR = 0.000001\n",
            "Epoch 94/199\n",
            "  Batch 0: Loss = 0.0179\n",
            "  Batch 10: Loss = 0.0297\n",
            "Epoch 94: Loss = 0.0503, LR = 0.000001\n",
            "Epoch 95/199\n",
            "  Batch 0: Loss = 0.0306\n",
            "  Batch 10: Loss = 0.0544\n",
            "Epoch 95: Loss = 0.0505, LR = 0.000001\n",
            "Epoch 96/199\n",
            "  Batch 0: Loss = 0.0152\n",
            "  Batch 10: Loss = 0.0241\n",
            "Epoch 96: Loss = 0.0529, LR = 0.000001\n",
            "Epoch 97/199\n",
            "  Batch 0: Loss = 0.0286\n",
            "  Batch 10: Loss = 0.0630\n",
            "Epoch 97: Loss = 0.0489, LR = 0.000001\n",
            "Epoch 98/199\n",
            "  Batch 0: Loss = 0.0236\n",
            "  Batch 10: Loss = 0.0351\n",
            "Epoch 98: Loss = 0.0487, LR = 0.000001\n",
            "Epoch 99/199\n",
            "  Batch 0: Loss = 0.0235\n",
            "  Batch 10: Loss = 0.0424\n",
            "Epoch 99: Loss = 0.0493, LR = 0.000001\n",
            "Epoch 100/199\n",
            "  Batch 0: Loss = 0.0553\n",
            "  Batch 10: Loss = 0.0152\n",
            "Epoch 100: Loss = 0.0498, LR = 0.000001\n",
            "Model saved to mae_model_epoch_100.pt\n",
            "Epoch 101/199\n",
            "  Batch 0: Loss = 0.0354\n",
            "  Batch 10: Loss = 0.0590\n",
            "Epoch 101: Loss = 0.0491, LR = 0.000001\n",
            "Epoch 102/199\n",
            "  Batch 0: Loss = 0.1665\n",
            "  Batch 10: Loss = 0.0778\n",
            "Epoch 102: Loss = 0.0474, LR = 0.000001\n",
            "Epoch 103/199\n",
            "  Batch 0: Loss = 0.0616\n",
            "  Batch 10: Loss = 0.0268\n",
            "Epoch 103: Loss = 0.0591, LR = 0.000001\n",
            "Epoch 104/199\n",
            "  Batch 0: Loss = 0.0257\n",
            "  Batch 10: Loss = 0.1677\n",
            "Epoch 104: Loss = 0.0503, LR = 0.000001\n",
            "Epoch 105/199\n",
            "  Batch 0: Loss = 0.0240\n",
            "  Batch 10: Loss = 0.0272\n",
            "Epoch 105: Loss = 0.0475, LR = 0.000001\n",
            "Epoch 106/199\n",
            "  Batch 0: Loss = 0.0440\n",
            "  Batch 10: Loss = 0.0484\n",
            "Epoch 106: Loss = 0.0468, LR = 0.000001\n",
            "Epoch 107/199\n",
            "  Batch 0: Loss = 0.0856\n",
            "  Batch 10: Loss = 0.0356\n",
            "Epoch 107: Loss = 0.0476, LR = 0.000001\n",
            "Epoch 108/199\n",
            "  Batch 0: Loss = 0.0353\n",
            "  Batch 10: Loss = 0.0215\n",
            "Epoch 108: Loss = 0.0471, LR = 0.000001\n",
            "Epoch 109/199\n",
            "  Batch 0: Loss = 0.0587\n",
            "  Batch 10: Loss = 0.0321\n",
            "Epoch 109: Loss = 0.0481, LR = 0.000001\n",
            "Epoch 110/199\n",
            "  Batch 0: Loss = 0.0394\n",
            "  Batch 10: Loss = 0.0635\n",
            "Epoch 110: Loss = 0.0464, LR = 0.000001\n",
            "Epoch 111/199\n",
            "  Batch 0: Loss = 0.0604\n",
            "  Batch 10: Loss = 0.0228\n",
            "Epoch 111: Loss = 0.0466, LR = 0.000001\n",
            "Epoch 112/199\n",
            "  Batch 0: Loss = 0.0373\n",
            "  Batch 10: Loss = 0.0207\n",
            "Epoch 112: Loss = 0.0465, LR = 0.000001\n",
            "Epoch 113/199\n",
            "  Batch 0: Loss = 0.0377\n",
            "  Batch 10: Loss = 0.0331\n",
            "Epoch 113: Loss = 0.0461, LR = 0.000001\n",
            "Epoch 114/199\n",
            "  Batch 0: Loss = 0.0183\n",
            "  Batch 10: Loss = 0.0362\n",
            "Epoch 114: Loss = 0.0460, LR = 0.000001\n",
            "Epoch 115/199\n",
            "  Batch 0: Loss = 0.0243\n",
            "  Batch 10: Loss = 0.0260\n",
            "Epoch 115: Loss = 0.0469, LR = 0.000001\n",
            "Epoch 116/199\n",
            "  Batch 0: Loss = 0.1631\n",
            "  Batch 10: Loss = 0.0335\n",
            "Epoch 116: Loss = 0.0468, LR = 0.000001\n",
            "Epoch 117/199\n",
            "  Batch 0: Loss = 0.0206\n",
            "  Batch 10: Loss = 0.0376\n",
            "Epoch 117: Loss = 0.0482, LR = 0.000001\n",
            "Epoch 118/199\n",
            "  Batch 0: Loss = 0.1596\n",
            "  Batch 10: Loss = 0.0316\n",
            "Epoch 118: Loss = 0.0467, LR = 0.000001\n",
            "Epoch 119/199\n",
            "  Batch 0: Loss = 0.0243\n",
            "  Batch 10: Loss = 0.0712\n",
            "Epoch 119: Loss = 0.0457, LR = 0.000001\n",
            "Epoch 120/199\n",
            "  Batch 0: Loss = 0.0461\n",
            "  Batch 10: Loss = 0.0268\n",
            "Epoch 120: Loss = 0.0466, LR = 0.000001\n",
            "Model saved to mae_model_epoch_120.pt\n",
            "Epoch 121/199\n",
            "  Batch 0: Loss = 0.0553\n",
            "  Batch 10: Loss = 0.0212\n",
            "Epoch 121: Loss = 0.0464, LR = 0.000001\n",
            "Epoch 122/199\n",
            "  Batch 0: Loss = 0.0224\n",
            "  Batch 10: Loss = 0.0806\n",
            "Epoch 122: Loss = 0.0455, LR = 0.000001\n",
            "Epoch 123/199\n",
            "  Batch 0: Loss = 0.0393\n",
            "  Batch 10: Loss = 0.0230\n",
            "Epoch 123: Loss = 0.0461, LR = 0.000001\n",
            "Epoch 124/199\n",
            "  Batch 0: Loss = 0.0181\n",
            "  Batch 10: Loss = 0.0210\n",
            "Epoch 124: Loss = 0.0483, LR = 0.000001\n",
            "Epoch 125/199\n",
            "  Batch 0: Loss = 0.0235\n",
            "  Batch 10: Loss = 0.0281\n",
            "Epoch 125: Loss = 0.0445, LR = 0.000001\n",
            "Epoch 126/199\n",
            "  Batch 0: Loss = 0.0526\n",
            "  Batch 10: Loss = 0.0262\n",
            "Epoch 126: Loss = 0.0446, LR = 0.000001\n",
            "Epoch 127/199\n",
            "  Batch 0: Loss = 0.0311\n",
            "  Batch 10: Loss = 0.0271\n",
            "Epoch 127: Loss = 0.0452, LR = 0.000001\n",
            "Epoch 128/199\n",
            "  Batch 0: Loss = 0.0224\n",
            "  Batch 10: Loss = 0.0309\n",
            "Epoch 128: Loss = 0.0449, LR = 0.000000\n",
            "Epoch 129/199\n",
            "  Batch 0: Loss = 0.0203\n",
            "  Batch 10: Loss = 0.0449\n",
            "Epoch 129: Loss = 0.0471, LR = 0.000000\n",
            "Epoch 130/199\n",
            "  Batch 0: Loss = 0.0269\n",
            "  Batch 10: Loss = 0.0201\n",
            "Epoch 130: Loss = 0.0443, LR = 0.000000\n",
            "Epoch 131/199\n",
            "  Batch 0: Loss = 0.0188\n",
            "  Batch 10: Loss = 0.1506\n",
            "Epoch 131: Loss = 0.0443, LR = 0.000000\n",
            "Epoch 132/199\n",
            "  Batch 0: Loss = 0.0186\n",
            "  Batch 10: Loss = 0.0234\n",
            "Epoch 132: Loss = 0.0444, LR = 0.000000\n",
            "Epoch 133/199\n",
            "  Batch 0: Loss = 0.0411\n",
            "  Batch 10: Loss = 0.0582\n",
            "Epoch 133: Loss = 0.0444, LR = 0.000000\n",
            "Epoch 134/199\n",
            "  Batch 0: Loss = 0.0676\n",
            "  Batch 10: Loss = 0.0226\n",
            "Epoch 134: Loss = 0.0453, LR = 0.000000\n",
            "Epoch 135/199\n",
            "  Batch 0: Loss = 0.0331\n",
            "  Batch 10: Loss = 0.0300\n",
            "Epoch 135: Loss = 0.0448, LR = 0.000000\n",
            "Epoch 136/199\n",
            "  Batch 0: Loss = 0.0283\n",
            "  Batch 10: Loss = 0.0348\n",
            "Epoch 136: Loss = 0.0467, LR = 0.000000\n",
            "Epoch 137/199\n",
            "  Batch 0: Loss = 0.0629\n",
            "  Batch 10: Loss = 0.0578\n",
            "Epoch 137: Loss = 0.0444, LR = 0.000000\n",
            "Epoch 138/199\n",
            "  Batch 0: Loss = 0.0408\n",
            "  Batch 10: Loss = 0.1446\n",
            "Epoch 138: Loss = 0.0433, LR = 0.000000\n",
            "Epoch 139/199\n",
            "  Batch 0: Loss = 0.0213\n",
            "  Batch 10: Loss = 0.0397\n",
            "Epoch 139: Loss = 0.0437, LR = 0.000000\n",
            "Epoch 140/199\n",
            "  Batch 0: Loss = 0.0251\n",
            "  Batch 10: Loss = 0.0220\n",
            "Epoch 140: Loss = 0.0431, LR = 0.000000\n",
            "Model saved to mae_model_epoch_140.pt\n",
            "Epoch 141/199\n",
            "  Batch 0: Loss = 0.0278\n",
            "  Batch 10: Loss = 0.0533\n",
            "Epoch 141: Loss = 0.0430, LR = 0.000000\n",
            "Epoch 142/199\n",
            "  Batch 0: Loss = 0.0341\n",
            "  Batch 10: Loss = 0.0262\n",
            "Epoch 142: Loss = 0.0433, LR = 0.000000\n",
            "Epoch 143/199\n",
            "  Batch 0: Loss = 0.0468\n",
            "  Batch 10: Loss = 0.1793\n",
            "Epoch 143: Loss = 0.0438, LR = 0.000000\n",
            "Epoch 144/199\n",
            "  Batch 0: Loss = 0.0317\n",
            "  Batch 10: Loss = 0.0196\n",
            "Epoch 144: Loss = 0.0439, LR = 0.000000\n",
            "Epoch 145/199\n",
            "  Batch 0: Loss = 0.0241\n",
            "  Batch 10: Loss = 0.0360\n",
            "Epoch 145: Loss = 0.0449, LR = 0.000000\n",
            "Epoch 146/199\n",
            "  Batch 0: Loss = 0.0331\n",
            "  Batch 10: Loss = 0.0247\n",
            "Epoch 146: Loss = 0.0430, LR = 0.000000\n",
            "Epoch 147/199\n",
            "  Batch 0: Loss = 0.0295\n",
            "  Batch 10: Loss = 0.0253\n",
            "Epoch 147: Loss = 0.0439, LR = 0.000000\n",
            "Epoch 148/199\n",
            "  Batch 0: Loss = 0.0220\n",
            "  Batch 10: Loss = 0.0308\n",
            "Epoch 148: Loss = 0.0443, LR = 0.000000\n",
            "Epoch 149/199\n",
            "  Batch 0: Loss = 0.0631\n",
            "  Batch 10: Loss = 0.0430\n",
            "Epoch 149: Loss = 0.0451, LR = 0.000000\n",
            "Epoch 150/199\n",
            "  Batch 0: Loss = 0.0437\n",
            "  Batch 10: Loss = 0.0278\n",
            "Epoch 150: Loss = 0.0422, LR = 0.000000\n",
            "Epoch 151/199\n",
            "  Batch 0: Loss = 0.0354\n",
            "  Batch 10: Loss = 0.0516\n",
            "Epoch 151: Loss = 0.0436, LR = 0.000000\n",
            "Epoch 152/199\n",
            "  Batch 0: Loss = 0.0261\n",
            "  Batch 10: Loss = 0.0382\n",
            "Epoch 152: Loss = 0.0419, LR = 0.000000\n",
            "Epoch 153/199\n",
            "  Batch 0: Loss = 0.0281\n",
            "  Batch 10: Loss = 0.0244\n",
            "Epoch 153: Loss = 0.0431, LR = 0.000000\n",
            "Epoch 154/199\n",
            "  Batch 0: Loss = 0.0326\n",
            "  Batch 10: Loss = 0.0615\n",
            "Epoch 154: Loss = 0.0429, LR = 0.000000\n",
            "Epoch 155/199\n",
            "  Batch 0: Loss = 0.0273\n",
            "  Batch 10: Loss = 0.0208\n",
            "Epoch 155: Loss = 0.0430, LR = 0.000000\n",
            "Epoch 156/199\n",
            "  Batch 0: Loss = 0.0409\n",
            "  Batch 10: Loss = 0.0685\n",
            "Epoch 156: Loss = 0.0433, LR = 0.000000\n",
            "Epoch 157/199\n",
            "  Batch 0: Loss = 0.0165\n",
            "  Batch 10: Loss = 0.0701\n",
            "Epoch 157: Loss = 0.0429, LR = 0.000000\n",
            "Epoch 158/199\n",
            "  Batch 0: Loss = 0.0227\n",
            "  Batch 10: Loss = 0.0340\n",
            "Epoch 158: Loss = 0.0428, LR = 0.000000\n",
            "Epoch 159/199\n",
            "  Batch 0: Loss = 0.0322\n",
            "  Batch 10: Loss = 0.0335\n",
            "Epoch 159: Loss = 0.0422, LR = 0.000000\n",
            "Epoch 160/199\n",
            "  Batch 0: Loss = 0.0273\n",
            "  Batch 10: Loss = 0.0546\n",
            "Epoch 160: Loss = 0.0514, LR = 0.000000\n",
            "Model saved to mae_model_epoch_160.pt\n",
            "Epoch 161/199\n",
            "  Batch 0: Loss = 0.0286\n",
            "  Batch 10: Loss = 0.1422\n",
            "Epoch 161: Loss = 0.0429, LR = 0.000000\n",
            "Epoch 162/199\n",
            "  Batch 0: Loss = 0.0303\n",
            "  Batch 10: Loss = 0.0297\n",
            "Epoch 162: Loss = 0.0430, LR = 0.000000\n",
            "Epoch 163/199\n",
            "  Batch 0: Loss = 0.0477\n",
            "  Batch 10: Loss = 0.0286\n",
            "Epoch 163: Loss = 0.0438, LR = 0.000000\n",
            "Epoch 164/199\n",
            "  Batch 0: Loss = 0.0228\n",
            "  Batch 10: Loss = 0.0325\n",
            "Epoch 164: Loss = 0.0515, LR = 0.000000\n",
            "Epoch 165/199\n",
            "  Batch 0: Loss = 0.0409\n",
            "  Batch 10: Loss = 0.0396\n",
            "Epoch 165: Loss = 0.0429, LR = 0.000000\n",
            "Epoch 166/199\n",
            "  Batch 0: Loss = 0.0372\n",
            "  Batch 10: Loss = 0.0202\n",
            "Epoch 166: Loss = 0.0425, LR = 0.000000\n",
            "Epoch 167/199\n",
            "  Batch 0: Loss = 0.0545\n",
            "  Batch 10: Loss = 0.0442\n",
            "Epoch 167: Loss = 0.0415, LR = 0.000000\n",
            "Epoch 168/199\n",
            "  Batch 0: Loss = 0.1425\n",
            "  Batch 10: Loss = 0.0538\n",
            "Epoch 168: Loss = 0.0421, LR = 0.000000\n",
            "Epoch 169/199\n",
            "  Batch 0: Loss = 0.1574\n",
            "  Batch 10: Loss = 0.0256\n",
            "Epoch 169: Loss = 0.0420, LR = 0.000000\n",
            "Epoch 170/199\n",
            "  Batch 0: Loss = 0.0182\n",
            "  Batch 10: Loss = 0.0351\n",
            "Epoch 170: Loss = 0.0421, LR = 0.000000\n",
            "Epoch 171/199\n",
            "  Batch 0: Loss = 0.0389\n",
            "  Batch 10: Loss = 0.0333\n",
            "Epoch 171: Loss = 0.0428, LR = 0.000000\n",
            "Epoch 172/199\n",
            "  Batch 0: Loss = 0.0228\n",
            "  Batch 10: Loss = 0.0309\n",
            "Epoch 172: Loss = 0.0513, LR = 0.000000\n",
            "Epoch 173/199\n",
            "  Batch 0: Loss = 0.0512\n",
            "  Batch 10: Loss = 0.0655\n",
            "Epoch 173: Loss = 0.0431, LR = 0.000000\n",
            "Epoch 174/199\n",
            "  Batch 0: Loss = 0.0484\n",
            "  Batch 10: Loss = 0.0257\n",
            "Epoch 174: Loss = 0.0418, LR = 0.000000\n",
            "Epoch 175/199\n",
            "  Batch 0: Loss = 0.0210\n",
            "  Batch 10: Loss = 0.0330\n",
            "Epoch 175: Loss = 0.0421, LR = 0.000000\n",
            "Epoch 176/199\n",
            "  Batch 0: Loss = 0.0281\n",
            "  Batch 10: Loss = 0.0257\n",
            "Epoch 176: Loss = 0.0425, LR = 0.000000\n",
            "Epoch 177/199\n",
            "  Batch 0: Loss = 0.0605\n",
            "  Batch 10: Loss = 0.0406\n",
            "Epoch 177: Loss = 0.0419, LR = 0.000000\n",
            "Epoch 178/199\n",
            "  Batch 0: Loss = 0.0176\n",
            "  Batch 10: Loss = 0.0479\n",
            "Epoch 178: Loss = 0.0414, LR = 0.000000\n",
            "Epoch 179/199\n",
            "  Batch 0: Loss = 0.0548\n",
            "  Batch 10: Loss = 0.0290\n",
            "Epoch 179: Loss = 0.0418, LR = 0.000000\n",
            "Epoch 180/199\n",
            "  Batch 0: Loss = 0.0236\n",
            "  Batch 10: Loss = 0.0355\n",
            "Epoch 180: Loss = 0.0414, LR = 0.000000\n",
            "Model saved to mae_model_epoch_180.pt\n",
            "Epoch 181/199\n",
            "  Batch 0: Loss = 0.0451\n",
            "  Batch 10: Loss = 0.0190\n",
            "Epoch 181: Loss = 0.0418, LR = 0.000000\n",
            "Epoch 182/199\n",
            "  Batch 0: Loss = 0.0214\n",
            "  Batch 10: Loss = 0.0325\n",
            "Epoch 182: Loss = 0.0416, LR = 0.000000\n",
            "Epoch 183/199\n",
            "  Batch 0: Loss = 0.0390\n",
            "  Batch 10: Loss = 0.0458\n",
            "Epoch 183: Loss = 0.0418, LR = 0.000000\n",
            "Epoch 184/199\n",
            "  Batch 0: Loss = 0.0518\n",
            "  Batch 10: Loss = 0.0290\n",
            "Epoch 184: Loss = 0.0417, LR = 0.000000\n",
            "Epoch 185/199\n",
            "  Batch 0: Loss = 0.0275\n",
            "  Batch 10: Loss = 0.0239\n",
            "Epoch 185: Loss = 0.0429, LR = 0.000000\n",
            "Epoch 186/199\n",
            "  Batch 0: Loss = 0.0295\n",
            "  Batch 10: Loss = 0.0206\n",
            "Epoch 186: Loss = 0.0419, LR = 0.000000\n",
            "Epoch 187/199\n",
            "  Batch 0: Loss = 0.0240\n",
            "  Batch 10: Loss = 0.0210\n",
            "Epoch 187: Loss = 0.0415, LR = 0.000000\n",
            "Epoch 188/199\n",
            "  Batch 0: Loss = 0.0456\n",
            "  Batch 10: Loss = 0.0559\n",
            "Epoch 188: Loss = 0.0415, LR = 0.000000\n",
            "Epoch 189/199\n",
            "  Batch 0: Loss = 0.1406\n",
            "  Batch 10: Loss = 0.0229\n",
            "Epoch 189: Loss = 0.0418, LR = 0.000000\n",
            "Epoch 190/199\n",
            "  Batch 0: Loss = 0.0333\n",
            "  Batch 10: Loss = 0.0261\n",
            "Epoch 190: Loss = 0.0411, LR = 0.000000\n",
            "Epoch 191/199\n",
            "  Batch 0: Loss = 0.0212\n",
            "  Batch 10: Loss = 0.0532\n",
            "Epoch 191: Loss = 0.0420, LR = 0.000000\n",
            "Epoch 192/199\n",
            "  Batch 0: Loss = 0.0377\n",
            "  Batch 10: Loss = 0.0212\n",
            "Epoch 192: Loss = 0.0441, LR = 0.000000\n",
            "Epoch 193/199\n",
            "  Batch 0: Loss = 0.0303\n",
            "  Batch 10: Loss = 0.0190\n",
            "Epoch 193: Loss = 0.0418, LR = 0.000000\n",
            "Epoch 194/199\n",
            "  Batch 0: Loss = 0.0478\n",
            "  Batch 10: Loss = 0.0349\n",
            "Epoch 194: Loss = 0.0419, LR = 0.000000\n",
            "Epoch 195/199\n",
            "  Batch 0: Loss = 0.0535\n",
            "  Batch 10: Loss = 0.0260\n",
            "Epoch 195: Loss = 0.0428, LR = 0.000000\n",
            "Epoch 196/199\n",
            "  Batch 0: Loss = 0.0194\n",
            "  Batch 10: Loss = 0.0247\n",
            "Epoch 196: Loss = 0.0420, LR = 0.000000\n",
            "Epoch 197/199\n",
            "  Batch 0: Loss = 0.1527\n",
            "  Batch 10: Loss = 0.0243\n",
            "Epoch 197: Loss = 0.0429, LR = 0.000000\n",
            "Epoch 198/199\n",
            "  Batch 0: Loss = 0.0225\n",
            "  Batch 10: Loss = 0.0638\n",
            "Epoch 198: Loss = 0.0419, LR = 0.000000\n",
            "Epoch 199/199\n",
            "  Batch 0: Loss = 0.0363\n",
            "  Batch 10: Loss = 0.0194\n",
            "Epoch 199: Loss = 0.0441, LR = 0.000000\n",
            "Model saved to mae_model_epoch_199.pt\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# =============== Utility: 2D Sin-Cos Positional Embedding ===============\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size):\n",
        "    \"\"\"Generate 2D positional embeddings with correct dimensions\"\"\"\n",
        "    assert embed_dim % 2 == 0, \"Embedding dimension must be even\"\n",
        "    # Generate grid coordinates for both axes\n",
        "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
        "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
        "\n",
        "    # Get embeddings for each axis\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid_h)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid_w)\n",
        "\n",
        "    # Create 2D embeddings through broadcasting\n",
        "    emb = np.concatenate([\n",
        "        np.repeat(emb_h[:, None, :], grid_size, axis=1),\n",
        "        np.repeat(emb_w[None, :, :], grid_size, axis=0)\n",
        "    ], axis=-1)\n",
        "\n",
        "    return emb.reshape(-1, embed_dim)\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"Generate 1D positional embeddings from grid positions\"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega  # Frequency calculation\n",
        "\n",
        "    # Ensure pos is treated as array even with single value\n",
        "    pos = np.asarray(pos).reshape(-1)\n",
        "    out = np.outer(pos, omega)  # Vectorized calculation\n",
        "\n",
        "    return np.concatenate([np.sin(out), np.cos(out)], axis=1)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# =============== Dataset Loader ===============\n",
        "class EnfaceMaskedDataset(Dataset):\n",
        "    def __init__(self, root_dir, image_size=512):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_size = image_size\n",
        "        if not os.path.isdir(self.root_dir):\n",
        "            raise ValueError(f\"Provided root_dir does not exist: {self.root_dir}\")\n",
        "        self.patients = [d for d in os.listdir(self.root_dir) if os.path.isdir(os.path.join(self.root_dir, d))]\n",
        "        self.augment = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "        ])\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "        self.resize = transforms.Resize((image_size, image_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patients)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        patient = self.patients[idx]\n",
        "        enface_path = os.path.join(self.root_dir, patient, 'raw enface')\n",
        "        if not os.path.exists(enface_path):\n",
        "            raise FileNotFoundError(f\"Missing 'raw enface' folder for patient: {patient}\")\n",
        "        enface_file = next((f for f in os.listdir(enface_path) if f.lower().endswith('.jpg')), None)\n",
        "        if enface_file is None:\n",
        "            raise FileNotFoundError(f\"No JPG image found in: {enface_path}\")\n",
        "        enface_img = Image.open(os.path.join(enface_path, enface_file)).convert('RGB')\n",
        "\n",
        "        enface_img = self.augment(enface_img)\n",
        "        enface_tensor = self.to_tensor(self.resize(enface_img)) * 2 - 1\n",
        "        return enface_tensor\n",
        "\n",
        "# =============== Relative Positional Embeddings ===============\n",
        "def get_rel_pos_index(h, w):\n",
        "    coords = torch.stack(torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\"))\n",
        "    coords_flat = coords.flatten(1)\n",
        "    rel_coords = coords_flat[:, :, None] - coords_flat[:, None, :]\n",
        "    rel_coords = rel_coords.permute(1, 2, 0).contiguous()\n",
        "    rel_coords[:, :, 0] += h - 1\n",
        "    rel_coords[:, :, 1] += w - 1\n",
        "    rel_coords[:, :, 0] *= 2 * w - 1\n",
        "    return rel_coords.sum(-1)\n",
        "\n",
        "class RelativeAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads, grid_size):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = (dim // num_heads) ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "        # Generate relative index for expanded grid\n",
        "        self.rel_index = get_rel_pos_index(grid_size, grid_size)\n",
        "        self.rel_bias = nn.Parameter(\n",
        "            torch.zeros((2 * grid_size - 1) ** 2, num_heads)\n",
        "        )\n",
        "        nn.init.trunc_normal_(self.rel_bias, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # Initialize attention scores\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale  # This line was missing!\n",
        "\n",
        "        # Handle relative bias\n",
        "        rel_bias = self.rel_bias[self.rel_index.to(x.device)]\n",
        "        if N > self.rel_index.shape[0]:\n",
        "            pad_size = N - self.rel_index.shape[0]\n",
        "            rel_bias = F.pad(rel_bias, (0,0,0,pad_size,0,pad_size), value=0)\n",
        "        rel = rel_bias.unsqueeze(0).permute(0, 3, 1, 2)\n",
        "\n",
        "        attn = attn + rel[:, :, :N, :N]  # Now attn is properly initialized\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        return self.proj(out)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, grid_size, mlp_ratio=4., norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = RelativeAttention(dim, num_heads, grid_size)\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(int(dim * mlp_ratio), dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "# =============== MAE Model ===============\n",
        "class MAEModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=512,\n",
        "        patch_size=16,\n",
        "        in_chans=3,\n",
        "        embed_dim=768,\n",
        "        depth=12,\n",
        "        num_heads=12,\n",
        "        decoder_embed_dim=512,\n",
        "        decoder_depth=8,\n",
        "        decoder_num_heads=16,\n",
        "        mlp_ratio=4.,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "        mask_ratio=0.75\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Encoder parameters\n",
        "        self.patch_size = patch_size\n",
        "        self.in_chans = in_chans\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.mask_ratio = mask_ratio\n",
        "\n",
        "        # Save decoder embed dimension as instance variable\n",
        "        self.decoder_embed_dim = decoder_embed_dim\n",
        "        self.decoder_num_heads = decoder_num_heads\n",
        "\n",
        "        # Size info\n",
        "        self.img_size = img_size\n",
        "        self.patch_embed = nn.Conv2d(\n",
        "            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n",
        "        )\n",
        "        self.grid_size = img_size // patch_size\n",
        "        self.num_patches = self.grid_size ** 2\n",
        "        print(f\"Grid size: {self.grid_size}x{self.grid_size}, Num patches: {self.num_patches}\")\n",
        "\n",
        "        # Define patch dimension for reconstruction\n",
        "        self.patch_dim = patch_size * patch_size * in_chans\n",
        "        print(f\"Patch dim: {self.patch_dim}\")\n",
        "\n",
        "        # Encoder embeddings\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(\n",
        "            torch.zeros(1, 1 + self.num_patches, embed_dim),\n",
        "            requires_grad=False\n",
        "        )  # fixed sin-cos embedding\n",
        "\n",
        "        # Encoder blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                embed_dim,\n",
        "                num_heads,\n",
        "                self.grid_size,  # Use original grid_size without +1\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                norm_layer=norm_layer\n",
        "            )\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "        # Decoder embeddings\n",
        "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
        "        self.decoder_pos_embed = nn.Parameter(\n",
        "            torch.zeros(1, 1 + self.num_patches, decoder_embed_dim),\n",
        "            requires_grad=False\n",
        "        )\n",
        "\n",
        "        # Decoder blocks\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                decoder_embed_dim,\n",
        "                decoder_num_heads,\n",
        "                self.grid_size,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                norm_layer=norm_layer\n",
        "            )\n",
        "            for _ in range(decoder_depth)\n",
        "        ])\n",
        "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
        "\n",
        "        # Decoder prediction head\n",
        "        self.decoder_pred = nn.Linear(decoder_embed_dim, self.patch_dim, bias=True)\n",
        "\n",
        "        # Init weights\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # Encoder positional embedding\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.embed_dim, self.grid_size)\n",
        "        pos_embed = torch.from_numpy(pos_embed).float().unsqueeze(0)\n",
        "        pe_token = torch.zeros(1, 1, self.embed_dim)\n",
        "        self.pos_embed.data.copy_(torch.cat([pe_token, pos_embed], dim=1))\n",
        "\n",
        "        # Decoder positional embedding (same pattern)\n",
        "        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_embed_dim, self.grid_size)\n",
        "        decoder_pos_embed = torch.from_numpy(decoder_pos_embed).float().unsqueeze(0)\n",
        "        decoder_pe_token = torch.zeros(1, 1, self.decoder_embed_dim)\n",
        "        self.decoder_pos_embed.data.copy_(torch.cat([decoder_pe_token, decoder_pos_embed], dim=1))\n",
        "\n",
        "        # Initialize cls_token and mask_token\n",
        "        nn.init.normal_(self.cls_token, std=0.02)\n",
        "        nn.init.normal_(self.mask_token, std=0.02)\n",
        "\n",
        "        # Initialize linear layers\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.LayerNorm):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def patchify(self, imgs):\n",
        "        \"\"\"Convert images into patches\"\"\"\n",
        "        # imgs: [B, 3, H, W]\n",
        "        p = self.patch_size\n",
        "        h = w = self.img_size // p\n",
        "        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
        "        x = x.permute(0, 2, 4, 3, 5, 1)  # [B, h, w, p, p, 3]\n",
        "        patches = x.reshape(shape=(x.shape[0], h * w, p * p * 3))  # [B, h*w, p*p*3]\n",
        "        return patches\n",
        "\n",
        "    def unpatchify(self, patches):\n",
        "        \"\"\"Restore patches back to images\"\"\"\n",
        "        # patches: [B, L, p*p*3]\n",
        "        p = self.patch_size\n",
        "        h = w = self.img_size // p\n",
        "        B = patches.shape[0]\n",
        "\n",
        "        x = patches.reshape(shape=(B, h, w, p, p, 3))\n",
        "        x = x.permute(0, 5, 1, 3, 2, 4)  # [B, 3, h, p, w, p]\n",
        "        imgs = x.reshape(shape=(B, 3, h * p, w * p))  # [B, 3, H, W]\n",
        "        return imgs\n",
        "\n",
        "    def random_masking(self, x, mask_ratio):\n",
        "        \"\"\"Random masking for MAE\"\"\"\n",
        "        N, L, D = x.shape  # batch, length, dim\n",
        "        len_keep = int(L * (1 - mask_ratio))\n",
        "\n",
        "        # Generate random noise and sort to identify indices to keep\n",
        "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
        "\n",
        "        # Sort noise for each sample\n",
        "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
        "        ids_restore = torch.argsort(ids_shuffle, dim=1)  # indices to restore original order\n",
        "\n",
        "        # Keep the first len_keep indices\n",
        "        ids_keep = ids_shuffle[:, :len_keep]\n",
        "\n",
        "        # Generate a binary mask: 0 is keep, 1 is remove\n",
        "        mask = torch.ones([N, L], device=x.device)\n",
        "        mask.scatter_(dim=1, index=ids_keep, value=0)\n",
        "\n",
        "        # Keep tokens indicated by the mask\n",
        "        x_masked = torch.gather(\n",
        "            x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D)\n",
        "        )\n",
        "\n",
        "        return x_masked, mask, ids_restore\n",
        "\n",
        "    def forward_encoder(self, x):\n",
        "        # Convert image to patches: [B, 3, H, W] -> [B, num_patches, embed_dim]\n",
        "        x = self.patch_embed(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "\n",
        "        # Add positional embedding (without cls token)\n",
        "        x = x + self.pos_embed[:, 1:, :]\n",
        "\n",
        "        # Apply masking\n",
        "        x, mask, ids_restore = self.random_masking(x, self.mask_ratio)\n",
        "\n",
        "        # Append cls token\n",
        "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
        "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        # Apply final normalization\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x, mask, ids_restore\n",
        "\n",
        "    def forward_decoder(self, x, ids_restore):\n",
        "        # Embed from encoder to decoder dimensions\n",
        "        x = self.decoder_embed(x)\n",
        "\n",
        "        # Add position embedding for decoder\n",
        "        x = x + self.decoder_pos_embed[:, :x.shape[1], :]\n",
        "\n",
        "        # Project encoder tokens to decoder tokens\n",
        "        # Note that the cls token is at position 0\n",
        "        x_vis = x[:, 1:, :]  # Remove cls token\n",
        "        B, N_vis, D = x_vis.shape\n",
        "\n",
        "        # Prepare mask tokens\n",
        "        mask_tokens = self.mask_token.repeat(B, self.num_patches - N_vis, 1)\n",
        "\n",
        "        # Concatenate visible tokens with mask tokens\n",
        "        x_ = torch.cat([x_vis, mask_tokens], dim=1)\n",
        "\n",
        "        # Unshuffle to put tokens back in original order\n",
        "        x_ = torch.gather(\n",
        "            x_, dim=1,\n",
        "            index=ids_restore.unsqueeze(-1).repeat(1, 1, D)\n",
        "        )\n",
        "\n",
        "        # Append cls token\n",
        "        x = torch.cat([x[:, :1, :], x_], dim=1)\n",
        "\n",
        "        # Apply decoder blocks\n",
        "        for blk in self.decoder_blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        # Apply final normalization and prediction head\n",
        "        x = self.decoder_norm(x)\n",
        "\n",
        "        # Predict only on patches (not cls token)\n",
        "        x = self.decoder_pred(x[:, 1:, :])\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, imgs):\n",
        "        # Move images to the correct device\n",
        "        imgs = imgs.to(self.cls_token.device)\n",
        "\n",
        "        # Encode\n",
        "        latent, mask, ids_restore = self.forward_encoder(imgs)\n",
        "\n",
        "        # Decode\n",
        "        pred = self.forward_decoder(latent, ids_restore)\n",
        "\n",
        "        # Calculate target patches\n",
        "        target = self.patchify(imgs)\n",
        "\n",
        "        # Calculate loss (L1 loss on masked patches)\n",
        "        loss = (pred - target) ** 2\n",
        "        loss = loss.mean(dim=-1)  # [B, L], mean loss per patch\n",
        "\n",
        "        # Apply mask: only compute loss on masked patches\n",
        "        mask = mask.to(loss.device)  # ensure on same device\n",
        "        loss = (loss * mask).sum() / mask.sum()  # mean loss on masked patches\n",
        "\n",
        "        return loss, pred, mask\n",
        "\n",
        "def train_mae(curated_path, batch_size=2, num_epochs=200,\n",
        "              patch_size=32, embed_dim=768, decoder_embed_dim=512,\n",
        "              mask_ratio=0.75):\n",
        "    \"\"\"Train MAE model\"\"\"\n",
        "    print(f\"Starting MAE training on {curated_path}\")\n",
        "    print(f\"Parameters: batch_size={batch_size}, patch_size={patch_size}, embed_dim={embed_dim}\")\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    dataset = EnfaceMaskedDataset(curated_path)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0  # To avoid CUDA issues\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    model = MAEModel(\n",
        "        img_size=512,\n",
        "        patch_size=patch_size,\n",
        "        in_chans=3,\n",
        "        embed_dim=embed_dim,\n",
        "        depth=8,\n",
        "        num_heads=12,\n",
        "        decoder_embed_dim=decoder_embed_dim,\n",
        "        decoder_depth=4,\n",
        "        decoder_num_heads=16,\n",
        "        mlp_ratio=4,\n",
        "        mask_ratio=mask_ratio\n",
        "    ).to(device)\n",
        "\n",
        "    # Check model device\n",
        "    print(f\"Model is on device: {next(model.parameters()).device}\")\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=1.5e-6,\n",
        "        betas=(0.9, 0.95),\n",
        "        weight_decay=0.05\n",
        "    )\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=num_epochs,\n",
        "        eta_min=1e-7\n",
        "    )\n",
        "\n",
        "    # Main training loop\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch}/{num_epochs-1}\")\n",
        "        epoch_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, imgs in enumerate(dataloader):\n",
        "            # Forward pass\n",
        "            loss, _, _ = model(imgs)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip gradients for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Optimizer step\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track metrics\n",
        "            batch_loss = loss.item()\n",
        "            epoch_loss += batch_loss\n",
        "            num_batches += 1\n",
        "\n",
        "            # Print progress\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"  Batch {batch_idx}: Loss = {batch_loss:.4f}\")\n",
        "\n",
        "        # Update learning rate\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # End of epoch tracking\n",
        "        avg_epoch_loss = epoch_loss / num_batches if num_batches > 0 else 0\n",
        "        print(f\"Epoch {epoch}: Loss = {avg_epoch_loss:.4f}, LR = {lr_scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "        # Save model checkpoint\n",
        "        if epoch % 20 == 0 or epoch == num_epochs - 1:\n",
        "            save_path = f\"mae_model_epoch_{epoch}.pt\"\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': avg_epoch_loss,\n",
        "            }, save_path)\n",
        "            print(f\"Model saved to {save_path}\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    return model\n",
        "\n",
        "# Run training if script is executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    # try:\n",
        "    curated_path = curated_path\n",
        "    print(f\"Using curated_path: {curated_path}\")\n",
        "    model = train_mae(\n",
        "        curated_path,\n",
        "        batch_size=2,\n",
        "        num_epochs=200,\n",
        "        patch_size=32,\n",
        "        embed_dim=768,\n",
        "        decoder_embed_dim=512,\n",
        "        mask_ratio=0.75\n",
        "    )\n",
        "    # except NameError:\n",
        "    #     print(\"curated_path not found. Please define curated_path before running.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zO7Jy1mQWDZ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}